{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, Imputer, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "\n",
    "np.random.seed(20171212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../datasource/APP埋点信用/%s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path%'total_app_var.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list (df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "before_sumbit_df = df.loc[:,df.columns[df.columns.str.endswith('_7') | df.columns.str.endswith('_30') | df.columns.str.endswith('_07')\n",
    "                   |df.columns.str.endswith('_15')|df.columns.str.endswith('_30')|df.columns.str.endswith('_60')\n",
    "                   |df.columns.str.endswith('_90')|df.columns.str.endswith('flag')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "before_sumbit_df.to_csv(path%'total_app_var_before.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "st = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_be_scala = before_sumbit_df.iloc[:, :-1 ].values\n",
    "Y = before_sumbit_df['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = st.fit_transform(to_be_scala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(path%'x_before',X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = np.load(path%'x.npz.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=20171214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    path = '../datasource/APP埋点信用/%s'\n",
    "    X = np.load(path%'x_before.npy')\n",
    "    Y = np.load(path%'y_before.npy')\n",
    "    print (X.shape)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=20171214)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(174,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout({{uniform(0,1)}}))\n",
    "    model.add(Dense({{choice([256,512,1024])}}))\n",
    "    model.add(Dropout({{uniform(0,1)}}))\n",
    "    model.add(Dense({{choice([256,512,1024])}}))\n",
    "    model.add(Dropout({{uniform(0,1)}}))\n",
    "    \n",
    "    # if we choose 'four', we add an additional foutth layer\n",
    "    if conditional({{choice(['three', 'four'])}}) == 'four':\n",
    "        model.add(Dense(100))\n",
    "        model.add({{choice([Dropout(0.5), Activation('linear')])}})\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "    # 输出层\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    print (model.summary())\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy'],\n",
    "        optimizer='sgd')\n",
    "    \n",
    "    model.fit(x_train, y_train, batch_size={{choice([64,128,256,512,1024,2048])}},epochs=10, verbose=2, validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print ('Test acc:', acc)\n",
    "    return {'loss': score, 'status': STATUS_OK, 'model':model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    best_run, best_model = optim.minimize(model=create_model,\n",
    "                                         data=data,\n",
    "                                         algo=tpe.suggest,\n",
    "                                         max_evals=20,\n",
    "                                         trials=Trials(),\n",
    "                                         notebook_name='模型')\n",
    "    X_train, Y_train, X_test, Y_test = data()\n",
    "    print (best_model.evaluate(X_test, Y_test))\n",
    "    print (best_run)\n",
    "    return best_run, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import datetime\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler, Imputer, OneHotEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils.np_utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import Callback\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import clone_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.uniform('Dropout', 0,1),\n",
      "        'Dense': hp.choice('Dense', [256,512,1024]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0,1),\n",
      "        'Dense_1': hp.choice('Dense_1', [256,512,1024]),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0,1),\n",
      "        'conditional': hp.choice('conditional', ['three', 'four']),\n",
      "        'add': hp.choice('add', [Dropout(0.5), Activation('linear')]),\n",
      "        'batch_size': hp.choice('batch_size', [64,128,256,512,1024,2048]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: path = '../datasource/APP埋点信用/%s'\n",
      "  3: X = np.load(path%'x_before.npy')\n",
      "  4: Y = np.load(path%'y_before.npy')\n",
      "  5: print (X.shape)\n",
      "  6: x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=20171214)\n",
      "  7: \n",
      "  8: \n",
      "  9: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     model = Sequential()\n",
      "   5:     model.add(Dense(256, input_shape=(174,)))\n",
      "   6:     model.add(Activation('relu'))\n",
      "   7:     model.add(Dropout(space['Dropout']))\n",
      "   8:     model.add(Dense(space['Dense']))\n",
      "   9:     model.add(Dropout(space['Dropout_1']))\n",
      "  10:     model.add(Dense(space['Dense_1']))\n",
      "  11:     model.add(Dropout(space['Dropout_2']))\n",
      "  12:     \n",
      "  13:     # if we choose 'four', we add an additional foutth layer\n",
      "  14:     if conditional(space['conditional']) == 'four':\n",
      "  15:         model.add(Dense(100))\n",
      "  16:         model.add(space['add'])\n",
      "  17:         model.add(Activation('relu'))\n",
      "  18:     \n",
      "  19:     # 输出层\n",
      "  20:     model.add(Dense(1))\n",
      "  21:     model.add(Activation('sigmoid'))\n",
      "  22:     print (model.summary())\n",
      "  23:     \n",
      "  24:     model.compile(\n",
      "  25:         loss='binary_crossentropy', \n",
      "  26:         metrics=['accuracy'],\n",
      "  27:         optimizer='sgd')\n",
      "  28:     \n",
      "  29:     model.fit(x_train, y_train, batch_size=space['batch_size'],epochs=10, verbose=2, validation_data=(x_test, y_test))\n",
      "  30:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  31:     print ('Test acc:', acc)\n",
      "  32:     return {'loss': score, 'status': STATUS_OK, 'model':model}\n",
      "  33: \n",
      "(46102, 174)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 1025      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,358,593\n",
      "Trainable params: 1,358,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "5s - loss: 0.5754 - acc: 0.7559 - val_loss: 0.4455 - val_acc: 0.8141\n",
      "Epoch 2/10\n",
      "4s - loss: 0.5027 - acc: 0.7969 - val_loss: 0.4298 - val_acc: 0.8185\n",
      "Epoch 3/10\n",
      "4s - loss: 0.4696 - acc: 0.8104 - val_loss: 0.4261 - val_acc: 0.8190\n",
      "Epoch 4/10\n",
      "4s - loss: 0.4602 - acc: 0.8113 - val_loss: 0.4204 - val_acc: 0.8212\n",
      "Epoch 5/10\n",
      "4s - loss: 0.4496 - acc: 0.8179 - val_loss: 0.4199 - val_acc: 0.8207\n",
      "Epoch 6/10\n",
      "4s - loss: 0.4387 - acc: 0.8179 - val_loss: 0.4186 - val_acc: 0.8215\n",
      "Epoch 7/10\n",
      "4s - loss: 0.4340 - acc: 0.8199 - val_loss: 0.4160 - val_acc: 0.8221\n",
      "Epoch 8/10\n",
      "4s - loss: 0.4317 - acc: 0.8220 - val_loss: 0.4153 - val_acc: 0.8225\n",
      "Epoch 9/10\n",
      "4s - loss: 0.4237 - acc: 0.8239 - val_loss: 0.4156 - val_acc: 0.8229\n",
      "Epoch 10/10\n",
      "4s - loss: 0.4266 - acc: 0.8222 - val_loss: 0.4149 - val_acc: 0.8226\n",
      "Test acc: 0.822623501989\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 242,689\n",
      "Trainable params: 242,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "1s - loss: 1.1462 - acc: 0.5809 - val_loss: 0.5535 - val_acc: 0.7838\n",
      "Epoch 2/10\n",
      "1s - loss: 0.8502 - acc: 0.6212 - val_loss: 0.4940 - val_acc: 0.7999\n",
      "Epoch 3/10\n",
      "1s - loss: 0.8301 - acc: 0.6580 - val_loss: 0.4985 - val_acc: 0.8006\n",
      "Epoch 4/10\n",
      "1s - loss: 0.8779 - acc: 0.6795 - val_loss: 0.5919 - val_acc: 0.7997\n",
      "Epoch 5/10\n",
      "1s - loss: 1.0206 - acc: 0.6890 - val_loss: 1.1139 - val_acc: 0.7774\n",
      "Epoch 6/10\n",
      "1s - loss: 1.2884 - acc: 0.6971 - val_loss: 0.9531 - val_acc: 0.7970\n",
      "Epoch 7/10\n",
      "1s - loss: 1.7124 - acc: 0.6933 - val_loss: 1.3254 - val_acc: 0.8052\n",
      "Epoch 8/10\n",
      "1s - loss: 1.4606 - acc: 0.7392 - val_loss: 1.1034 - val_acc: 0.8079\n",
      "Epoch 9/10\n",
      "1s - loss: 1.3145 - acc: 0.7385 - val_loss: 1.0598 - val_acc: 0.8095\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 1.2169 - acc: 0.7509 - val_loss: 1.1919 - val_acc: 0.8074\n",
      "Test acc: 0.80743994361\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               51300     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 884,169\n",
      "Trainable params: 884,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "2s - loss: 0.6523 - acc: 0.6899 - val_loss: 0.4842 - val_acc: 0.8023\n",
      "Epoch 2/10\n",
      "2s - loss: 0.5477 - acc: 0.7677 - val_loss: 0.4652 - val_acc: 0.8093\n",
      "Epoch 3/10\n",
      "2s - loss: 0.5041 - acc: 0.7868 - val_loss: 0.4484 - val_acc: 0.8155\n",
      "Epoch 4/10\n",
      "2s - loss: 0.4891 - acc: 0.7994 - val_loss: 0.4454 - val_acc: 0.8162\n",
      "Epoch 5/10\n",
      "2s - loss: 0.4779 - acc: 0.8041 - val_loss: 0.4427 - val_acc: 0.8174\n",
      "Epoch 6/10\n",
      "2s - loss: 0.4682 - acc: 0.8074 - val_loss: 0.4385 - val_acc: 0.8187\n",
      "Epoch 7/10\n",
      "2s - loss: 0.4660 - acc: 0.8094 - val_loss: 0.4346 - val_acc: 0.8219\n",
      "Epoch 8/10\n",
      "2s - loss: 0.4544 - acc: 0.8118 - val_loss: 0.4341 - val_acc: 0.8209\n",
      "Epoch 9/10\n",
      "2s - loss: 0.4514 - acc: 0.8148 - val_loss: 0.4323 - val_acc: 0.8205\n",
      "Epoch 10/10\n",
      "2s - loss: 0.4458 - acc: 0.8182 - val_loss: 0.4272 - val_acc: 0.8229\n",
      "Test acc: 0.822948863954\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 202,185\n",
      "Trainable params: 202,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "0s - loss: 0.8995 - acc: 0.5219 - val_loss: 0.6475 - val_acc: 0.6977\n",
      "Epoch 2/10\n",
      "0s - loss: 0.7748 - acc: 0.5880 - val_loss: 0.5917 - val_acc: 0.7665\n",
      "Epoch 3/10\n",
      "0s - loss: 0.7187 - acc: 0.6311 - val_loss: 0.5587 - val_acc: 0.7830\n",
      "Epoch 4/10\n",
      "0s - loss: 0.6858 - acc: 0.6597 - val_loss: 0.5350 - val_acc: 0.7907\n",
      "Epoch 5/10\n",
      "0s - loss: 0.6537 - acc: 0.6803 - val_loss: 0.5191 - val_acc: 0.7935\n",
      "Epoch 6/10\n",
      "0s - loss: 0.6350 - acc: 0.6982 - val_loss: 0.5064 - val_acc: 0.7960\n",
      "Epoch 7/10\n",
      "0s - loss: 0.6133 - acc: 0.7118 - val_loss: 0.4969 - val_acc: 0.7989\n",
      "Epoch 8/10\n",
      "0s - loss: 0.5996 - acc: 0.7241 - val_loss: 0.4898 - val_acc: 0.7998\n",
      "Epoch 9/10\n",
      "0s - loss: 0.5894 - acc: 0.7358 - val_loss: 0.4832 - val_acc: 0.8014\n",
      "Epoch 10/10\n",
      "0s - loss: 0.5901 - acc: 0.7368 - val_loss: 0.4785 - val_acc: 0.8023\n",
      "Test acc: 0.802288379162\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 100)               102500    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,460,169\n",
      "Trainable params: 1,460,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "3s - loss: 0.5669 - acc: 0.7565 - val_loss: 0.4959 - val_acc: 0.7981\n",
      "Epoch 2/10\n",
      "3s - loss: 0.4774 - acc: 0.8043 - val_loss: 0.4636 - val_acc: 0.8054\n",
      "Epoch 3/10\n",
      "3s - loss: 0.4526 - acc: 0.8110 - val_loss: 0.4492 - val_acc: 0.8099\n",
      "Epoch 4/10\n",
      "3s - loss: 0.4382 - acc: 0.8161 - val_loss: 0.4495 - val_acc: 0.8105\n",
      "Epoch 5/10\n",
      "3s - loss: 0.4312 - acc: 0.8185 - val_loss: 0.4378 - val_acc: 0.8158\n",
      "Epoch 6/10\n",
      "3s - loss: 0.4263 - acc: 0.8208 - val_loss: 0.4448 - val_acc: 0.8133\n",
      "Epoch 7/10\n",
      "3s - loss: 0.4212 - acc: 0.8240 - val_loss: 0.4319 - val_acc: 0.8176\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3s - loss: 0.4191 - acc: 0.8238 - val_loss: 0.4287 - val_acc: 0.8193\n",
      "Epoch 9/10\n",
      "3s - loss: 0.4146 - acc: 0.8245 - val_loss: 0.4260 - val_acc: 0.8192\n",
      "Epoch 10/10\n",
      "3s - loss: 0.4138 - acc: 0.8263 - val_loss: 0.4243 - val_acc: 0.8205\n",
      "Test acc: 0.820508649212\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 1025      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 702,721\n",
      "Trainable params: 702,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "2s - loss: 1.1401 - acc: 0.5998 - val_loss: 0.4843 - val_acc: 0.7994\n",
      "Epoch 2/10\n",
      "2s - loss: 0.7359 - acc: 0.6775 - val_loss: 0.4742 - val_acc: 0.8064\n",
      "Epoch 3/10\n",
      "2s - loss: 0.6368 - acc: 0.7049 - val_loss: 0.4739 - val_acc: 0.8075\n",
      "Epoch 4/10\n",
      "2s - loss: 0.6014 - acc: 0.7237 - val_loss: 0.4599 - val_acc: 0.8110\n",
      "Epoch 5/10\n",
      "2s - loss: 0.5813 - acc: 0.7366 - val_loss: 0.4513 - val_acc: 0.8115\n",
      "Epoch 6/10\n",
      "2s - loss: 0.5628 - acc: 0.7489 - val_loss: 0.4491 - val_acc: 0.8117\n",
      "Epoch 7/10\n",
      "2s - loss: 0.5513 - acc: 0.7593 - val_loss: 0.4439 - val_acc: 0.8131\n",
      "Epoch 8/10\n",
      "2s - loss: 0.5440 - acc: 0.7633 - val_loss: 0.4416 - val_acc: 0.8136\n",
      "Epoch 9/10\n",
      "2s - loss: 0.5322 - acc: 0.7686 - val_loss: 0.4413 - val_acc: 0.8139\n",
      "Epoch 10/10\n",
      "2s - loss: 0.5397 - acc: 0.7659 - val_loss: 0.4382 - val_acc: 0.8152\n",
      "Test acc: 0.815194403784\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 100)               51300     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 490,441\n",
      "Trainable params: 490,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "1s - loss: 0.7560 - acc: 0.6160 - val_loss: 0.4969 - val_acc: 0.8010\n",
      "Epoch 2/10\n",
      "1s - loss: 0.6149 - acc: 0.7202 - val_loss: 0.4664 - val_acc: 0.8074\n",
      "Epoch 3/10\n",
      "1s - loss: 0.5699 - acc: 0.7559 - val_loss: 0.4551 - val_acc: 0.8087\n",
      "Epoch 4/10\n",
      "1s - loss: 0.5402 - acc: 0.7670 - val_loss: 0.4469 - val_acc: 0.8113\n",
      "Epoch 5/10\n",
      "1s - loss: 0.5266 - acc: 0.7772 - val_loss: 0.4427 - val_acc: 0.8118\n",
      "Epoch 6/10\n",
      "1s - loss: 0.5147 - acc: 0.7862 - val_loss: 0.4404 - val_acc: 0.8122\n",
      "Epoch 7/10\n",
      "1s - loss: 0.5011 - acc: 0.7895 - val_loss: 0.4378 - val_acc: 0.8137\n",
      "Epoch 8/10\n",
      "1s - loss: 0.4929 - acc: 0.7968 - val_loss: 0.4357 - val_acc: 0.8149\n",
      "Epoch 9/10\n",
      "1s - loss: 0.4882 - acc: 0.7978 - val_loss: 0.4345 - val_acc: 0.8154\n",
      "Epoch 10/10\n",
      "1s - loss: 0.4788 - acc: 0.7998 - val_loss: 0.4338 - val_acc: 0.8159\n",
      "Test acc: 0.815899354708\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 307,969\n",
      "Trainable params: 307,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "1s - loss: 0.6180 - acc: 0.6879 - val_loss: 0.5214 - val_acc: 0.7881\n",
      "Epoch 2/10\n",
      "0s - loss: 0.5279 - acc: 0.7738 - val_loss: 0.4811 - val_acc: 0.8008\n",
      "Epoch 3/10\n",
      "0s - loss: 0.4905 - acc: 0.7939 - val_loss: 0.4621 - val_acc: 0.8071\n",
      "Epoch 4/10\n",
      "0s - loss: 0.4750 - acc: 0.8000 - val_loss: 0.4522 - val_acc: 0.8103\n",
      "Epoch 5/10\n",
      "0s - loss: 0.4626 - acc: 0.8067 - val_loss: 0.4457 - val_acc: 0.8120\n",
      "Epoch 6/10\n",
      "0s - loss: 0.4569 - acc: 0.8074 - val_loss: 0.4407 - val_acc: 0.8136\n",
      "Epoch 7/10\n",
      "0s - loss: 0.4550 - acc: 0.8125 - val_loss: 0.4376 - val_acc: 0.8152\n",
      "Epoch 8/10\n",
      "0s - loss: 0.4511 - acc: 0.8129 - val_loss: 0.4351 - val_acc: 0.8167\n",
      "Epoch 9/10\n",
      "0s - loss: 0.4466 - acc: 0.8138 - val_loss: 0.4327 - val_acc: 0.8175\n",
      "Epoch 10/10\n",
      "0s - loss: 0.4428 - acc: 0.8153 - val_loss: 0.4314 - val_acc: 0.8181\n",
      "Test acc: 0.818068434473\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 439,553\n",
      "Trainable params: 439,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "1s - loss: 0.6117 - acc: 0.7222 - val_loss: 0.4511 - val_acc: 0.8125\n",
      "Epoch 2/10\n",
      "1s - loss: 0.5300 - acc: 0.7738 - val_loss: 0.4411 - val_acc: 0.8150\n",
      "Epoch 3/10\n",
      "1s - loss: 0.5119 - acc: 0.7854 - val_loss: 0.4367 - val_acc: 0.8165\n",
      "Epoch 4/10\n",
      "1s - loss: 0.4999 - acc: 0.7915 - val_loss: 0.4368 - val_acc: 0.8163\n",
      "Epoch 5/10\n",
      "1s - loss: 0.4920 - acc: 0.7955 - val_loss: 0.4366 - val_acc: 0.8175\n",
      "Epoch 6/10\n",
      "1s - loss: 0.4834 - acc: 0.7994 - val_loss: 0.4310 - val_acc: 0.8186\n",
      "Epoch 7/10\n",
      "1s - loss: 0.4821 - acc: 0.8030 - val_loss: 0.4315 - val_acc: 0.8185\n",
      "Epoch 8/10\n",
      "1s - loss: 0.4728 - acc: 0.8018 - val_loss: 0.4271 - val_acc: 0.8184\n",
      "Epoch 9/10\n",
      "1s - loss: 0.4723 - acc: 0.8030 - val_loss: 0.4264 - val_acc: 0.8186\n",
      "Epoch 10/10\n",
      "1s - loss: 0.4684 - acc: 0.8046 - val_loss: 0.4250 - val_acc: 0.8197\n",
      "Test acc: 0.819695244302\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 100)               102500    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 804,297\n",
      "Trainable params: 804,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "4s - loss: 0.5056 - acc: 0.7828 - val_loss: 0.4342 - val_acc: 0.8178\n",
      "Epoch 2/10\n",
      "4s - loss: 0.4603 - acc: 0.8103 - val_loss: 0.4300 - val_acc: 0.8183\n",
      "Epoch 3/10\n",
      "4s - loss: 0.4507 - acc: 0.8128 - val_loss: 0.4285 - val_acc: 0.8201\n",
      "Epoch 4/10\n",
      "4s - loss: 0.4462 - acc: 0.8159 - val_loss: 0.4210 - val_acc: 0.8224\n",
      "Epoch 5/10\n",
      "4s - loss: 0.4384 - acc: 0.8162 - val_loss: 0.4219 - val_acc: 0.8229\n",
      "Epoch 6/10\n",
      "4s - loss: 0.4304 - acc: 0.8227 - val_loss: 0.4201 - val_acc: 0.8234\n",
      "Epoch 7/10\n",
      "4s - loss: 0.4278 - acc: 0.8217 - val_loss: 0.4224 - val_acc: 0.8233\n",
      "Epoch 8/10\n",
      "4s - loss: 0.4236 - acc: 0.8219 - val_loss: 0.4223 - val_acc: 0.8225\n",
      "Epoch 9/10\n",
      "4s - loss: 0.4217 - acc: 0.8246 - val_loss: 0.4213 - val_acc: 0.8226\n",
      "Epoch 10/10\n",
      "4s - loss: 0.4171 - acc: 0.8244 - val_loss: 0.4158 - val_acc: 0.8251\n",
      "Test acc: 0.825063716725\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 100)               102500    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,460,169\n",
      "Trainable params: 1,460,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "5s - loss: 0.4761 - acc: 0.7977 - val_loss: 0.4294 - val_acc: 0.8199\n",
      "Epoch 2/10\n",
      "4s - loss: 0.4282 - acc: 0.8204 - val_loss: 0.4254 - val_acc: 0.8213\n",
      "Epoch 3/10\n",
      "4s - loss: 0.4214 - acc: 0.8237 - val_loss: 0.4198 - val_acc: 0.8244\n",
      "Epoch 4/10\n",
      "5s - loss: 0.4124 - acc: 0.8272 - val_loss: 0.4169 - val_acc: 0.8246\n",
      "Epoch 5/10\n",
      "5s - loss: 0.4076 - acc: 0.8284 - val_loss: 0.4166 - val_acc: 0.8244\n",
      "Epoch 6/10\n",
      "5s - loss: 0.4060 - acc: 0.8291 - val_loss: 0.4137 - val_acc: 0.8259\n",
      "Epoch 7/10\n",
      "5s - loss: 0.4037 - acc: 0.8305 - val_loss: 0.4128 - val_acc: 0.8259\n",
      "Epoch 8/10\n",
      "4s - loss: 0.4012 - acc: 0.8300 - val_loss: 0.4100 - val_acc: 0.8271\n",
      "Epoch 9/10\n",
      "4s - loss: 0.4000 - acc: 0.8310 - val_loss: 0.4091 - val_acc: 0.8267\n",
      "Epoch 10/10\n",
      "4s - loss: 0.3982 - acc: 0.8317 - val_loss: 0.4087 - val_acc: 0.8267\n",
      "Test acc: 0.826690526551\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 100)               51300     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 884,169\n",
      "Trainable params: 884,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "2s - loss: 0.6014 - acc: 0.7009 - val_loss: 0.4980 - val_acc: 0.7990\n",
      "Epoch 2/10\n",
      "2s - loss: 0.5189 - acc: 0.7778 - val_loss: 0.4636 - val_acc: 0.8079\n",
      "Epoch 3/10\n",
      "2s - loss: 0.4917 - acc: 0.7936 - val_loss: 0.4499 - val_acc: 0.8121\n",
      "Epoch 4/10\n",
      "2s - loss: 0.4772 - acc: 0.7998 - val_loss: 0.4439 - val_acc: 0.8145\n",
      "Epoch 5/10\n",
      "2s - loss: 0.4705 - acc: 0.8043 - val_loss: 0.4396 - val_acc: 0.8151\n",
      "Epoch 6/10\n",
      "2s - loss: 0.4633 - acc: 0.8092 - val_loss: 0.4385 - val_acc: 0.8170\n",
      "Epoch 7/10\n",
      "2s - loss: 0.4579 - acc: 0.8111 - val_loss: 0.4345 - val_acc: 0.8180\n",
      "Epoch 8/10\n",
      "2s - loss: 0.4562 - acc: 0.8119 - val_loss: 0.4330 - val_acc: 0.8186\n",
      "Epoch 9/10\n",
      "2s - loss: 0.4459 - acc: 0.8140 - val_loss: 0.4303 - val_acc: 0.8183\n",
      "Epoch 10/10\n",
      "2s - loss: 0.4458 - acc: 0.8163 - val_loss: 0.4285 - val_acc: 0.8185\n",
      "Test acc: 0.818502250427\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_56 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 333,513\n",
      "Trainable params: 333,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "2s - loss: 0.5519 - acc: 0.7551 - val_loss: 0.4428 - val_acc: 0.8111\n",
      "Epoch 2/10\n",
      "1s - loss: 0.4875 - acc: 0.7995 - val_loss: 0.4314 - val_acc: 0.8166\n",
      "Epoch 3/10\n",
      "1s - loss: 0.4643 - acc: 0.8097 - val_loss: 0.4270 - val_acc: 0.8188\n",
      "Epoch 4/10\n",
      "1s - loss: 0.4516 - acc: 0.8140 - val_loss: 0.4247 - val_acc: 0.8212\n",
      "Epoch 5/10\n",
      "1s - loss: 0.4472 - acc: 0.8176 - val_loss: 0.4196 - val_acc: 0.8216\n",
      "Epoch 6/10\n",
      "1s - loss: 0.4361 - acc: 0.8195 - val_loss: 0.4180 - val_acc: 0.8226\n",
      "Epoch 7/10\n",
      "1s - loss: 0.4303 - acc: 0.8232 - val_loss: 0.4157 - val_acc: 0.8234\n",
      "Epoch 8/10\n",
      "1s - loss: 0.4317 - acc: 0.8226 - val_loss: 0.4154 - val_acc: 0.8250\n",
      "Epoch 9/10\n",
      "1s - loss: 0.4229 - acc: 0.8260 - val_loss: 0.4127 - val_acc: 0.8253\n",
      "Epoch 10/10\n",
      "1s - loss: 0.4164 - acc: 0.8261 - val_loss: 0.4117 - val_acc: 0.8257\n",
      "Test acc: 0.825660213661\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_61 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 176,641\n",
      "Trainable params: 176,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "1s - loss: 0.5023 - acc: 0.7818 - val_loss: 0.4349 - val_acc: 0.8170\n",
      "Epoch 2/10\n",
      "1s - loss: 0.4467 - acc: 0.8163 - val_loss: 0.4277 - val_acc: 0.8196\n",
      "Epoch 3/10\n",
      "1s - loss: 0.4356 - acc: 0.8180 - val_loss: 0.4225 - val_acc: 0.8217\n",
      "Epoch 4/10\n",
      "1s - loss: 0.4245 - acc: 0.8248 - val_loss: 0.4186 - val_acc: 0.8233\n",
      "Epoch 5/10\n",
      "1s - loss: 0.4214 - acc: 0.8241 - val_loss: 0.4167 - val_acc: 0.8234\n",
      "Epoch 6/10\n",
      "1s - loss: 0.4161 - acc: 0.8282 - val_loss: 0.4153 - val_acc: 0.8241\n",
      "Epoch 7/10\n",
      "1s - loss: 0.4140 - acc: 0.8294 - val_loss: 0.4136 - val_acc: 0.8247\n",
      "Epoch 8/10\n",
      "1s - loss: 0.4098 - acc: 0.8302 - val_loss: 0.4131 - val_acc: 0.8251\n",
      "Epoch 9/10\n",
      "1s - loss: 0.4071 - acc: 0.8302 - val_loss: 0.4120 - val_acc: 0.8245\n",
      "Epoch 10/10\n",
      "1s - loss: 0.4050 - acc: 0.8307 - val_loss: 0.4107 - val_acc: 0.8259\n",
      "Test acc: 0.825877121638\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_65 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 100)               102500    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 804,297\n",
      "Trainable params: 804,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "4s - loss: 0.5873 - acc: 0.7448 - val_loss: 0.4383 - val_acc: 0.8152\n",
      "Epoch 2/10\n",
      "4s - loss: 0.4845 - acc: 0.8005 - val_loss: 0.4280 - val_acc: 0.8177\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4s - loss: 0.4551 - acc: 0.8110 - val_loss: 0.4239 - val_acc: 0.8188\n",
      "Epoch 4/10\n",
      "4s - loss: 0.4441 - acc: 0.8178 - val_loss: 0.4208 - val_acc: 0.8197\n",
      "Epoch 5/10\n",
      "4s - loss: 0.4337 - acc: 0.8187 - val_loss: 0.4189 - val_acc: 0.8197\n",
      "Epoch 6/10\n",
      "4s - loss: 0.4308 - acc: 0.8213 - val_loss: 0.4178 - val_acc: 0.8200\n",
      "Epoch 7/10\n",
      "4s - loss: 0.4301 - acc: 0.8230 - val_loss: 0.4161 - val_acc: 0.8210\n",
      "Epoch 8/10\n",
      "4s - loss: 0.4257 - acc: 0.8243 - val_loss: 0.4163 - val_acc: 0.8219\n",
      "Epoch 9/10\n",
      "4s - loss: 0.4245 - acc: 0.8259 - val_loss: 0.4145 - val_acc: 0.8238\n",
      "Epoch 10/10\n",
      "4s - loss: 0.4191 - acc: 0.8261 - val_loss: 0.4133 - val_acc: 0.8234\n",
      "Test acc: 0.823382679905\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_70 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 100)               102500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 804,297\n",
      "Trainable params: 804,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "3s - loss: 0.5621 - acc: 0.7443 - val_loss: 0.4535 - val_acc: 0.8110\n",
      "Epoch 2/10\n",
      "3s - loss: 0.4791 - acc: 0.8008 - val_loss: 0.4351 - val_acc: 0.8168\n",
      "Epoch 3/10\n",
      "3s - loss: 0.4562 - acc: 0.8101 - val_loss: 0.4279 - val_acc: 0.8191\n",
      "Epoch 4/10\n",
      "3s - loss: 0.4462 - acc: 0.8145 - val_loss: 0.4257 - val_acc: 0.8202\n",
      "Epoch 5/10\n",
      "3s - loss: 0.4399 - acc: 0.8181 - val_loss: 0.4230 - val_acc: 0.8222\n",
      "Epoch 6/10\n",
      "3s - loss: 0.4342 - acc: 0.8221 - val_loss: 0.4233 - val_acc: 0.8224\n",
      "Epoch 7/10\n",
      "3s - loss: 0.4287 - acc: 0.8222 - val_loss: 0.4201 - val_acc: 0.8226\n",
      "Epoch 8/10\n",
      "3s - loss: 0.4238 - acc: 0.8230 - val_loss: 0.4177 - val_acc: 0.8234\n",
      "Epoch 9/10\n",
      "3s - loss: 0.4236 - acc: 0.8240 - val_loss: 0.4168 - val_acc: 0.8229\n",
      "Epoch 10/10\n",
      "3s - loss: 0.4181 - acc: 0.8272 - val_loss: 0.4158 - val_acc: 0.8237\n",
      "Test acc: 0.823653814876\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 202,185\n",
      "Trainable params: 202,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "1s - loss: 0.6862 - acc: 0.6815 - val_loss: 0.4614 - val_acc: 0.8128\n",
      "Epoch 2/10\n",
      "1s - loss: 0.5519 - acc: 0.7637 - val_loss: 0.4447 - val_acc: 0.8160\n",
      "Epoch 3/10\n",
      "1s - loss: 0.5086 - acc: 0.7866 - val_loss: 0.4366 - val_acc: 0.8170\n",
      "Epoch 4/10\n",
      "1s - loss: 0.4990 - acc: 0.7908 - val_loss: 0.4328 - val_acc: 0.8179\n",
      "Epoch 5/10\n",
      "1s - loss: 0.4801 - acc: 0.7989 - val_loss: 0.4300 - val_acc: 0.8183\n",
      "Epoch 6/10\n",
      "1s - loss: 0.4743 - acc: 0.8044 - val_loss: 0.4280 - val_acc: 0.8194\n",
      "Epoch 7/10\n",
      "1s - loss: 0.4611 - acc: 0.8096 - val_loss: 0.4266 - val_acc: 0.8198\n",
      "Epoch 8/10\n",
      "1s - loss: 0.4582 - acc: 0.8086 - val_loss: 0.4241 - val_acc: 0.8209\n",
      "Epoch 9/10\n",
      "1s - loss: 0.4548 - acc: 0.8127 - val_loss: 0.4287 - val_acc: 0.8218\n",
      "Epoch 10/10\n",
      "1s - loss: 0.4489 - acc: 0.8155 - val_loss: 0.4220 - val_acc: 0.8203\n",
      "Test acc: 0.820345968229\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_80 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 100)               102500    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 476,361\n",
      "Trainable params: 476,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3s - loss: 0.5285 - acc: 0.7678 - val_loss: 0.4357 - val_acc: 0.8155\n",
      "Epoch 2/10\n",
      "2s - loss: 0.4636 - acc: 0.8097 - val_loss: 0.4262 - val_acc: 0.8206\n",
      "Epoch 3/10\n",
      "2s - loss: 0.4467 - acc: 0.8149 - val_loss: 0.4206 - val_acc: 0.8214\n",
      "Epoch 4/10\n",
      "2s - loss: 0.4371 - acc: 0.8178 - val_loss: 0.4180 - val_acc: 0.8225\n",
      "Epoch 5/10\n",
      "3s - loss: 0.4283 - acc: 0.8221 - val_loss: 0.4158 - val_acc: 0.8225\n",
      "Epoch 6/10\n",
      "3s - loss: 0.4200 - acc: 0.8255 - val_loss: 0.4136 - val_acc: 0.8237\n",
      "Epoch 7/10\n",
      "3s - loss: 0.4188 - acc: 0.8250 - val_loss: 0.4130 - val_acc: 0.8245\n",
      "Epoch 8/10\n",
      "2s - loss: 0.4165 - acc: 0.8256 - val_loss: 0.4137 - val_acc: 0.8239\n",
      "Epoch 9/10\n",
      "2s - loss: 0.4127 - acc: 0.8273 - val_loss: 0.4110 - val_acc: 0.8251\n",
      "Epoch 10/10\n",
      "2s - loss: 0.4077 - acc: 0.8289 - val_loss: 0.4095 - val_acc: 0.8250\n",
      "Test acc: 0.824955262736\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_85 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 1)                 1025      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,358,593\n",
      "Trainable params: 1,358,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "6s - loss: 0.4675 - acc: 0.8052 - val_loss: 0.4272 - val_acc: 0.8187\n",
      "Epoch 2/10\n",
      "6s - loss: 0.4401 - acc: 0.8174 - val_loss: 0.4195 - val_acc: 0.8212\n",
      "Epoch 3/10\n",
      "6s - loss: 0.4268 - acc: 0.8229 - val_loss: 0.4188 - val_acc: 0.8228\n",
      "Epoch 4/10\n",
      "6s - loss: 0.4250 - acc: 0.8246 - val_loss: 0.4160 - val_acc: 0.8231\n",
      "Epoch 5/10\n",
      "6s - loss: 0.4193 - acc: 0.8258 - val_loss: 0.4138 - val_acc: 0.8230\n",
      "Epoch 6/10\n",
      "6s - loss: 0.4128 - acc: 0.8289 - val_loss: 0.4112 - val_acc: 0.8238\n",
      "Epoch 7/10\n",
      "6s - loss: 0.4127 - acc: 0.8268 - val_loss: 0.4096 - val_acc: 0.8246\n",
      "Epoch 8/10\n",
      "6s - loss: 0.4076 - acc: 0.8293 - val_loss: 0.4101 - val_acc: 0.8250\n",
      "Epoch 9/10\n",
      "6s - loss: 0.4096 - acc: 0.8284 - val_loss: 0.4082 - val_acc: 0.8254\n",
      "Epoch 10/10\n",
      "6s - loss: 0.4078 - acc: 0.8291 - val_loss: 0.4071 - val_acc: 0.8253\n",
      "Test acc: 0.825334851696\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_89 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 333,513\n",
      "Trainable params: 333,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/10\n",
      "2s - loss: 0.5322 - acc: 0.7696 - val_loss: 0.4308 - val_acc: 0.8177\n",
      "Epoch 2/10\n",
      "2s - loss: 0.4612 - acc: 0.8090 - val_loss: 0.4285 - val_acc: 0.8196\n",
      "Epoch 3/10\n",
      "2s - loss: 0.4422 - acc: 0.8174 - val_loss: 0.4212 - val_acc: 0.8201\n",
      "Epoch 4/10\n",
      "2s - loss: 0.4365 - acc: 0.8184 - val_loss: 0.4197 - val_acc: 0.8208\n",
      "Epoch 5/10\n",
      "2s - loss: 0.4281 - acc: 0.8220 - val_loss: 0.4152 - val_acc: 0.8219\n",
      "Epoch 6/10\n",
      "2s - loss: 0.4241 - acc: 0.8242 - val_loss: 0.4176 - val_acc: 0.8215\n",
      "Epoch 7/10\n",
      "2s - loss: 0.4208 - acc: 0.8265 - val_loss: 0.4157 - val_acc: 0.8220\n",
      "Epoch 8/10\n",
      "2s - loss: 0.4200 - acc: 0.8257 - val_loss: 0.4142 - val_acc: 0.8215\n",
      "Epoch 9/10\n",
      "2s - loss: 0.4163 - acc: 0.8249 - val_loss: 0.4114 - val_acc: 0.8227\n",
      "Epoch 10/10\n",
      "2s - loss: 0.4138 - acc: 0.8265 - val_loss: 0.4107 - val_acc: 0.8229\n",
      "Test acc: 0.822894636957\n",
      "(46102, 174)\n",
      "18272/18441 [============================>.] - ETA: 0s[0.40710732907898789, 0.82533485169563525]\n",
      "{'Dense': 2, 'Dense_1': 2, 'Dropout': 0.38757827904833575, 'Dropout_1': 0.36117159829314394, 'Dropout_2': 0.3640245915268092, 'add': 0, 'batch_size': 0, 'conditional': 0}\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense': 2,\n",
       " 'Dense_1': 2,\n",
       " 'Dropout': 0.38757827904833575,\n",
       " 'Dropout_1': 0.36117159829314394,\n",
       " 'Dropout_2': 0.3640245915268092,\n",
       " 'add': 0,\n",
       " 'batch_size': 0,\n",
       " 'conditional': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用进件钱数据\n",
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_94 (Dense)             (None, 256)               44800     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 1)                 1025      \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,358,593\n",
      "Trainable params: 1,358,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 确定参数后，调整epochs个数\n",
    "# x_train, y_train, x_test, y_test = data()\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(174,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.388))\n",
    "model.add(Dense(1024))\n",
    "model.add(Dropout(0.361))\n",
    "model.add(Dense(1024))\n",
    "model.add(Dropout(0.364))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 输出层\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "print (model.summary())\n",
    "\n",
    "\n",
    "# batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'keras_tunning_lr' from '/tmp/working/app埋点信用/keras_tunning_lr.py'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras_tunning_lr as tlr\n",
    "import importlib\n",
    "importlib.reload(tlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46102, 174)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = [x_train, x_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tunning_lr = tlr.LR_Tunning(model, batch_size=64, epochs=20, dataSet=data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/20\n",
      "7s - loss: 0.7270 - acc: 0.7936 - val_loss: 1.3410 - val_acc: 0.8025\n",
      "Epoch 2/20\n",
      "6s - loss: 2.2843 - acc: 0.7781 - val_loss: 2.8707 - val_acc: 0.7939\n",
      "Epoch 3/20\n",
      "6s - loss: 2.5666 - acc: 0.8054 - val_loss: 3.1947 - val_acc: 0.7918\n",
      "Epoch 4/20\n",
      "6s - loss: 3.0982 - acc: 0.8042 - val_loss: 3.0949 - val_acc: 0.8060\n",
      "Epoch 5/20\n",
      "6s - loss: 3.0140 - acc: 0.8114 - val_loss: 3.1012 - val_acc: 0.8061\n",
      "Epoch 6/20\n",
      "6s - loss: 3.4303 - acc: 0.7851 - val_loss: 3.1056 - val_acc: 0.8060\n",
      "Epoch 7/20\n",
      "6s - loss: 3.3498 - acc: 0.7909 - val_loss: 3.5402 - val_acc: 0.7788\n",
      "Epoch 8/20\n",
      "6s - loss: 3.0534 - acc: 0.8092 - val_loss: 3.0697 - val_acc: 0.8082\n",
      "Epoch 9/20\n",
      "6s - loss: 2.9837 - acc: 0.8136 - val_loss: 3.0707 - val_acc: 0.8081\n",
      "Epoch 10/20\n",
      "6s - loss: 3.0065 - acc: 0.8120 - val_loss: 3.1378 - val_acc: 0.8041\n",
      "Epoch 11/20\n",
      "6s - loss: 3.0355 - acc: 0.8105 - val_loss: 3.1384 - val_acc: 0.8040\n",
      "Epoch 12/20\n",
      "6s - loss: 4.1957 - acc: 0.7388 - val_loss: 3.3819 - val_acc: 0.7890\n",
      "Epoch 13/20\n",
      "6s - loss: 4.5654 - acc: 0.7160 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 14/20\n",
      "6s - loss: 7.2858 - acc: 0.5480 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 15/20\n",
      "6s - loss: 7.2859 - acc: 0.5480 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 16/20\n",
      "6s - loss: 7.2899 - acc: 0.5477 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 17/20\n",
      "6s - loss: 6.7657 - acc: 0.5792 - val_loss: 5.6181 - val_acc: 0.6481\n",
      "Epoch 18/20\n",
      "6s - loss: 5.5426 - acc: 0.6528 - val_loss: 5.6181 - val_acc: 0.6481\n",
      "Epoch 19/20\n",
      "6s - loss: 5.5248 - acc: 0.6540 - val_loss: 5.6181 - val_acc: 0.6481\n",
      "Epoch 20/20\n",
      "6s - loss: 5.4964 - acc: 0.6557 - val_loss: 5.6181 - val_acc: 0.6481\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/20\n",
      "7s - loss: 0.4563 - acc: 0.8163 - val_loss: 0.4223 - val_acc: 0.8237\n",
      "Epoch 2/20\n",
      "6s - loss: 0.4155 - acc: 0.8276 - val_loss: 0.4115 - val_acc: 0.8270\n",
      "Epoch 3/20\n",
      "6s - loss: 0.4081 - acc: 0.8323 - val_loss: 0.4113 - val_acc: 0.8259\n",
      "Epoch 4/20\n",
      "6s - loss: 0.4040 - acc: 0.8323 - val_loss: 0.4046 - val_acc: 0.8276\n",
      "Epoch 5/20\n",
      "6s - loss: 0.3962 - acc: 0.8349 - val_loss: 0.4033 - val_acc: 0.8288\n",
      "Epoch 6/20\n",
      "6s - loss: 0.3929 - acc: 0.8355 - val_loss: 0.4025 - val_acc: 0.8291\n",
      "Epoch 7/20\n",
      "6s - loss: 0.3937 - acc: 0.8351 - val_loss: 0.4009 - val_acc: 0.8291\n",
      "Epoch 8/20\n",
      "6s - loss: 0.3908 - acc: 0.8346 - val_loss: 0.4017 - val_acc: 0.8295\n",
      "Epoch 9/20\n",
      "6s - loss: 0.3905 - acc: 0.8363 - val_loss: 0.4011 - val_acc: 0.8293\n",
      "Epoch 10/20\n",
      "6s - loss: 0.3877 - acc: 0.8369 - val_loss: 0.4028 - val_acc: 0.8295\n",
      "Epoch 11/20\n",
      "6s - loss: 0.3880 - acc: 0.8364 - val_loss: 0.4003 - val_acc: 0.8297\n",
      "Epoch 12/20\n",
      "6s - loss: 0.3887 - acc: 0.8374 - val_loss: 0.4004 - val_acc: 0.8296\n",
      "Epoch 13/20\n",
      "6s - loss: 0.3914 - acc: 0.8362 - val_loss: 0.4004 - val_acc: 0.8291\n",
      "Epoch 14/20\n",
      "6s - loss: 0.3884 - acc: 0.8362 - val_loss: 0.4000 - val_acc: 0.8298\n",
      "Epoch 15/20\n",
      "6s - loss: 0.3900 - acc: 0.8369 - val_loss: 0.4011 - val_acc: 0.8292\n",
      "Epoch 16/20\n",
      "6s - loss: 0.3872 - acc: 0.8370 - val_loss: 0.3996 - val_acc: 0.8291\n",
      "Epoch 17/20\n",
      "6s - loss: 0.3882 - acc: 0.8369 - val_loss: 0.4001 - val_acc: 0.8294\n",
      "Epoch 18/20\n",
      "6s - loss: 0.3888 - acc: 0.8371 - val_loss: 0.4004 - val_acc: 0.8295\n",
      "Epoch 19/20\n",
      "6s - loss: 0.3860 - acc: 0.8387 - val_loss: 0.3993 - val_acc: 0.8301\n",
      "Epoch 20/20\n",
      "6s - loss: 0.3838 - acc: 0.8387 - val_loss: 0.3992 - val_acc: 0.8302\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/20\n",
      "6s - loss: 0.5469 - acc: 0.7992 - val_loss: 1.0425 - val_acc: 0.7219\n",
      "Epoch 2/20\n",
      "6s - loss: 0.8789 - acc: 0.7924 - val_loss: 0.7673 - val_acc: 0.8157\n",
      "Epoch 3/20\n",
      "6s - loss: 1.1639 - acc: 0.7928 - val_loss: 1.4588 - val_acc: 0.7982\n",
      "Epoch 4/20\n",
      "6s - loss: 2.6940 - acc: 0.7724 - val_loss: 3.8996 - val_acc: 0.6906\n",
      "Epoch 5/20\n",
      "6s - loss: 3.0123 - acc: 0.7979 - val_loss: 3.4229 - val_acc: 0.7864\n",
      "Epoch 6/20\n",
      "6s - loss: 3.9116 - acc: 0.7553 - val_loss: 4.8472 - val_acc: 0.6963\n",
      "Epoch 7/20\n",
      "6s - loss: 7.5058 - acc: 0.5293 - val_loss: 8.6131 - val_acc: 0.4597\n",
      "Epoch 8/20\n",
      "6s - loss: 8.0491 - acc: 0.4958 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 9/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 10/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 11/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 12/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 13/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 14/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 15/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 16/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 17/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 18/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 19/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 20/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/20\n",
      "6s - loss: 5.8524 - acc: 0.5940 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 2/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 3/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 4/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 5/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 6/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 7/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 8/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 9/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 10/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 11/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 12/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 13/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 14/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 15/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 16/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 17/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 18/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 19/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Epoch 20/20\n",
      "6s - loss: 7.3653 - acc: 0.5430 - val_loss: 7.3961 - val_acc: 0.5411\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/20\n",
      "8s - loss: 0.5047 - acc: 0.8079 - val_loss: 0.6310 - val_acc: 0.7856\n",
      "Epoch 2/20\n",
      "7s - loss: 0.5854 - acc: 0.8089 - val_loss: 0.6622 - val_acc: 0.8170\n",
      "Epoch 3/20\n",
      "7s - loss: 0.5396 - acc: 0.8233 - val_loss: 0.4400 - val_acc: 0.8235\n",
      "Epoch 4/20\n",
      "7s - loss: 0.4837 - acc: 0.8285 - val_loss: 0.4664 - val_acc: 0.8252\n",
      "Epoch 5/20\n",
      "7s - loss: 0.4450 - acc: 0.8340 - val_loss: 0.4327 - val_acc: 0.8298\n",
      "Epoch 6/20\n",
      "7s - loss: 0.4641 - acc: 0.8319 - val_loss: 0.4385 - val_acc: 0.8264\n",
      "Epoch 7/20\n",
      "7s - loss: 0.4235 - acc: 0.8388 - val_loss: 0.4926 - val_acc: 0.8217\n",
      "Epoch 8/20\n",
      "7s - loss: 0.4149 - acc: 0.8369 - val_loss: 0.4374 - val_acc: 0.8243\n",
      "Epoch 9/20\n",
      "7s - loss: 0.4083 - acc: 0.8376 - val_loss: 0.4141 - val_acc: 0.8310\n",
      "Epoch 10/20\n",
      "7s - loss: 0.4132 - acc: 0.8357 - val_loss: 0.4204 - val_acc: 0.8306\n",
      "Epoch 11/20\n",
      "7s - loss: 0.4033 - acc: 0.8388 - val_loss: 0.5253 - val_acc: 0.7963\n",
      "Epoch 12/20\n",
      "7s - loss: 0.3973 - acc: 0.8388 - val_loss: 0.4351 - val_acc: 0.8231\n",
      "Epoch 13/20\n",
      "7s - loss: 0.3939 - acc: 0.8397 - val_loss: 0.4134 - val_acc: 0.8294\n",
      "Epoch 14/20\n",
      "7s - loss: 0.3892 - acc: 0.8416 - val_loss: 0.4104 - val_acc: 0.8321\n",
      "Epoch 15/20\n",
      "7s - loss: 0.3863 - acc: 0.8413 - val_loss: 0.4093 - val_acc: 0.8319\n",
      "Epoch 16/20\n",
      "7s - loss: 0.3873 - acc: 0.8430 - val_loss: 0.4024 - val_acc: 0.8320\n",
      "Epoch 17/20\n",
      "7s - loss: 0.3831 - acc: 0.8427 - val_loss: 0.4272 - val_acc: 0.8280\n",
      "Epoch 18/20\n",
      "7s - loss: 0.3860 - acc: 0.8413 - val_loss: 0.4125 - val_acc: 0.8306\n",
      "Epoch 19/20\n",
      "7s - loss: 0.3852 - acc: 0.8407 - val_loss: 0.4124 - val_acc: 0.8317\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7s - loss: 0.3854 - acc: 0.8432 - val_loss: 0.4047 - val_acc: 0.8327\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/20\n",
      "10s - loss: 0.4385 - acc: 0.8202 - val_loss: 0.4142 - val_acc: 0.8239\n",
      "Epoch 2/20\n",
      "9s - loss: 0.4129 - acc: 0.8293 - val_loss: 0.4216 - val_acc: 0.8241\n",
      "Epoch 3/20\n",
      "9s - loss: 0.4032 - acc: 0.8323 - val_loss: 0.4076 - val_acc: 0.8279\n",
      "Epoch 4/20\n",
      "9s - loss: 0.3960 - acc: 0.8356 - val_loss: 0.4098 - val_acc: 0.8249\n",
      "Epoch 5/20\n",
      "9s - loss: 0.3910 - acc: 0.8381 - val_loss: 0.4023 - val_acc: 0.8276\n",
      "Epoch 6/20\n",
      "9s - loss: 0.3869 - acc: 0.8371 - val_loss: 0.4022 - val_acc: 0.8276\n",
      "Epoch 7/20\n",
      "9s - loss: 0.3894 - acc: 0.8396 - val_loss: 0.4006 - val_acc: 0.8308\n",
      "Epoch 8/20\n",
      "9s - loss: 0.3852 - acc: 0.8408 - val_loss: 0.3953 - val_acc: 0.8303\n",
      "Epoch 9/20\n",
      "9s - loss: 0.3829 - acc: 0.8402 - val_loss: 0.4036 - val_acc: 0.8287\n",
      "Epoch 10/20\n",
      "9s - loss: 0.3800 - acc: 0.8413 - val_loss: 0.3975 - val_acc: 0.8295\n",
      "Epoch 11/20\n",
      "9s - loss: 0.3772 - acc: 0.8420 - val_loss: 0.4001 - val_acc: 0.8301\n",
      "Epoch 12/20\n",
      "9s - loss: 0.3765 - acc: 0.8433 - val_loss: 0.3945 - val_acc: 0.8334\n",
      "Epoch 13/20\n",
      "9s - loss: 0.3742 - acc: 0.8428 - val_loss: 0.3966 - val_acc: 0.8342\n",
      "Epoch 14/20\n",
      "9s - loss: 0.3772 - acc: 0.8436 - val_loss: 0.4067 - val_acc: 0.8286\n",
      "Epoch 15/20\n",
      "9s - loss: 0.3730 - acc: 0.8461 - val_loss: 0.3942 - val_acc: 0.8351\n",
      "Epoch 16/20\n",
      "9s - loss: 0.3728 - acc: 0.8441 - val_loss: 0.4014 - val_acc: 0.8328\n",
      "Epoch 17/20\n",
      "9s - loss: 0.3680 - acc: 0.8455 - val_loss: 0.3961 - val_acc: 0.8341\n",
      "Epoch 18/20\n",
      "9s - loss: 0.3651 - acc: 0.8473 - val_loss: 0.4005 - val_acc: 0.8303\n",
      "Epoch 19/20\n",
      "9s - loss: 0.3695 - acc: 0.8472 - val_loss: 0.3987 - val_acc: 0.8301\n",
      "Epoch 20/20\n",
      "9s - loss: 0.3637 - acc: 0.8486 - val_loss: 0.3979 - val_acc: 0.8344\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/20\n",
      "8s - loss: 0.4741 - acc: 0.8110 - val_loss: 0.9507 - val_acc: 0.6283\n",
      "Epoch 2/20\n",
      "7s - loss: 0.4751 - acc: 0.8205 - val_loss: 0.5203 - val_acc: 0.8251\n",
      "Epoch 3/20\n",
      "7s - loss: 0.5331 - acc: 0.8192 - val_loss: 0.5396 - val_acc: 0.8278\n",
      "Epoch 4/20\n",
      "8s - loss: 0.5265 - acc: 0.8242 - val_loss: 0.5248 - val_acc: 0.8246\n",
      "Epoch 5/20\n",
      "9s - loss: 0.5432 - acc: 0.8225 - val_loss: 0.5229 - val_acc: 0.8268\n",
      "Epoch 6/20\n",
      "8s - loss: 0.5556 - acc: 0.8228 - val_loss: 0.6872 - val_acc: 0.8218\n",
      "Epoch 7/20\n",
      "7s - loss: 0.7972 - acc: 0.8096 - val_loss: 0.8519 - val_acc: 0.8207\n",
      "Epoch 8/20\n",
      "8s - loss: 0.7920 - acc: 0.8211 - val_loss: 0.7735 - val_acc: 0.8288\n",
      "Epoch 9/20\n",
      "7s - loss: 0.8397 - acc: 0.8124 - val_loss: 0.9759 - val_acc: 0.6528\n",
      "Epoch 10/20\n",
      "7s - loss: 1.2313 - acc: 0.7848 - val_loss: 1.3604 - val_acc: 0.5706\n",
      "Epoch 11/20\n",
      "7s - loss: 0.9661 - acc: 0.7832 - val_loss: 1.1519 - val_acc: 0.7246\n",
      "Epoch 12/20\n",
      "7s - loss: 2.3110 - acc: 0.7934 - val_loss: 2.8852 - val_acc: 0.8129\n",
      "Epoch 13/20\n",
      "7s - loss: 2.8445 - acc: 0.8174 - val_loss: 2.7918 - val_acc: 0.8233\n",
      "Epoch 14/20\n",
      "7s - loss: 2.7456 - acc: 0.8255 - val_loss: 2.8276 - val_acc: 0.8216\n",
      "Epoch 15/20\n",
      "7s - loss: 2.7142 - acc: 0.8288 - val_loss: 2.8504 - val_acc: 0.8212\n",
      "Epoch 16/20\n",
      "7s - loss: 2.9096 - acc: 0.8171 - val_loss: 2.8629 - val_acc: 0.8202\n",
      "Epoch 17/20\n",
      "7s - loss: 2.7183 - acc: 0.8293 - val_loss: 2.8644 - val_acc: 0.8206\n",
      "Epoch 18/20\n",
      "7s - loss: 2.7621 - acc: 0.8264 - val_loss: 2.8181 - val_acc: 0.8234\n",
      "Epoch 19/20\n",
      "7s - loss: 2.7404 - acc: 0.8282 - val_loss: 2.8038 - val_acc: 0.8241\n",
      "Epoch 20/20\n",
      "7s - loss: 2.7215 - acc: 0.8295 - val_loss: 2.8057 - val_acc: 0.8240\n",
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/20\n",
      "9s - loss: 0.4434 - acc: 0.8189 - val_loss: 0.4156 - val_acc: 0.8248\n",
      "Epoch 2/20\n",
      "8s - loss: 0.4090 - acc: 0.8326 - val_loss: 0.4117 - val_acc: 0.8239\n",
      "Epoch 3/20\n",
      "8s - loss: 0.4007 - acc: 0.8354 - val_loss: 0.4112 - val_acc: 0.8286\n",
      "Epoch 4/20\n",
      "8s - loss: 0.3971 - acc: 0.8366 - val_loss: 0.4017 - val_acc: 0.8311\n",
      "Epoch 5/20\n",
      "8s - loss: 0.3894 - acc: 0.8401 - val_loss: 0.3985 - val_acc: 0.8291\n",
      "Epoch 6/20\n",
      "8s - loss: 0.3908 - acc: 0.8396 - val_loss: 0.4081 - val_acc: 0.8280\n",
      "Epoch 7/20\n",
      "8s - loss: 0.3895 - acc: 0.8383 - val_loss: 0.4036 - val_acc: 0.8325\n",
      "Epoch 8/20\n",
      "8s - loss: 0.3866 - acc: 0.8418 - val_loss: 0.4029 - val_acc: 0.8292\n",
      "Epoch 9/20\n",
      "8s - loss: 0.3823 - acc: 0.8415 - val_loss: 0.4365 - val_acc: 0.8316\n",
      "Epoch 10/20\n",
      "8s - loss: 0.3824 - acc: 0.8422 - val_loss: 0.4087 - val_acc: 0.8330\n",
      "Epoch 11/20\n",
      "8s - loss: 0.3761 - acc: 0.8421 - val_loss: 0.4017 - val_acc: 0.8328\n",
      "Epoch 12/20\n",
      "8s - loss: 0.3781 - acc: 0.8428 - val_loss: 0.4266 - val_acc: 0.8320\n",
      "Epoch 13/20\n",
      "8s - loss: 0.3797 - acc: 0.8424 - val_loss: 0.4135 - val_acc: 0.8320\n",
      "Epoch 14/20\n",
      "8s - loss: 0.3738 - acc: 0.8455 - val_loss: 0.4011 - val_acc: 0.8304\n",
      "Epoch 15/20\n",
      "8s - loss: 0.3795 - acc: 0.8436 - val_loss: 0.4050 - val_acc: 0.8321\n",
      "Epoch 16/20\n",
      "8s - loss: 0.3722 - acc: 0.8451 - val_loss: 0.4164 - val_acc: 0.8314\n",
      "Epoch 17/20\n",
      "8s - loss: 0.3691 - acc: 0.8447 - val_loss: 0.4079 - val_acc: 0.8322\n",
      "Epoch 18/20\n",
      "8s - loss: 0.3703 - acc: 0.8472 - val_loss: 0.3989 - val_acc: 0.8334\n",
      "Epoch 19/20\n",
      "9s - loss: 0.3682 - acc: 0.8468 - val_loss: 0.4077 - val_acc: 0.8275\n",
      "Epoch 20/20\n",
      "8s - loss: 0.3654 - acc: 0.8477 - val_loss: 0.4117 - val_acc: 0.8323\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuIAAAHwCAYAAADjFQoyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABADv4PCMY1AJwaAn4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f965092c780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtIAAAHwCAYAAACL5ogKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9650826c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tunning_lr.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras_metrics as km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = km.Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27661 samples, validate on 18441 samples\n",
      "Epoch 1/20\n",
      "- val_f1: 0.818711 - val_precision: 0.785180 - val_recall 0.855235\n",
      "12s - loss: 0.4372 - acc: 0.8176 - val_loss: 0.4128 - val_acc: 0.8262\n",
      "Epoch 2/20\n",
      "- val_f1: 0.820539 - val_precision: 0.789152 - val_recall 0.854526\n",
      "11s - loss: 0.4104 - acc: 0.8297 - val_loss: 0.4072 - val_acc: 0.8285\n",
      "Epoch 3/20\n",
      "- val_f1: 0.822105 - val_precision: 0.775542 - val_recall 0.874616\n",
      "11s - loss: 0.3994 - acc: 0.8332 - val_loss: 0.4102 - val_acc: 0.8263\n",
      "Epoch 4/20\n",
      "- val_f1: 0.820553 - val_precision: 0.784107 - val_recall 0.860553\n",
      "11s - loss: 0.3954 - acc: 0.8354 - val_loss: 0.4017 - val_acc: 0.8273\n",
      "Epoch 5/20\n",
      "- val_f1: 0.824556 - val_precision: 0.782470 - val_recall 0.871425\n",
      "11s - loss: 0.3929 - acc: 0.8366 - val_loss: 0.4153 - val_acc: 0.8298\n",
      "Epoch 6/20\n",
      "- val_f1: 0.824464 - val_precision: 0.782974 - val_recall 0.870598\n",
      "11s - loss: 0.3885 - acc: 0.8381 - val_loss: 0.4012 - val_acc: 0.8299\n",
      "Epoch 7/20\n",
      "- val_f1: 0.813086 - val_precision: 0.811125 - val_recall 0.815056\n",
      "11s - loss: 0.3846 - acc: 0.8386 - val_loss: 0.4111 - val_acc: 0.8280\n",
      "Epoch 8/20\n",
      "- val_f1: 0.820507 - val_precision: 0.802122 - val_recall 0.839754\n",
      "11s - loss: 0.3841 - acc: 0.8401 - val_loss: 0.3973 - val_acc: 0.8314\n",
      "Epoch 9/20\n",
      "- val_f1: 0.822315 - val_precision: 0.794795 - val_recall 0.851808\n",
      "11s - loss: 0.3840 - acc: 0.8396 - val_loss: 0.4001 - val_acc: 0.8311\n",
      "Epoch 10/20\n",
      "- val_f1: 0.824022 - val_precision: 0.800602 - val_recall 0.848854\n",
      "11s - loss: 0.3804 - acc: 0.8404 - val_loss: 0.3955 - val_acc: 0.8336\n",
      "Epoch 11/20\n",
      "- val_f1: 0.825350 - val_precision: 0.793115 - val_recall 0.860317\n",
      "11s - loss: 0.3789 - acc: 0.8423 - val_loss: 0.3961 - val_acc: 0.8329\n",
      "Epoch 12/20\n",
      "- val_f1: 0.826065 - val_precision: 0.794134 - val_recall 0.860671\n",
      "11s - loss: 0.3784 - acc: 0.8419 - val_loss: 0.3951 - val_acc: 0.8337\n",
      "Epoch 13/20\n",
      "- val_f1: 0.827156 - val_precision: 0.789203 - val_recall 0.868944\n",
      "11s - loss: 0.3752 - acc: 0.8417 - val_loss: 0.3971 - val_acc: 0.8334\n",
      "Epoch 14/20\n",
      "- val_f1: 0.818796 - val_precision: 0.807048 - val_recall 0.830891\n",
      "11s - loss: 0.3731 - acc: 0.8443 - val_loss: 0.3949 - val_acc: 0.8312\n",
      "Epoch 15/20\n",
      "- val_f1: 0.819879 - val_precision: 0.809491 - val_recall 0.830537\n",
      "11s - loss: 0.3736 - acc: 0.8446 - val_loss: 0.3984 - val_acc: 0.8325\n",
      "Epoch 16/20\n",
      "- val_f1: 0.825592 - val_precision: 0.796398 - val_recall 0.857008\n",
      "11s - loss: 0.3706 - acc: 0.8446 - val_loss: 0.3953 - val_acc: 0.8338\n",
      "Epoch 17/20\n",
      "- val_f1: 0.826352 - val_precision: 0.798019 - val_recall 0.856771\n",
      "11s - loss: 0.3678 - acc: 0.8464 - val_loss: 0.3942 - val_acc: 0.8348\n",
      "Epoch 18/20\n",
      "- val_f1: 0.817991 - val_precision: 0.808732 - val_recall 0.827464\n",
      "11s - loss: 0.3687 - acc: 0.8473 - val_loss: 0.3961 - val_acc: 0.8310\n",
      "Epoch 19/20\n",
      "- val_f1: 0.821577 - val_precision: 0.804279 - val_recall 0.839636\n",
      "11s - loss: 0.3695 - acc: 0.8453 - val_loss: 0.3976 - val_acc: 0.8327\n",
      "Epoch 20/20\n",
      "- val_f1: 0.826386 - val_precision: 0.789559 - val_recall 0.866816\n",
      "11s - loss: 0.3642 - acc: 0.8484 - val_loss: 0.3978 - val_acc: 0.8329\n"
     ]
    }
   ],
   "source": [
    "# we get the best model:\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "\t\t\t\t\t  optimizer=Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0),\n",
    "\t\t\t\t\t  metrics=['accuracy'])\n",
    "history_adadelta = model.fit(x_train, y_train, \n",
    "                     validation_data=(x_test, y_test), \n",
    "                     epochs=20, \n",
    "                     batch_size=64,\n",
    "                     verbose=2,callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(his.history['loss'])\n",
    "plt.plot(his.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(his.history['loss'])\n",
    "plt.plot(his.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(his.history['loss'])\n",
    "plt.plot(his.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(his.history['acc'])\n",
    "plt.plot(his.history['val_acc'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 另一个版本的model\n",
    "\n",
    "{'Dense': 0,\n",
    " 'Dense_1': 1,\n",
    " 'Dropout': 0.2283813112902432,\n",
    " 'Dropout_1': 0.5422412690636627,\n",
    " 'Dropout_2': 0.33351393608141355,\n",
    " 'add': 1,\n",
    " 'batch_size': 0,\n",
    " 'conditional': 1,\n",
    " 'optimizer': 3}\n",
    "\n",
    "\n",
    "# 确定参数后，调整epochs个数\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(311,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.422))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.329))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 输出层\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "print (model.summary())\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'],\n",
    "    optimizer='rmsprop')\n",
    "\n",
    "his = model.fit(x_train, y_train, batch_size=64,epochs=200, verbose=2, validation_data=(x_test, y_test))\n",
    "# score, acc = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_eval = lgb.Dataset(x_test, y_test, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss', 'auc'},\n",
    "    'num_leaves': 5,\n",
    "    'max_depth': 6,\n",
    "    'min_data_in_leaf': 450,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction':0.9,\n",
    "    'bagging_fraction':0.95,\n",
    "    'bagging_freq':5,\n",
    "    'lambda_l1':1,\n",
    "    'lambda_l2':0.001,\n",
    "    'min_gain_to_split':0.2,\n",
    "    'verbose':2,\n",
    "    'is_unbalance': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.65101\tvalid_0's auc: 0.858713\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.61619\tvalid_0's auc: 0.86708\n",
      "[3]\tvalid_0's binary_logloss: 0.587686\tvalid_0's auc: 0.866309\n",
      "[4]\tvalid_0's binary_logloss: 0.563889\tvalid_0's auc: 0.872553\n",
      "[5]\tvalid_0's binary_logloss: 0.543974\tvalid_0's auc: 0.875199\n",
      "[6]\tvalid_0's binary_logloss: 0.526913\tvalid_0's auc: 0.87608\n",
      "[7]\tvalid_0's binary_logloss: 0.512376\tvalid_0's auc: 0.881666\n",
      "[8]\tvalid_0's binary_logloss: 0.499877\tvalid_0's auc: 0.883065\n",
      "[9]\tvalid_0's binary_logloss: 0.489199\tvalid_0's auc: 0.88349\n",
      "[10]\tvalid_0's binary_logloss: 0.479978\tvalid_0's auc: 0.883352\n",
      "[11]\tvalid_0's binary_logloss: 0.47216\tvalid_0's auc: 0.884855\n",
      "[12]\tvalid_0's binary_logloss: 0.465136\tvalid_0's auc: 0.886661\n",
      "[13]\tvalid_0's binary_logloss: 0.459098\tvalid_0's auc: 0.887461\n",
      "[14]\tvalid_0's binary_logloss: 0.453447\tvalid_0's auc: 0.88721\n",
      "[15]\tvalid_0's binary_logloss: 0.448344\tvalid_0's auc: 0.889037\n",
      "[16]\tvalid_0's binary_logloss: 0.444098\tvalid_0's auc: 0.889848\n",
      "[17]\tvalid_0's binary_logloss: 0.440233\tvalid_0's auc: 0.890216\n",
      "[18]\tvalid_0's binary_logloss: 0.437025\tvalid_0's auc: 0.89072\n",
      "[19]\tvalid_0's binary_logloss: 0.433955\tvalid_0's auc: 0.891241\n",
      "[20]\tvalid_0's binary_logloss: 0.431125\tvalid_0's auc: 0.891676\n",
      "[21]\tvalid_0's binary_logloss: 0.428673\tvalid_0's auc: 0.892049\n",
      "[22]\tvalid_0's binary_logloss: 0.426374\tvalid_0's auc: 0.892127\n",
      "[23]\tvalid_0's binary_logloss: 0.424191\tvalid_0's auc: 0.892441\n",
      "[24]\tvalid_0's binary_logloss: 0.422396\tvalid_0's auc: 0.892715\n",
      "[25]\tvalid_0's binary_logloss: 0.420613\tvalid_0's auc: 0.892947\n",
      "[26]\tvalid_0's binary_logloss: 0.419014\tvalid_0's auc: 0.893632\n",
      "[27]\tvalid_0's binary_logloss: 0.417377\tvalid_0's auc: 0.894272\n",
      "[28]\tvalid_0's binary_logloss: 0.415465\tvalid_0's auc: 0.895281\n",
      "[29]\tvalid_0's binary_logloss: 0.414324\tvalid_0's auc: 0.89556\n",
      "[30]\tvalid_0's binary_logloss: 0.413081\tvalid_0's auc: 0.896122\n",
      "[31]\tvalid_0's binary_logloss: 0.411962\tvalid_0's auc: 0.896427\n",
      "[32]\tvalid_0's binary_logloss: 0.410717\tvalid_0's auc: 0.896856\n",
      "[33]\tvalid_0's binary_logloss: 0.409698\tvalid_0's auc: 0.896883\n",
      "[34]\tvalid_0's binary_logloss: 0.408747\tvalid_0's auc: 0.89719\n",
      "[35]\tvalid_0's binary_logloss: 0.407632\tvalid_0's auc: 0.897578\n",
      "[36]\tvalid_0's binary_logloss: 0.406631\tvalid_0's auc: 0.89786\n",
      "[37]\tvalid_0's binary_logloss: 0.405726\tvalid_0's auc: 0.898059\n",
      "[38]\tvalid_0's binary_logloss: 0.404985\tvalid_0's auc: 0.898248\n",
      "[39]\tvalid_0's binary_logloss: 0.404327\tvalid_0's auc: 0.898451\n",
      "[40]\tvalid_0's binary_logloss: 0.403637\tvalid_0's auc: 0.898678\n",
      "[41]\tvalid_0's binary_logloss: 0.403193\tvalid_0's auc: 0.898797\n",
      "[42]\tvalid_0's binary_logloss: 0.40246\tvalid_0's auc: 0.899004\n",
      "[43]\tvalid_0's binary_logloss: 0.402019\tvalid_0's auc: 0.899231\n",
      "[44]\tvalid_0's binary_logloss: 0.401374\tvalid_0's auc: 0.899561\n",
      "[45]\tvalid_0's binary_logloss: 0.400709\tvalid_0's auc: 0.89978\n",
      "[46]\tvalid_0's binary_logloss: 0.400079\tvalid_0's auc: 0.900058\n",
      "[47]\tvalid_0's binary_logloss: 0.399235\tvalid_0's auc: 0.900397\n",
      "[48]\tvalid_0's binary_logloss: 0.398683\tvalid_0's auc: 0.90057\n",
      "[49]\tvalid_0's binary_logloss: 0.398218\tvalid_0's auc: 0.900724\n",
      "[50]\tvalid_0's binary_logloss: 0.397903\tvalid_0's auc: 0.900813\n",
      "[51]\tvalid_0's binary_logloss: 0.397452\tvalid_0's auc: 0.901019\n",
      "[52]\tvalid_0's binary_logloss: 0.396959\tvalid_0's auc: 0.901128\n",
      "[53]\tvalid_0's binary_logloss: 0.396564\tvalid_0's auc: 0.901278\n",
      "[54]\tvalid_0's binary_logloss: 0.396222\tvalid_0's auc: 0.901359\n",
      "[55]\tvalid_0's binary_logloss: 0.395725\tvalid_0's auc: 0.901556\n",
      "[56]\tvalid_0's binary_logloss: 0.395417\tvalid_0's auc: 0.901621\n",
      "[57]\tvalid_0's binary_logloss: 0.395202\tvalid_0's auc: 0.901707\n",
      "[58]\tvalid_0's binary_logloss: 0.39484\tvalid_0's auc: 0.901789\n",
      "[59]\tvalid_0's binary_logloss: 0.394527\tvalid_0's auc: 0.90189\n",
      "[60]\tvalid_0's binary_logloss: 0.394189\tvalid_0's auc: 0.901961\n",
      "[61]\tvalid_0's binary_logloss: 0.393965\tvalid_0's auc: 0.901995\n",
      "[62]\tvalid_0's binary_logloss: 0.39381\tvalid_0's auc: 0.90202\n",
      "[63]\tvalid_0's binary_logloss: 0.393445\tvalid_0's auc: 0.90217\n",
      "[64]\tvalid_0's binary_logloss: 0.393126\tvalid_0's auc: 0.90231\n",
      "[65]\tvalid_0's binary_logloss: 0.392896\tvalid_0's auc: 0.902355\n",
      "[66]\tvalid_0's binary_logloss: 0.392474\tvalid_0's auc: 0.902511\n",
      "[67]\tvalid_0's binary_logloss: 0.392182\tvalid_0's auc: 0.902577\n",
      "[68]\tvalid_0's binary_logloss: 0.392024\tvalid_0's auc: 0.902602\n",
      "[69]\tvalid_0's binary_logloss: 0.391885\tvalid_0's auc: 0.902642\n",
      "[70]\tvalid_0's binary_logloss: 0.391576\tvalid_0's auc: 0.902726\n",
      "[71]\tvalid_0's binary_logloss: 0.391398\tvalid_0's auc: 0.902804\n",
      "[72]\tvalid_0's binary_logloss: 0.391215\tvalid_0's auc: 0.902857\n",
      "[73]\tvalid_0's binary_logloss: 0.391175\tvalid_0's auc: 0.902853\n",
      "[74]\tvalid_0's binary_logloss: 0.390873\tvalid_0's auc: 0.902968\n",
      "[75]\tvalid_0's binary_logloss: 0.390713\tvalid_0's auc: 0.903032\n",
      "[76]\tvalid_0's binary_logloss: 0.390416\tvalid_0's auc: 0.90315\n",
      "[77]\tvalid_0's binary_logloss: 0.390166\tvalid_0's auc: 0.903276\n",
      "[78]\tvalid_0's binary_logloss: 0.389842\tvalid_0's auc: 0.903415\n",
      "[79]\tvalid_0's binary_logloss: 0.38977\tvalid_0's auc: 0.903471\n",
      "[80]\tvalid_0's binary_logloss: 0.389585\tvalid_0's auc: 0.903546\n",
      "[81]\tvalid_0's binary_logloss: 0.389328\tvalid_0's auc: 0.903623\n",
      "[82]\tvalid_0's binary_logloss: 0.389118\tvalid_0's auc: 0.903702\n",
      "[83]\tvalid_0's binary_logloss: 0.388948\tvalid_0's auc: 0.903738\n",
      "[84]\tvalid_0's binary_logloss: 0.388834\tvalid_0's auc: 0.903753\n",
      "[85]\tvalid_0's binary_logloss: 0.388635\tvalid_0's auc: 0.903851\n",
      "[86]\tvalid_0's binary_logloss: 0.388475\tvalid_0's auc: 0.903903\n",
      "[87]\tvalid_0's binary_logloss: 0.388314\tvalid_0's auc: 0.903954\n",
      "[88]\tvalid_0's binary_logloss: 0.388059\tvalid_0's auc: 0.904068\n",
      "[89]\tvalid_0's binary_logloss: 0.387846\tvalid_0's auc: 0.904136\n",
      "[90]\tvalid_0's binary_logloss: 0.387711\tvalid_0's auc: 0.904169\n",
      "[91]\tvalid_0's binary_logloss: 0.387592\tvalid_0's auc: 0.904213\n",
      "[92]\tvalid_0's binary_logloss: 0.387479\tvalid_0's auc: 0.904254\n",
      "[93]\tvalid_0's binary_logloss: 0.38736\tvalid_0's auc: 0.904305\n",
      "[94]\tvalid_0's binary_logloss: 0.387285\tvalid_0's auc: 0.904322\n",
      "[95]\tvalid_0's binary_logloss: 0.387195\tvalid_0's auc: 0.904371\n",
      "[96]\tvalid_0's binary_logloss: 0.387166\tvalid_0's auc: 0.904345\n",
      "[97]\tvalid_0's binary_logloss: 0.387066\tvalid_0's auc: 0.904369\n",
      "[98]\tvalid_0's binary_logloss: 0.386732\tvalid_0's auc: 0.904496\n",
      "[99]\tvalid_0's binary_logloss: 0.38665\tvalid_0's auc: 0.904518\n",
      "[100]\tvalid_0's binary_logloss: 0.386487\tvalid_0's auc: 0.904558\n",
      "[101]\tvalid_0's binary_logloss: 0.386317\tvalid_0's auc: 0.904656\n",
      "[102]\tvalid_0's binary_logloss: 0.386236\tvalid_0's auc: 0.904689\n",
      "[103]\tvalid_0's binary_logloss: 0.386105\tvalid_0's auc: 0.904764\n",
      "[104]\tvalid_0's binary_logloss: 0.385969\tvalid_0's auc: 0.90483\n",
      "[105]\tvalid_0's binary_logloss: 0.38588\tvalid_0's auc: 0.904884\n",
      "[106]\tvalid_0's binary_logloss: 0.385779\tvalid_0's auc: 0.90497\n",
      "[107]\tvalid_0's binary_logloss: 0.385681\tvalid_0's auc: 0.904985\n",
      "[108]\tvalid_0's binary_logloss: 0.385574\tvalid_0's auc: 0.905047\n",
      "[109]\tvalid_0's binary_logloss: 0.385486\tvalid_0's auc: 0.905072\n",
      "[110]\tvalid_0's binary_logloss: 0.385437\tvalid_0's auc: 0.905082\n",
      "[111]\tvalid_0's binary_logloss: 0.385355\tvalid_0's auc: 0.905091\n",
      "[112]\tvalid_0's binary_logloss: 0.385083\tvalid_0's auc: 0.905239\n",
      "[113]\tvalid_0's binary_logloss: 0.38505\tvalid_0's auc: 0.905257\n",
      "[114]\tvalid_0's binary_logloss: 0.384966\tvalid_0's auc: 0.905292\n",
      "[115]\tvalid_0's binary_logloss: 0.384769\tvalid_0's auc: 0.905378\n",
      "[116]\tvalid_0's binary_logloss: 0.384656\tvalid_0's auc: 0.905434\n",
      "[117]\tvalid_0's binary_logloss: 0.384545\tvalid_0's auc: 0.905477\n",
      "[118]\tvalid_0's binary_logloss: 0.384466\tvalid_0's auc: 0.905509\n",
      "[119]\tvalid_0's binary_logloss: 0.384387\tvalid_0's auc: 0.905562\n",
      "[120]\tvalid_0's binary_logloss: 0.384308\tvalid_0's auc: 0.905606\n",
      "[121]\tvalid_0's binary_logloss: 0.384202\tvalid_0's auc: 0.905663\n",
      "[122]\tvalid_0's binary_logloss: 0.384013\tvalid_0's auc: 0.905757\n",
      "[123]\tvalid_0's binary_logloss: 0.38394\tvalid_0's auc: 0.905809\n",
      "[124]\tvalid_0's binary_logloss: 0.383896\tvalid_0's auc: 0.905826\n",
      "[125]\tvalid_0's binary_logloss: 0.383879\tvalid_0's auc: 0.905831\n",
      "[126]\tvalid_0's binary_logloss: 0.383737\tvalid_0's auc: 0.905899\n",
      "[127]\tvalid_0's binary_logloss: 0.383717\tvalid_0's auc: 0.905913\n",
      "[128]\tvalid_0's binary_logloss: 0.383685\tvalid_0's auc: 0.905907\n",
      "[129]\tvalid_0's binary_logloss: 0.383573\tvalid_0's auc: 0.905961\n",
      "[130]\tvalid_0's binary_logloss: 0.38341\tvalid_0's auc: 0.906029\n",
      "[131]\tvalid_0's binary_logloss: 0.383375\tvalid_0's auc: 0.906059\n",
      "[132]\tvalid_0's binary_logloss: 0.383263\tvalid_0's auc: 0.906083\n",
      "[133]\tvalid_0's binary_logloss: 0.383179\tvalid_0's auc: 0.906116\n",
      "[134]\tvalid_0's binary_logloss: 0.38309\tvalid_0's auc: 0.90614\n",
      "[135]\tvalid_0's binary_logloss: 0.383073\tvalid_0's auc: 0.906137\n",
      "[136]\tvalid_0's binary_logloss: 0.383077\tvalid_0's auc: 0.906133\n",
      "[137]\tvalid_0's binary_logloss: 0.383031\tvalid_0's auc: 0.906146\n",
      "[138]\tvalid_0's binary_logloss: 0.38302\tvalid_0's auc: 0.906144\n",
      "[139]\tvalid_0's binary_logloss: 0.383016\tvalid_0's auc: 0.906141\n",
      "[140]\tvalid_0's binary_logloss: 0.38294\tvalid_0's auc: 0.906178\n",
      "[141]\tvalid_0's binary_logloss: 0.382818\tvalid_0's auc: 0.906283\n",
      "[142]\tvalid_0's binary_logloss: 0.382725\tvalid_0's auc: 0.906326\n",
      "[143]\tvalid_0's binary_logloss: 0.382712\tvalid_0's auc: 0.906334\n",
      "[144]\tvalid_0's binary_logloss: 0.382736\tvalid_0's auc: 0.906323\n",
      "[145]\tvalid_0's binary_logloss: 0.382674\tvalid_0's auc: 0.906353\n",
      "[146]\tvalid_0's binary_logloss: 0.382679\tvalid_0's auc: 0.90636\n",
      "[147]\tvalid_0's binary_logloss: 0.38257\tvalid_0's auc: 0.90641\n",
      "[148]\tvalid_0's binary_logloss: 0.382591\tvalid_0's auc: 0.906414\n",
      "[149]\tvalid_0's binary_logloss: 0.382499\tvalid_0's auc: 0.906458\n",
      "[150]\tvalid_0's binary_logloss: 0.382475\tvalid_0's auc: 0.906463\n",
      "[151]\tvalid_0's binary_logloss: 0.382335\tvalid_0's auc: 0.906525\n",
      "[152]\tvalid_0's binary_logloss: 0.382267\tvalid_0's auc: 0.906584\n",
      "[153]\tvalid_0's binary_logloss: 0.382197\tvalid_0's auc: 0.906602\n",
      "[154]\tvalid_0's binary_logloss: 0.382115\tvalid_0's auc: 0.906617\n",
      "[155]\tvalid_0's binary_logloss: 0.382063\tvalid_0's auc: 0.906653\n",
      "[156]\tvalid_0's binary_logloss: 0.382035\tvalid_0's auc: 0.90667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157]\tvalid_0's binary_logloss: 0.38207\tvalid_0's auc: 0.906658\n",
      "[158]\tvalid_0's binary_logloss: 0.382025\tvalid_0's auc: 0.906681\n",
      "[159]\tvalid_0's binary_logloss: 0.381928\tvalid_0's auc: 0.906721\n",
      "[160]\tvalid_0's binary_logloss: 0.381892\tvalid_0's auc: 0.906735\n",
      "[161]\tvalid_0's binary_logloss: 0.381823\tvalid_0's auc: 0.906752\n",
      "[162]\tvalid_0's binary_logloss: 0.381761\tvalid_0's auc: 0.906777\n",
      "[163]\tvalid_0's binary_logloss: 0.381716\tvalid_0's auc: 0.906784\n",
      "[164]\tvalid_0's binary_logloss: 0.381624\tvalid_0's auc: 0.90682\n",
      "[165]\tvalid_0's binary_logloss: 0.381615\tvalid_0's auc: 0.906832\n",
      "[166]\tvalid_0's binary_logloss: 0.381597\tvalid_0's auc: 0.906836\n",
      "[167]\tvalid_0's binary_logloss: 0.381541\tvalid_0's auc: 0.906858\n",
      "[168]\tvalid_0's binary_logloss: 0.381423\tvalid_0's auc: 0.906914\n",
      "[169]\tvalid_0's binary_logloss: 0.381361\tvalid_0's auc: 0.906941\n",
      "[170]\tvalid_0's binary_logloss: 0.38129\tvalid_0's auc: 0.906978\n",
      "[171]\tvalid_0's binary_logloss: 0.381304\tvalid_0's auc: 0.906994\n",
      "[172]\tvalid_0's binary_logloss: 0.381255\tvalid_0's auc: 0.907017\n",
      "[173]\tvalid_0's binary_logloss: 0.381208\tvalid_0's auc: 0.907042\n",
      "[174]\tvalid_0's binary_logloss: 0.381223\tvalid_0's auc: 0.907028\n",
      "[175]\tvalid_0's binary_logloss: 0.381166\tvalid_0's auc: 0.907059\n",
      "[176]\tvalid_0's binary_logloss: 0.38119\tvalid_0's auc: 0.90705\n",
      "[177]\tvalid_0's binary_logloss: 0.38107\tvalid_0's auc: 0.907093\n",
      "[178]\tvalid_0's binary_logloss: 0.381008\tvalid_0's auc: 0.907124\n",
      "[179]\tvalid_0's binary_logloss: 0.380986\tvalid_0's auc: 0.907145\n",
      "[180]\tvalid_0's binary_logloss: 0.380988\tvalid_0's auc: 0.907133\n",
      "[181]\tvalid_0's binary_logloss: 0.381002\tvalid_0's auc: 0.907123\n",
      "[182]\tvalid_0's binary_logloss: 0.380976\tvalid_0's auc: 0.907142\n",
      "[183]\tvalid_0's binary_logloss: 0.380851\tvalid_0's auc: 0.907233\n",
      "[184]\tvalid_0's binary_logloss: 0.380843\tvalid_0's auc: 0.907233\n",
      "[185]\tvalid_0's binary_logloss: 0.380689\tvalid_0's auc: 0.907302\n",
      "[186]\tvalid_0's binary_logloss: 0.380662\tvalid_0's auc: 0.907318\n",
      "[187]\tvalid_0's binary_logloss: 0.380591\tvalid_0's auc: 0.907344\n",
      "[188]\tvalid_0's binary_logloss: 0.380556\tvalid_0's auc: 0.907356\n",
      "[189]\tvalid_0's binary_logloss: 0.380441\tvalid_0's auc: 0.907413\n",
      "[190]\tvalid_0's binary_logloss: 0.380394\tvalid_0's auc: 0.907447\n",
      "[191]\tvalid_0's binary_logloss: 0.380298\tvalid_0's auc: 0.907503\n",
      "[192]\tvalid_0's binary_logloss: 0.380202\tvalid_0's auc: 0.90754\n",
      "[193]\tvalid_0's binary_logloss: 0.380191\tvalid_0's auc: 0.907545\n",
      "[194]\tvalid_0's binary_logloss: 0.380186\tvalid_0's auc: 0.907546\n",
      "[195]\tvalid_0's binary_logloss: 0.380137\tvalid_0's auc: 0.907562\n",
      "[196]\tvalid_0's binary_logloss: 0.380069\tvalid_0's auc: 0.907591\n",
      "[197]\tvalid_0's binary_logloss: 0.37992\tvalid_0's auc: 0.907651\n",
      "[198]\tvalid_0's binary_logloss: 0.379825\tvalid_0's auc: 0.907684\n",
      "[199]\tvalid_0's binary_logloss: 0.379807\tvalid_0's auc: 0.907689\n",
      "[200]\tvalid_0's binary_logloss: 0.379745\tvalid_0's auc: 0.907726\n",
      "[201]\tvalid_0's binary_logloss: 0.379731\tvalid_0's auc: 0.907725\n",
      "[202]\tvalid_0's binary_logloss: 0.379718\tvalid_0's auc: 0.907728\n",
      "[203]\tvalid_0's binary_logloss: 0.379638\tvalid_0's auc: 0.907768\n",
      "[204]\tvalid_0's binary_logloss: 0.379593\tvalid_0's auc: 0.907792\n",
      "[205]\tvalid_0's binary_logloss: 0.379528\tvalid_0's auc: 0.907813\n",
      "[206]\tvalid_0's binary_logloss: 0.37954\tvalid_0's auc: 0.907801\n",
      "[207]\tvalid_0's binary_logloss: 0.379548\tvalid_0's auc: 0.907794\n",
      "[208]\tvalid_0's binary_logloss: 0.379515\tvalid_0's auc: 0.907803\n",
      "[209]\tvalid_0's binary_logloss: 0.379531\tvalid_0's auc: 0.907791\n",
      "[210]\tvalid_0's binary_logloss: 0.379533\tvalid_0's auc: 0.907795\n",
      "[211]\tvalid_0's binary_logloss: 0.379478\tvalid_0's auc: 0.907826\n",
      "[212]\tvalid_0's binary_logloss: 0.379419\tvalid_0's auc: 0.907878\n",
      "[213]\tvalid_0's binary_logloss: 0.379398\tvalid_0's auc: 0.907876\n",
      "[214]\tvalid_0's binary_logloss: 0.379395\tvalid_0's auc: 0.907871\n",
      "[215]\tvalid_0's binary_logloss: 0.37939\tvalid_0's auc: 0.907874\n",
      "[216]\tvalid_0's binary_logloss: 0.379434\tvalid_0's auc: 0.907851\n",
      "[217]\tvalid_0's binary_logloss: 0.379446\tvalid_0's auc: 0.907842\n",
      "[218]\tvalid_0's binary_logloss: 0.37942\tvalid_0's auc: 0.90785\n",
      "[219]\tvalid_0's binary_logloss: 0.379379\tvalid_0's auc: 0.907877\n",
      "[220]\tvalid_0's binary_logloss: 0.379417\tvalid_0's auc: 0.907867\n",
      "[221]\tvalid_0's binary_logloss: 0.379391\tvalid_0's auc: 0.907877\n",
      "[222]\tvalid_0's binary_logloss: 0.379404\tvalid_0's auc: 0.907867\n",
      "[223]\tvalid_0's binary_logloss: 0.379381\tvalid_0's auc: 0.907868\n",
      "[224]\tvalid_0's binary_logloss: 0.37926\tvalid_0's auc: 0.907921\n",
      "[225]\tvalid_0's binary_logloss: 0.379227\tvalid_0's auc: 0.907946\n",
      "[226]\tvalid_0's binary_logloss: 0.379147\tvalid_0's auc: 0.907978\n",
      "[227]\tvalid_0's binary_logloss: 0.379095\tvalid_0's auc: 0.908003\n",
      "[228]\tvalid_0's binary_logloss: 0.379074\tvalid_0's auc: 0.908018\n",
      "[229]\tvalid_0's binary_logloss: 0.37907\tvalid_0's auc: 0.908015\n",
      "[230]\tvalid_0's binary_logloss: 0.379061\tvalid_0's auc: 0.908018\n",
      "[231]\tvalid_0's binary_logloss: 0.379027\tvalid_0's auc: 0.908044\n",
      "[232]\tvalid_0's binary_logloss: 0.379018\tvalid_0's auc: 0.908045\n",
      "[233]\tvalid_0's binary_logloss: 0.379011\tvalid_0's auc: 0.908051\n",
      "[234]\tvalid_0's binary_logloss: 0.378986\tvalid_0's auc: 0.90807\n",
      "[235]\tvalid_0's binary_logloss: 0.378897\tvalid_0's auc: 0.90811\n",
      "[236]\tvalid_0's binary_logloss: 0.378825\tvalid_0's auc: 0.908139\n",
      "[237]\tvalid_0's binary_logloss: 0.378763\tvalid_0's auc: 0.908162\n",
      "[238]\tvalid_0's binary_logloss: 0.378662\tvalid_0's auc: 0.908212\n",
      "[239]\tvalid_0's binary_logloss: 0.378619\tvalid_0's auc: 0.908225\n",
      "[240]\tvalid_0's binary_logloss: 0.378621\tvalid_0's auc: 0.908227\n",
      "[241]\tvalid_0's binary_logloss: 0.378551\tvalid_0's auc: 0.908258\n",
      "[242]\tvalid_0's binary_logloss: 0.378495\tvalid_0's auc: 0.908284\n",
      "[243]\tvalid_0's binary_logloss: 0.378418\tvalid_0's auc: 0.908322\n",
      "[244]\tvalid_0's binary_logloss: 0.378392\tvalid_0's auc: 0.908339\n",
      "[245]\tvalid_0's binary_logloss: 0.378377\tvalid_0's auc: 0.908341\n",
      "[246]\tvalid_0's binary_logloss: 0.378294\tvalid_0's auc: 0.90839\n",
      "[247]\tvalid_0's binary_logloss: 0.378258\tvalid_0's auc: 0.908404\n",
      "[248]\tvalid_0's binary_logloss: 0.378107\tvalid_0's auc: 0.908484\n",
      "[249]\tvalid_0's binary_logloss: 0.378106\tvalid_0's auc: 0.908488\n",
      "[250]\tvalid_0's binary_logloss: 0.378114\tvalid_0's auc: 0.908488\n",
      "[251]\tvalid_0's binary_logloss: 0.378096\tvalid_0's auc: 0.908503\n",
      "[252]\tvalid_0's binary_logloss: 0.37809\tvalid_0's auc: 0.908515\n",
      "[253]\tvalid_0's binary_logloss: 0.378077\tvalid_0's auc: 0.90852\n",
      "[254]\tvalid_0's binary_logloss: 0.378094\tvalid_0's auc: 0.90851\n",
      "[255]\tvalid_0's binary_logloss: 0.378059\tvalid_0's auc: 0.908541\n",
      "[256]\tvalid_0's binary_logloss: 0.377953\tvalid_0's auc: 0.908596\n",
      "[257]\tvalid_0's binary_logloss: 0.377959\tvalid_0's auc: 0.908606\n",
      "[258]\tvalid_0's binary_logloss: 0.377975\tvalid_0's auc: 0.908594\n",
      "[259]\tvalid_0's binary_logloss: 0.377963\tvalid_0's auc: 0.908595\n",
      "[260]\tvalid_0's binary_logloss: 0.377917\tvalid_0's auc: 0.908608\n",
      "[261]\tvalid_0's binary_logloss: 0.377906\tvalid_0's auc: 0.908619\n",
      "[262]\tvalid_0's binary_logloss: 0.377887\tvalid_0's auc: 0.908612\n",
      "[263]\tvalid_0's binary_logloss: 0.377852\tvalid_0's auc: 0.908632\n",
      "[264]\tvalid_0's binary_logloss: 0.377875\tvalid_0's auc: 0.908619\n",
      "[265]\tvalid_0's binary_logloss: 0.377871\tvalid_0's auc: 0.908636\n",
      "[266]\tvalid_0's binary_logloss: 0.37785\tvalid_0's auc: 0.908646\n",
      "[267]\tvalid_0's binary_logloss: 0.377793\tvalid_0's auc: 0.908668\n",
      "[268]\tvalid_0's binary_logloss: 0.377793\tvalid_0's auc: 0.908668\n",
      "[269]\tvalid_0's binary_logloss: 0.377773\tvalid_0's auc: 0.908681\n",
      "[270]\tvalid_0's binary_logloss: 0.37774\tvalid_0's auc: 0.908684\n",
      "[271]\tvalid_0's binary_logloss: 0.377722\tvalid_0's auc: 0.908692\n",
      "[272]\tvalid_0's binary_logloss: 0.377657\tvalid_0's auc: 0.908718\n",
      "[273]\tvalid_0's binary_logloss: 0.377666\tvalid_0's auc: 0.908718\n",
      "[274]\tvalid_0's binary_logloss: 0.377638\tvalid_0's auc: 0.908727\n",
      "[275]\tvalid_0's binary_logloss: 0.377674\tvalid_0's auc: 0.908706\n",
      "[276]\tvalid_0's binary_logloss: 0.377624\tvalid_0's auc: 0.908745\n",
      "[277]\tvalid_0's binary_logloss: 0.3776\tvalid_0's auc: 0.908754\n",
      "[278]\tvalid_0's binary_logloss: 0.37755\tvalid_0's auc: 0.90878\n",
      "[279]\tvalid_0's binary_logloss: 0.3775\tvalid_0's auc: 0.90881\n",
      "[280]\tvalid_0's binary_logloss: 0.377435\tvalid_0's auc: 0.908855\n",
      "[281]\tvalid_0's binary_logloss: 0.377315\tvalid_0's auc: 0.908926\n",
      "[282]\tvalid_0's binary_logloss: 0.377299\tvalid_0's auc: 0.908937\n",
      "[283]\tvalid_0's binary_logloss: 0.377258\tvalid_0's auc: 0.908957\n",
      "[284]\tvalid_0's binary_logloss: 0.377244\tvalid_0's auc: 0.90897\n",
      "[285]\tvalid_0's binary_logloss: 0.377217\tvalid_0's auc: 0.908992\n",
      "[286]\tvalid_0's binary_logloss: 0.377259\tvalid_0's auc: 0.908975\n",
      "[287]\tvalid_0's binary_logloss: 0.377312\tvalid_0's auc: 0.908942\n",
      "[288]\tvalid_0's binary_logloss: 0.377276\tvalid_0's auc: 0.908961\n",
      "[289]\tvalid_0's binary_logloss: 0.377223\tvalid_0's auc: 0.908985\n",
      "[290]\tvalid_0's binary_logloss: 0.377246\tvalid_0's auc: 0.908982\n",
      "[291]\tvalid_0's binary_logloss: 0.377248\tvalid_0's auc: 0.908982\n",
      "[292]\tvalid_0's binary_logloss: 0.377262\tvalid_0's auc: 0.908974\n",
      "[293]\tvalid_0's binary_logloss: 0.377191\tvalid_0's auc: 0.909015\n",
      "[294]\tvalid_0's binary_logloss: 0.377164\tvalid_0's auc: 0.90903\n",
      "[295]\tvalid_0's binary_logloss: 0.377129\tvalid_0's auc: 0.909047\n",
      "[296]\tvalid_0's binary_logloss: 0.377132\tvalid_0's auc: 0.90905\n",
      "[297]\tvalid_0's binary_logloss: 0.377113\tvalid_0's auc: 0.909062\n",
      "[298]\tvalid_0's binary_logloss: 0.377062\tvalid_0's auc: 0.909081\n",
      "[299]\tvalid_0's binary_logloss: 0.377064\tvalid_0's auc: 0.909074\n",
      "[300]\tvalid_0's binary_logloss: 0.377059\tvalid_0's auc: 0.909079\n",
      "[301]\tvalid_0's binary_logloss: 0.377025\tvalid_0's auc: 0.909097\n",
      "[302]\tvalid_0's binary_logloss: 0.377003\tvalid_0's auc: 0.909106\n",
      "[303]\tvalid_0's binary_logloss: 0.376893\tvalid_0's auc: 0.909152\n",
      "[304]\tvalid_0's binary_logloss: 0.376804\tvalid_0's auc: 0.909194\n",
      "[305]\tvalid_0's binary_logloss: 0.376705\tvalid_0's auc: 0.909252\n",
      "[306]\tvalid_0's binary_logloss: 0.376675\tvalid_0's auc: 0.909269\n",
      "[307]\tvalid_0's binary_logloss: 0.376611\tvalid_0's auc: 0.909307\n",
      "[308]\tvalid_0's binary_logloss: 0.37651\tvalid_0's auc: 0.909342\n",
      "[309]\tvalid_0's binary_logloss: 0.37646\tvalid_0's auc: 0.909368\n",
      "[310]\tvalid_0's binary_logloss: 0.376444\tvalid_0's auc: 0.909376\n",
      "[311]\tvalid_0's binary_logloss: 0.376427\tvalid_0's auc: 0.909383\n",
      "[312]\tvalid_0's binary_logloss: 0.376424\tvalid_0's auc: 0.909386\n",
      "[313]\tvalid_0's binary_logloss: 0.376422\tvalid_0's auc: 0.909388\n",
      "[314]\tvalid_0's binary_logloss: 0.376413\tvalid_0's auc: 0.90939\n",
      "[315]\tvalid_0's binary_logloss: 0.376442\tvalid_0's auc: 0.909372\n",
      "[316]\tvalid_0's binary_logloss: 0.376402\tvalid_0's auc: 0.909383\n",
      "[317]\tvalid_0's binary_logloss: 0.376378\tvalid_0's auc: 0.90939\n",
      "[318]\tvalid_0's binary_logloss: 0.376364\tvalid_0's auc: 0.909403\n",
      "[319]\tvalid_0's binary_logloss: 0.37636\tvalid_0's auc: 0.909404\n",
      "[320]\tvalid_0's binary_logloss: 0.376342\tvalid_0's auc: 0.909412\n",
      "[321]\tvalid_0's binary_logloss: 0.376272\tvalid_0's auc: 0.909442\n",
      "[322]\tvalid_0's binary_logloss: 0.376293\tvalid_0's auc: 0.90944\n",
      "[323]\tvalid_0's binary_logloss: 0.376237\tvalid_0's auc: 0.909465\n",
      "[324]\tvalid_0's binary_logloss: 0.376231\tvalid_0's auc: 0.909474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[325]\tvalid_0's binary_logloss: 0.376232\tvalid_0's auc: 0.909475\n",
      "[326]\tvalid_0's binary_logloss: 0.376231\tvalid_0's auc: 0.909477\n",
      "[327]\tvalid_0's binary_logloss: 0.376243\tvalid_0's auc: 0.90947\n",
      "[328]\tvalid_0's binary_logloss: 0.376223\tvalid_0's auc: 0.909471\n",
      "[329]\tvalid_0's binary_logloss: 0.376232\tvalid_0's auc: 0.909456\n",
      "[330]\tvalid_0's binary_logloss: 0.376242\tvalid_0's auc: 0.909447\n",
      "[331]\tvalid_0's binary_logloss: 0.376198\tvalid_0's auc: 0.909463\n",
      "[332]\tvalid_0's binary_logloss: 0.376239\tvalid_0's auc: 0.909448\n",
      "[333]\tvalid_0's binary_logloss: 0.376202\tvalid_0's auc: 0.909465\n",
      "[334]\tvalid_0's binary_logloss: 0.376137\tvalid_0's auc: 0.909492\n",
      "[335]\tvalid_0's binary_logloss: 0.376104\tvalid_0's auc: 0.909499\n",
      "[336]\tvalid_0's binary_logloss: 0.376049\tvalid_0's auc: 0.909529\n",
      "[337]\tvalid_0's binary_logloss: 0.376035\tvalid_0's auc: 0.909539\n",
      "[338]\tvalid_0's binary_logloss: 0.376026\tvalid_0's auc: 0.909542\n",
      "[339]\tvalid_0's binary_logloss: 0.375945\tvalid_0's auc: 0.909586\n",
      "[340]\tvalid_0's binary_logloss: 0.375954\tvalid_0's auc: 0.909581\n",
      "[341]\tvalid_0's binary_logloss: 0.375921\tvalid_0's auc: 0.909594\n",
      "[342]\tvalid_0's binary_logloss: 0.375911\tvalid_0's auc: 0.909599\n",
      "[343]\tvalid_0's binary_logloss: 0.375895\tvalid_0's auc: 0.909597\n",
      "[344]\tvalid_0's binary_logloss: 0.37581\tvalid_0's auc: 0.909643\n",
      "[345]\tvalid_0's binary_logloss: 0.375843\tvalid_0's auc: 0.909633\n",
      "[346]\tvalid_0's binary_logloss: 0.375899\tvalid_0's auc: 0.909599\n",
      "[347]\tvalid_0's binary_logloss: 0.375894\tvalid_0's auc: 0.909602\n",
      "[348]\tvalid_0's binary_logloss: 0.375899\tvalid_0's auc: 0.9096\n",
      "[349]\tvalid_0's binary_logloss: 0.375887\tvalid_0's auc: 0.909602\n",
      "[350]\tvalid_0's binary_logloss: 0.375859\tvalid_0's auc: 0.909611\n",
      "[351]\tvalid_0's binary_logloss: 0.375874\tvalid_0's auc: 0.90961\n",
      "[352]\tvalid_0's binary_logloss: 0.375838\tvalid_0's auc: 0.909624\n",
      "[353]\tvalid_0's binary_logloss: 0.375873\tvalid_0's auc: 0.909613\n",
      "[354]\tvalid_0's binary_logloss: 0.375914\tvalid_0's auc: 0.909598\n",
      "[355]\tvalid_0's binary_logloss: 0.375861\tvalid_0's auc: 0.909633\n",
      "[356]\tvalid_0's binary_logloss: 0.37576\tvalid_0's auc: 0.909685\n",
      "[357]\tvalid_0's binary_logloss: 0.375698\tvalid_0's auc: 0.909714\n",
      "[358]\tvalid_0's binary_logloss: 0.375689\tvalid_0's auc: 0.909718\n",
      "[359]\tvalid_0's binary_logloss: 0.375697\tvalid_0's auc: 0.909713\n",
      "[360]\tvalid_0's binary_logloss: 0.375709\tvalid_0's auc: 0.909721\n",
      "[361]\tvalid_0's binary_logloss: 0.375707\tvalid_0's auc: 0.909724\n",
      "[362]\tvalid_0's binary_logloss: 0.375656\tvalid_0's auc: 0.909755\n",
      "[363]\tvalid_0's binary_logloss: 0.375683\tvalid_0's auc: 0.909748\n",
      "[364]\tvalid_0's binary_logloss: 0.375684\tvalid_0's auc: 0.90975\n",
      "[365]\tvalid_0's binary_logloss: 0.375688\tvalid_0's auc: 0.909748\n",
      "[366]\tvalid_0's binary_logloss: 0.375672\tvalid_0's auc: 0.909764\n",
      "[367]\tvalid_0's binary_logloss: 0.375707\tvalid_0's auc: 0.909739\n",
      "[368]\tvalid_0's binary_logloss: 0.375731\tvalid_0's auc: 0.909728\n",
      "[369]\tvalid_0's binary_logloss: 0.375772\tvalid_0's auc: 0.909703\n",
      "[370]\tvalid_0's binary_logloss: 0.375794\tvalid_0's auc: 0.90969\n",
      "[371]\tvalid_0's binary_logloss: 0.375785\tvalid_0's auc: 0.909696\n",
      "[372]\tvalid_0's binary_logloss: 0.375799\tvalid_0's auc: 0.90969\n",
      "[373]\tvalid_0's binary_logloss: 0.375786\tvalid_0's auc: 0.9097\n",
      "[374]\tvalid_0's binary_logloss: 0.375791\tvalid_0's auc: 0.909699\n",
      "[375]\tvalid_0's binary_logloss: 0.375774\tvalid_0's auc: 0.909707\n",
      "[376]\tvalid_0's binary_logloss: 0.375738\tvalid_0's auc: 0.909745\n",
      "[377]\tvalid_0's binary_logloss: 0.375725\tvalid_0's auc: 0.909753\n",
      "[378]\tvalid_0's binary_logloss: 0.375692\tvalid_0's auc: 0.90978\n",
      "[379]\tvalid_0's binary_logloss: 0.375686\tvalid_0's auc: 0.909782\n",
      "[380]\tvalid_0's binary_logloss: 0.375662\tvalid_0's auc: 0.909799\n",
      "[381]\tvalid_0's binary_logloss: 0.375668\tvalid_0's auc: 0.90979\n",
      "[382]\tvalid_0's binary_logloss: 0.375598\tvalid_0's auc: 0.909827\n",
      "[383]\tvalid_0's binary_logloss: 0.375588\tvalid_0's auc: 0.909831\n",
      "[384]\tvalid_0's binary_logloss: 0.375508\tvalid_0's auc: 0.909869\n",
      "[385]\tvalid_0's binary_logloss: 0.375514\tvalid_0's auc: 0.90986\n",
      "[386]\tvalid_0's binary_logloss: 0.375497\tvalid_0's auc: 0.90986\n",
      "[387]\tvalid_0's binary_logloss: 0.375492\tvalid_0's auc: 0.909866\n",
      "[388]\tvalid_0's binary_logloss: 0.375413\tvalid_0's auc: 0.9099\n",
      "[389]\tvalid_0's binary_logloss: 0.375421\tvalid_0's auc: 0.909895\n",
      "[390]\tvalid_0's binary_logloss: 0.375384\tvalid_0's auc: 0.909916\n",
      "[391]\tvalid_0's binary_logloss: 0.375381\tvalid_0's auc: 0.909925\n",
      "[392]\tvalid_0's binary_logloss: 0.375358\tvalid_0's auc: 0.909939\n",
      "[393]\tvalid_0's binary_logloss: 0.375369\tvalid_0's auc: 0.909929\n",
      "[394]\tvalid_0's binary_logloss: 0.375345\tvalid_0's auc: 0.909942\n",
      "[395]\tvalid_0's binary_logloss: 0.375318\tvalid_0's auc: 0.909961\n",
      "[396]\tvalid_0's binary_logloss: 0.375334\tvalid_0's auc: 0.909958\n",
      "[397]\tvalid_0's binary_logloss: 0.375328\tvalid_0's auc: 0.909961\n",
      "[398]\tvalid_0's binary_logloss: 0.375311\tvalid_0's auc: 0.909957\n",
      "[399]\tvalid_0's binary_logloss: 0.375312\tvalid_0's auc: 0.909957\n",
      "[400]\tvalid_0's binary_logloss: 0.375258\tvalid_0's auc: 0.90998\n",
      "[401]\tvalid_0's binary_logloss: 0.375251\tvalid_0's auc: 0.90999\n",
      "[402]\tvalid_0's binary_logloss: 0.375272\tvalid_0's auc: 0.909981\n",
      "[403]\tvalid_0's binary_logloss: 0.375306\tvalid_0's auc: 0.909972\n",
      "[404]\tvalid_0's binary_logloss: 0.375276\tvalid_0's auc: 0.909989\n",
      "[405]\tvalid_0's binary_logloss: 0.375271\tvalid_0's auc: 0.910001\n",
      "[406]\tvalid_0's binary_logloss: 0.375295\tvalid_0's auc: 0.909993\n",
      "[407]\tvalid_0's binary_logloss: 0.375289\tvalid_0's auc: 0.909986\n",
      "[408]\tvalid_0's binary_logloss: 0.375284\tvalid_0's auc: 0.909984\n",
      "[409]\tvalid_0's binary_logloss: 0.3753\tvalid_0's auc: 0.909977\n",
      "[410]\tvalid_0's binary_logloss: 0.375325\tvalid_0's auc: 0.909962\n",
      "[411]\tvalid_0's binary_logloss: 0.375298\tvalid_0's auc: 0.909983\n",
      "[412]\tvalid_0's binary_logloss: 0.375266\tvalid_0's auc: 0.909999\n",
      "[413]\tvalid_0's binary_logloss: 0.375223\tvalid_0's auc: 0.910025\n",
      "[414]\tvalid_0's binary_logloss: 0.375209\tvalid_0's auc: 0.910036\n",
      "[415]\tvalid_0's binary_logloss: 0.375166\tvalid_0's auc: 0.910055\n",
      "[416]\tvalid_0's binary_logloss: 0.375232\tvalid_0's auc: 0.910038\n",
      "[417]\tvalid_0's binary_logloss: 0.375191\tvalid_0's auc: 0.910049\n",
      "[418]\tvalid_0's binary_logloss: 0.375157\tvalid_0's auc: 0.910066\n",
      "[419]\tvalid_0's binary_logloss: 0.37514\tvalid_0's auc: 0.910077\n",
      "[420]\tvalid_0's binary_logloss: 0.375102\tvalid_0's auc: 0.910083\n",
      "[421]\tvalid_0's binary_logloss: 0.375142\tvalid_0's auc: 0.91007\n",
      "[422]\tvalid_0's binary_logloss: 0.375158\tvalid_0's auc: 0.910073\n",
      "[423]\tvalid_0's binary_logloss: 0.375179\tvalid_0's auc: 0.910078\n",
      "[424]\tvalid_0's binary_logloss: 0.375131\tvalid_0's auc: 0.910095\n",
      "[425]\tvalid_0's binary_logloss: 0.37514\tvalid_0's auc: 0.910092\n",
      "[426]\tvalid_0's binary_logloss: 0.37515\tvalid_0's auc: 0.910087\n",
      "[427]\tvalid_0's binary_logloss: 0.375129\tvalid_0's auc: 0.910093\n",
      "[428]\tvalid_0's binary_logloss: 0.375169\tvalid_0's auc: 0.910066\n",
      "[429]\tvalid_0's binary_logloss: 0.375122\tvalid_0's auc: 0.910091\n",
      "[430]\tvalid_0's binary_logloss: 0.375143\tvalid_0's auc: 0.91008\n",
      "[431]\tvalid_0's binary_logloss: 0.375142\tvalid_0's auc: 0.910082\n",
      "[432]\tvalid_0's binary_logloss: 0.375103\tvalid_0's auc: 0.910103\n",
      "[433]\tvalid_0's binary_logloss: 0.375114\tvalid_0's auc: 0.910103\n",
      "[434]\tvalid_0's binary_logloss: 0.375108\tvalid_0's auc: 0.910105\n",
      "[435]\tvalid_0's binary_logloss: 0.375105\tvalid_0's auc: 0.910105\n",
      "[436]\tvalid_0's binary_logloss: 0.375143\tvalid_0's auc: 0.910091\n",
      "[437]\tvalid_0's binary_logloss: 0.375141\tvalid_0's auc: 0.910089\n",
      "[438]\tvalid_0's binary_logloss: 0.37514\tvalid_0's auc: 0.910086\n",
      "[439]\tvalid_0's binary_logloss: 0.375153\tvalid_0's auc: 0.910074\n",
      "[440]\tvalid_0's binary_logloss: 0.375169\tvalid_0's auc: 0.91007\n",
      "[441]\tvalid_0's binary_logloss: 0.375209\tvalid_0's auc: 0.910056\n",
      "[442]\tvalid_0's binary_logloss: 0.375147\tvalid_0's auc: 0.910082\n",
      "[443]\tvalid_0's binary_logloss: 0.375126\tvalid_0's auc: 0.910096\n",
      "[444]\tvalid_0's binary_logloss: 0.375128\tvalid_0's auc: 0.910091\n",
      "[445]\tvalid_0's binary_logloss: 0.375119\tvalid_0's auc: 0.910095\n",
      "[446]\tvalid_0's binary_logloss: 0.375108\tvalid_0's auc: 0.910098\n",
      "[447]\tvalid_0's binary_logloss: 0.375115\tvalid_0's auc: 0.910094\n",
      "[448]\tvalid_0's binary_logloss: 0.37514\tvalid_0's auc: 0.91008\n",
      "[449]\tvalid_0's binary_logloss: 0.375109\tvalid_0's auc: 0.910083\n",
      "[450]\tvalid_0's binary_logloss: 0.375076\tvalid_0's auc: 0.910098\n",
      "[451]\tvalid_0's binary_logloss: 0.375094\tvalid_0's auc: 0.910091\n",
      "[452]\tvalid_0's binary_logloss: 0.374917\tvalid_0's auc: 0.910177\n",
      "[453]\tvalid_0's binary_logloss: 0.374942\tvalid_0's auc: 0.910161\n",
      "[454]\tvalid_0's binary_logloss: 0.374919\tvalid_0's auc: 0.910175\n",
      "[455]\tvalid_0's binary_logloss: 0.37487\tvalid_0's auc: 0.910203\n",
      "[456]\tvalid_0's binary_logloss: 0.374859\tvalid_0's auc: 0.910203\n",
      "[457]\tvalid_0's binary_logloss: 0.374828\tvalid_0's auc: 0.910226\n",
      "[458]\tvalid_0's binary_logloss: 0.374799\tvalid_0's auc: 0.910252\n",
      "[459]\tvalid_0's binary_logloss: 0.374783\tvalid_0's auc: 0.910263\n",
      "[460]\tvalid_0's binary_logloss: 0.374741\tvalid_0's auc: 0.910293\n",
      "[461]\tvalid_0's binary_logloss: 0.374701\tvalid_0's auc: 0.910306\n",
      "[462]\tvalid_0's binary_logloss: 0.374666\tvalid_0's auc: 0.910319\n",
      "[463]\tvalid_0's binary_logloss: 0.374679\tvalid_0's auc: 0.910326\n",
      "[464]\tvalid_0's binary_logloss: 0.374679\tvalid_0's auc: 0.910319\n",
      "[465]\tvalid_0's binary_logloss: 0.374702\tvalid_0's auc: 0.910315\n",
      "[466]\tvalid_0's binary_logloss: 0.374669\tvalid_0's auc: 0.910344\n",
      "[467]\tvalid_0's binary_logloss: 0.374673\tvalid_0's auc: 0.910344\n",
      "[468]\tvalid_0's binary_logloss: 0.374718\tvalid_0's auc: 0.910329\n",
      "[469]\tvalid_0's binary_logloss: 0.374667\tvalid_0's auc: 0.910353\n",
      "[470]\tvalid_0's binary_logloss: 0.374663\tvalid_0's auc: 0.910362\n",
      "[471]\tvalid_0's binary_logloss: 0.374659\tvalid_0's auc: 0.910365\n",
      "[472]\tvalid_0's binary_logloss: 0.374664\tvalid_0's auc: 0.910366\n",
      "[473]\tvalid_0's binary_logloss: 0.374644\tvalid_0's auc: 0.910374\n",
      "[474]\tvalid_0's binary_logloss: 0.374595\tvalid_0's auc: 0.910389\n",
      "[475]\tvalid_0's binary_logloss: 0.374586\tvalid_0's auc: 0.910398\n",
      "[476]\tvalid_0's binary_logloss: 0.374568\tvalid_0's auc: 0.910399\n",
      "[477]\tvalid_0's binary_logloss: 0.374608\tvalid_0's auc: 0.910387\n",
      "[478]\tvalid_0's binary_logloss: 0.374627\tvalid_0's auc: 0.910374\n",
      "[479]\tvalid_0's binary_logloss: 0.374575\tvalid_0's auc: 0.9104\n",
      "[480]\tvalid_0's binary_logloss: 0.374584\tvalid_0's auc: 0.91039\n",
      "[481]\tvalid_0's binary_logloss: 0.374581\tvalid_0's auc: 0.91038\n",
      "[482]\tvalid_0's binary_logloss: 0.374469\tvalid_0's auc: 0.910434\n",
      "[483]\tvalid_0's binary_logloss: 0.374458\tvalid_0's auc: 0.910444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[484]\tvalid_0's binary_logloss: 0.374452\tvalid_0's auc: 0.910443\n",
      "[485]\tvalid_0's binary_logloss: 0.374473\tvalid_0's auc: 0.910432\n",
      "[486]\tvalid_0's binary_logloss: 0.374398\tvalid_0's auc: 0.910456\n",
      "[487]\tvalid_0's binary_logloss: 0.374358\tvalid_0's auc: 0.910463\n",
      "[488]\tvalid_0's binary_logloss: 0.374333\tvalid_0's auc: 0.910478\n",
      "[489]\tvalid_0's binary_logloss: 0.374337\tvalid_0's auc: 0.910481\n",
      "[490]\tvalid_0's binary_logloss: 0.374305\tvalid_0's auc: 0.910498\n",
      "[491]\tvalid_0's binary_logloss: 0.37426\tvalid_0's auc: 0.910519\n",
      "[492]\tvalid_0's binary_logloss: 0.374241\tvalid_0's auc: 0.910534\n",
      "[493]\tvalid_0's binary_logloss: 0.37427\tvalid_0's auc: 0.910528\n",
      "[494]\tvalid_0's binary_logloss: 0.374256\tvalid_0's auc: 0.910537\n",
      "[495]\tvalid_0's binary_logloss: 0.374217\tvalid_0's auc: 0.910563\n",
      "[496]\tvalid_0's binary_logloss: 0.374139\tvalid_0's auc: 0.910595\n",
      "[497]\tvalid_0's binary_logloss: 0.37411\tvalid_0's auc: 0.910618\n",
      "[498]\tvalid_0's binary_logloss: 0.374116\tvalid_0's auc: 0.910624\n",
      "[499]\tvalid_0's binary_logloss: 0.37408\tvalid_0's auc: 0.910644\n",
      "[500]\tvalid_0's binary_logloss: 0.374084\tvalid_0's auc: 0.91064\n",
      "[501]\tvalid_0's binary_logloss: 0.37409\tvalid_0's auc: 0.910637\n",
      "[502]\tvalid_0's binary_logloss: 0.374107\tvalid_0's auc: 0.910628\n",
      "[503]\tvalid_0's binary_logloss: 0.374135\tvalid_0's auc: 0.91062\n",
      "[504]\tvalid_0's binary_logloss: 0.374101\tvalid_0's auc: 0.910635\n",
      "[505]\tvalid_0's binary_logloss: 0.374097\tvalid_0's auc: 0.910633\n",
      "[506]\tvalid_0's binary_logloss: 0.374086\tvalid_0's auc: 0.910629\n",
      "[507]\tvalid_0's binary_logloss: 0.374082\tvalid_0's auc: 0.910631\n",
      "[508]\tvalid_0's binary_logloss: 0.374053\tvalid_0's auc: 0.910638\n",
      "[509]\tvalid_0's binary_logloss: 0.374019\tvalid_0's auc: 0.910661\n",
      "[510]\tvalid_0's binary_logloss: 0.373991\tvalid_0's auc: 0.910672\n",
      "[511]\tvalid_0's binary_logloss: 0.37399\tvalid_0's auc: 0.910667\n",
      "[512]\tvalid_0's binary_logloss: 0.37395\tvalid_0's auc: 0.910683\n",
      "[513]\tvalid_0's binary_logloss: 0.37395\tvalid_0's auc: 0.910684\n",
      "[514]\tvalid_0's binary_logloss: 0.37391\tvalid_0's auc: 0.910701\n",
      "[515]\tvalid_0's binary_logloss: 0.373903\tvalid_0's auc: 0.910712\n",
      "[516]\tvalid_0's binary_logloss: 0.373904\tvalid_0's auc: 0.91071\n",
      "[517]\tvalid_0's binary_logloss: 0.37392\tvalid_0's auc: 0.910707\n",
      "[518]\tvalid_0's binary_logloss: 0.373891\tvalid_0's auc: 0.910723\n",
      "[519]\tvalid_0's binary_logloss: 0.373877\tvalid_0's auc: 0.910732\n",
      "[520]\tvalid_0's binary_logloss: 0.373833\tvalid_0's auc: 0.91075\n",
      "[521]\tvalid_0's binary_logloss: 0.373858\tvalid_0's auc: 0.910745\n",
      "[522]\tvalid_0's binary_logloss: 0.373842\tvalid_0's auc: 0.910752\n",
      "[523]\tvalid_0's binary_logloss: 0.373856\tvalid_0's auc: 0.910752\n",
      "[524]\tvalid_0's binary_logloss: 0.373861\tvalid_0's auc: 0.91075\n",
      "[525]\tvalid_0's binary_logloss: 0.373819\tvalid_0's auc: 0.910768\n",
      "[526]\tvalid_0's binary_logloss: 0.373779\tvalid_0's auc: 0.910791\n",
      "[527]\tvalid_0's binary_logloss: 0.373749\tvalid_0's auc: 0.910799\n",
      "[528]\tvalid_0's binary_logloss: 0.373725\tvalid_0's auc: 0.91081\n",
      "[529]\tvalid_0's binary_logloss: 0.373702\tvalid_0's auc: 0.910823\n",
      "[530]\tvalid_0's binary_logloss: 0.373704\tvalid_0's auc: 0.910821\n",
      "[531]\tvalid_0's binary_logloss: 0.373717\tvalid_0's auc: 0.910818\n",
      "[532]\tvalid_0's binary_logloss: 0.373685\tvalid_0's auc: 0.910833\n",
      "[533]\tvalid_0's binary_logloss: 0.373699\tvalid_0's auc: 0.910838\n",
      "[534]\tvalid_0's binary_logloss: 0.373661\tvalid_0's auc: 0.910854\n",
      "[535]\tvalid_0's binary_logloss: 0.373642\tvalid_0's auc: 0.910872\n",
      "[536]\tvalid_0's binary_logloss: 0.373654\tvalid_0's auc: 0.910856\n",
      "[537]\tvalid_0's binary_logloss: 0.373656\tvalid_0's auc: 0.910848\n",
      "[538]\tvalid_0's binary_logloss: 0.373697\tvalid_0's auc: 0.910829\n",
      "[539]\tvalid_0's binary_logloss: 0.373702\tvalid_0's auc: 0.910828\n",
      "[540]\tvalid_0's binary_logloss: 0.373722\tvalid_0's auc: 0.910804\n",
      "[541]\tvalid_0's binary_logloss: 0.373744\tvalid_0's auc: 0.910796\n",
      "[542]\tvalid_0's binary_logloss: 0.373715\tvalid_0's auc: 0.910812\n",
      "[543]\tvalid_0's binary_logloss: 0.37371\tvalid_0's auc: 0.910818\n",
      "[544]\tvalid_0's binary_logloss: 0.373719\tvalid_0's auc: 0.9108\n",
      "[545]\tvalid_0's binary_logloss: 0.373682\tvalid_0's auc: 0.910818\n",
      "[546]\tvalid_0's binary_logloss: 0.37369\tvalid_0's auc: 0.910808\n",
      "[547]\tvalid_0's binary_logloss: 0.373681\tvalid_0's auc: 0.910817\n",
      "[548]\tvalid_0's binary_logloss: 0.373696\tvalid_0's auc: 0.910807\n",
      "[549]\tvalid_0's binary_logloss: 0.37367\tvalid_0's auc: 0.910822\n",
      "[550]\tvalid_0's binary_logloss: 0.373663\tvalid_0's auc: 0.910818\n",
      "[551]\tvalid_0's binary_logloss: 0.373656\tvalid_0's auc: 0.910826\n",
      "[552]\tvalid_0's binary_logloss: 0.373658\tvalid_0's auc: 0.910824\n",
      "[553]\tvalid_0's binary_logloss: 0.373657\tvalid_0's auc: 0.910823\n",
      "[554]\tvalid_0's binary_logloss: 0.373616\tvalid_0's auc: 0.91085\n",
      "[555]\tvalid_0's binary_logloss: 0.373614\tvalid_0's auc: 0.910842\n",
      "[556]\tvalid_0's binary_logloss: 0.37362\tvalid_0's auc: 0.910837\n",
      "[557]\tvalid_0's binary_logloss: 0.373611\tvalid_0's auc: 0.910851\n",
      "[558]\tvalid_0's binary_logloss: 0.373641\tvalid_0's auc: 0.910841\n",
      "[559]\tvalid_0's binary_logloss: 0.373634\tvalid_0's auc: 0.910847\n",
      "[560]\tvalid_0's binary_logloss: 0.373644\tvalid_0's auc: 0.910836\n",
      "[561]\tvalid_0's binary_logloss: 0.373659\tvalid_0's auc: 0.910843\n",
      "[562]\tvalid_0's binary_logloss: 0.373674\tvalid_0's auc: 0.910834\n",
      "[563]\tvalid_0's binary_logloss: 0.37365\tvalid_0's auc: 0.910847\n",
      "[564]\tvalid_0's binary_logloss: 0.373679\tvalid_0's auc: 0.910832\n",
      "[565]\tvalid_0's binary_logloss: 0.373706\tvalid_0's auc: 0.910822\n",
      "[566]\tvalid_0's binary_logloss: 0.373691\tvalid_0's auc: 0.910835\n",
      "[567]\tvalid_0's binary_logloss: 0.373698\tvalid_0's auc: 0.910831\n",
      "[568]\tvalid_0's binary_logloss: 0.373748\tvalid_0's auc: 0.910814\n",
      "[569]\tvalid_0's binary_logloss: 0.373775\tvalid_0's auc: 0.910811\n",
      "[570]\tvalid_0's binary_logloss: 0.373782\tvalid_0's auc: 0.910816\n",
      "[571]\tvalid_0's binary_logloss: 0.373803\tvalid_0's auc: 0.910803\n",
      "[572]\tvalid_0's binary_logloss: 0.373806\tvalid_0's auc: 0.91081\n",
      "[573]\tvalid_0's binary_logloss: 0.373815\tvalid_0's auc: 0.910806\n",
      "[574]\tvalid_0's binary_logloss: 0.373781\tvalid_0's auc: 0.910821\n",
      "[575]\tvalid_0's binary_logloss: 0.373761\tvalid_0's auc: 0.910837\n",
      "[576]\tvalid_0's binary_logloss: 0.373747\tvalid_0's auc: 0.910842\n",
      "[577]\tvalid_0's binary_logloss: 0.373759\tvalid_0's auc: 0.910843\n",
      "[578]\tvalid_0's binary_logloss: 0.373767\tvalid_0's auc: 0.91085\n",
      "[579]\tvalid_0's binary_logloss: 0.373777\tvalid_0's auc: 0.910839\n",
      "[580]\tvalid_0's binary_logloss: 0.373768\tvalid_0's auc: 0.910837\n",
      "[581]\tvalid_0's binary_logloss: 0.373781\tvalid_0's auc: 0.91083\n",
      "[582]\tvalid_0's binary_logloss: 0.373789\tvalid_0's auc: 0.910826\n",
      "[583]\tvalid_0's binary_logloss: 0.373825\tvalid_0's auc: 0.910815\n",
      "[584]\tvalid_0's binary_logloss: 0.373871\tvalid_0's auc: 0.910793\n",
      "[585]\tvalid_0's binary_logloss: 0.373848\tvalid_0's auc: 0.91081\n",
      "[586]\tvalid_0's binary_logloss: 0.373802\tvalid_0's auc: 0.910832\n",
      "[587]\tvalid_0's binary_logloss: 0.373777\tvalid_0's auc: 0.910857\n",
      "[588]\tvalid_0's binary_logloss: 0.373777\tvalid_0's auc: 0.91086\n",
      "[589]\tvalid_0's binary_logloss: 0.373748\tvalid_0's auc: 0.910878\n",
      "[590]\tvalid_0's binary_logloss: 0.373736\tvalid_0's auc: 0.910887\n",
      "[591]\tvalid_0's binary_logloss: 0.373764\tvalid_0's auc: 0.910878\n",
      "[592]\tvalid_0's binary_logloss: 0.373711\tvalid_0's auc: 0.910898\n",
      "[593]\tvalid_0's binary_logloss: 0.373695\tvalid_0's auc: 0.910915\n",
      "[594]\tvalid_0's binary_logloss: 0.373665\tvalid_0's auc: 0.910932\n",
      "[595]\tvalid_0's binary_logloss: 0.373638\tvalid_0's auc: 0.910945\n",
      "[596]\tvalid_0's binary_logloss: 0.373664\tvalid_0's auc: 0.910935\n",
      "[597]\tvalid_0's binary_logloss: 0.373637\tvalid_0's auc: 0.910942\n",
      "[598]\tvalid_0's binary_logloss: 0.373645\tvalid_0's auc: 0.91094\n",
      "[599]\tvalid_0's binary_logloss: 0.373647\tvalid_0's auc: 0.910933\n",
      "[600]\tvalid_0's binary_logloss: 0.373613\tvalid_0's auc: 0.91095\n",
      "[601]\tvalid_0's binary_logloss: 0.373588\tvalid_0's auc: 0.910951\n",
      "[602]\tvalid_0's binary_logloss: 0.37358\tvalid_0's auc: 0.910957\n",
      "[603]\tvalid_0's binary_logloss: 0.37356\tvalid_0's auc: 0.910967\n",
      "[604]\tvalid_0's binary_logloss: 0.373541\tvalid_0's auc: 0.910971\n",
      "[605]\tvalid_0's binary_logloss: 0.373549\tvalid_0's auc: 0.910971\n",
      "[606]\tvalid_0's binary_logloss: 0.373502\tvalid_0's auc: 0.910991\n",
      "[607]\tvalid_0's binary_logloss: 0.373502\tvalid_0's auc: 0.910992\n",
      "[608]\tvalid_0's binary_logloss: 0.373499\tvalid_0's auc: 0.910992\n",
      "[609]\tvalid_0's binary_logloss: 0.373496\tvalid_0's auc: 0.91099\n",
      "[610]\tvalid_0's binary_logloss: 0.373497\tvalid_0's auc: 0.910989\n",
      "[611]\tvalid_0's binary_logloss: 0.373472\tvalid_0's auc: 0.911005\n",
      "[612]\tvalid_0's binary_logloss: 0.373474\tvalid_0's auc: 0.911011\n",
      "[613]\tvalid_0's binary_logloss: 0.373519\tvalid_0's auc: 0.910989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[614]\tvalid_0's binary_logloss: 0.373485\tvalid_0's auc: 0.911008\n",
      "[615]\tvalid_0's binary_logloss: 0.373496\tvalid_0's auc: 0.911001\n",
      "[616]\tvalid_0's binary_logloss: 0.373512\tvalid_0's auc: 0.910996\n",
      "[617]\tvalid_0's binary_logloss: 0.373495\tvalid_0's auc: 0.910997\n",
      "[618]\tvalid_0's binary_logloss: 0.37345\tvalid_0's auc: 0.911019\n",
      "[619]\tvalid_0's binary_logloss: 0.373425\tvalid_0's auc: 0.91102\n",
      "[620]\tvalid_0's binary_logloss: 0.373412\tvalid_0's auc: 0.911024\n",
      "[621]\tvalid_0's binary_logloss: 0.373435\tvalid_0's auc: 0.911018\n",
      "[622]\tvalid_0's binary_logloss: 0.373421\tvalid_0's auc: 0.911027\n",
      "[623]\tvalid_0's binary_logloss: 0.37339\tvalid_0's auc: 0.911044\n",
      "[624]\tvalid_0's binary_logloss: 0.373371\tvalid_0's auc: 0.911053\n",
      "[625]\tvalid_0's binary_logloss: 0.373373\tvalid_0's auc: 0.911059\n",
      "[626]\tvalid_0's binary_logloss: 0.373346\tvalid_0's auc: 0.911068\n",
      "[627]\tvalid_0's binary_logloss: 0.373357\tvalid_0's auc: 0.911062\n",
      "[628]\tvalid_0's binary_logloss: 0.373355\tvalid_0's auc: 0.911057\n",
      "[629]\tvalid_0's binary_logloss: 0.373336\tvalid_0's auc: 0.911059\n",
      "[630]\tvalid_0's binary_logloss: 0.373346\tvalid_0's auc: 0.911058\n",
      "[631]\tvalid_0's binary_logloss: 0.373327\tvalid_0's auc: 0.911072\n",
      "[632]\tvalid_0's binary_logloss: 0.373243\tvalid_0's auc: 0.911107\n",
      "[633]\tvalid_0's binary_logloss: 0.373261\tvalid_0's auc: 0.911092\n",
      "[634]\tvalid_0's binary_logloss: 0.373272\tvalid_0's auc: 0.911078\n",
      "[635]\tvalid_0's binary_logloss: 0.373277\tvalid_0's auc: 0.911082\n",
      "[636]\tvalid_0's binary_logloss: 0.373291\tvalid_0's auc: 0.911079\n",
      "[637]\tvalid_0's binary_logloss: 0.373321\tvalid_0's auc: 0.911066\n",
      "[638]\tvalid_0's binary_logloss: 0.37335\tvalid_0's auc: 0.911046\n",
      "[639]\tvalid_0's binary_logloss: 0.373376\tvalid_0's auc: 0.911035\n",
      "[640]\tvalid_0's binary_logloss: 0.373392\tvalid_0's auc: 0.911034\n",
      "[641]\tvalid_0's binary_logloss: 0.373382\tvalid_0's auc: 0.911045\n",
      "[642]\tvalid_0's binary_logloss: 0.373335\tvalid_0's auc: 0.911057\n",
      "[643]\tvalid_0's binary_logloss: 0.37333\tvalid_0's auc: 0.911076\n",
      "[644]\tvalid_0's binary_logloss: 0.373327\tvalid_0's auc: 0.911072\n",
      "[645]\tvalid_0's binary_logloss: 0.373293\tvalid_0's auc: 0.911078\n",
      "[646]\tvalid_0's binary_logloss: 0.373341\tvalid_0's auc: 0.911059\n",
      "[647]\tvalid_0's binary_logloss: 0.373324\tvalid_0's auc: 0.911081\n",
      "[648]\tvalid_0's binary_logloss: 0.373356\tvalid_0's auc: 0.911062\n",
      "[649]\tvalid_0's binary_logloss: 0.373377\tvalid_0's auc: 0.911053\n",
      "[650]\tvalid_0's binary_logloss: 0.373399\tvalid_0's auc: 0.911049\n",
      "[651]\tvalid_0's binary_logloss: 0.3734\tvalid_0's auc: 0.911054\n",
      "[652]\tvalid_0's binary_logloss: 0.3734\tvalid_0's auc: 0.911058\n",
      "[653]\tvalid_0's binary_logloss: 0.373466\tvalid_0's auc: 0.911021\n",
      "[654]\tvalid_0's binary_logloss: 0.373501\tvalid_0's auc: 0.911004\n",
      "[655]\tvalid_0's binary_logloss: 0.373499\tvalid_0's auc: 0.911006\n",
      "[656]\tvalid_0's binary_logloss: 0.373502\tvalid_0's auc: 0.911008\n",
      "[657]\tvalid_0's binary_logloss: 0.373488\tvalid_0's auc: 0.911017\n",
      "[658]\tvalid_0's binary_logloss: 0.373471\tvalid_0's auc: 0.911025\n",
      "[659]\tvalid_0's binary_logloss: 0.373516\tvalid_0's auc: 0.911005\n",
      "[660]\tvalid_0's binary_logloss: 0.373479\tvalid_0's auc: 0.911012\n",
      "[661]\tvalid_0's binary_logloss: 0.37348\tvalid_0's auc: 0.911009\n",
      "[662]\tvalid_0's binary_logloss: 0.373487\tvalid_0's auc: 0.911007\n",
      "[663]\tvalid_0's binary_logloss: 0.373496\tvalid_0's auc: 0.911015\n",
      "[664]\tvalid_0's binary_logloss: 0.373478\tvalid_0's auc: 0.911028\n",
      "[665]\tvalid_0's binary_logloss: 0.373463\tvalid_0's auc: 0.91103\n",
      "[666]\tvalid_0's binary_logloss: 0.373508\tvalid_0's auc: 0.911014\n",
      "[667]\tvalid_0's binary_logloss: 0.373539\tvalid_0's auc: 0.911003\n",
      "[668]\tvalid_0's binary_logloss: 0.373578\tvalid_0's auc: 0.910993\n",
      "[669]\tvalid_0's binary_logloss: 0.373597\tvalid_0's auc: 0.910983\n",
      "[670]\tvalid_0's binary_logloss: 0.373583\tvalid_0's auc: 0.910984\n",
      "[671]\tvalid_0's binary_logloss: 0.373607\tvalid_0's auc: 0.910973\n",
      "[672]\tvalid_0's binary_logloss: 0.373623\tvalid_0's auc: 0.910965\n",
      "[673]\tvalid_0's binary_logloss: 0.373622\tvalid_0's auc: 0.910967\n",
      "[674]\tvalid_0's binary_logloss: 0.373573\tvalid_0's auc: 0.910999\n",
      "[675]\tvalid_0's binary_logloss: 0.373563\tvalid_0's auc: 0.911002\n",
      "[676]\tvalid_0's binary_logloss: 0.373577\tvalid_0's auc: 0.911003\n",
      "[677]\tvalid_0's binary_logloss: 0.373635\tvalid_0's auc: 0.910969\n",
      "[678]\tvalid_0's binary_logloss: 0.373643\tvalid_0's auc: 0.91096\n",
      "[679]\tvalid_0's binary_logloss: 0.373657\tvalid_0's auc: 0.910955\n",
      "[680]\tvalid_0's binary_logloss: 0.373675\tvalid_0's auc: 0.910938\n",
      "[681]\tvalid_0's binary_logloss: 0.373667\tvalid_0's auc: 0.910946\n",
      "[682]\tvalid_0's binary_logloss: 0.37368\tvalid_0's auc: 0.910933\n",
      "[683]\tvalid_0's binary_logloss: 0.373667\tvalid_0's auc: 0.91093\n",
      "[684]\tvalid_0's binary_logloss: 0.373653\tvalid_0's auc: 0.910945\n",
      "[685]\tvalid_0's binary_logloss: 0.37365\tvalid_0's auc: 0.910945\n",
      "[686]\tvalid_0's binary_logloss: 0.373666\tvalid_0's auc: 0.910944\n",
      "[687]\tvalid_0's binary_logloss: 0.373684\tvalid_0's auc: 0.910936\n",
      "[688]\tvalid_0's binary_logloss: 0.373709\tvalid_0's auc: 0.910932\n",
      "[689]\tvalid_0's binary_logloss: 0.37368\tvalid_0's auc: 0.910952\n",
      "[690]\tvalid_0's binary_logloss: 0.373701\tvalid_0's auc: 0.910946\n",
      "[691]\tvalid_0's binary_logloss: 0.373696\tvalid_0's auc: 0.910952\n",
      "[692]\tvalid_0's binary_logloss: 0.373729\tvalid_0's auc: 0.910936\n",
      "[693]\tvalid_0's binary_logloss: 0.373722\tvalid_0's auc: 0.910939\n",
      "[694]\tvalid_0's binary_logloss: 0.373724\tvalid_0's auc: 0.91094\n",
      "[695]\tvalid_0's binary_logloss: 0.373715\tvalid_0's auc: 0.910944\n",
      "[696]\tvalid_0's binary_logloss: 0.37372\tvalid_0's auc: 0.910949\n",
      "[697]\tvalid_0's binary_logloss: 0.37374\tvalid_0's auc: 0.910948\n",
      "[698]\tvalid_0's binary_logloss: 0.373739\tvalid_0's auc: 0.910958\n",
      "[699]\tvalid_0's binary_logloss: 0.373749\tvalid_0's auc: 0.910963\n",
      "[700]\tvalid_0's binary_logloss: 0.373774\tvalid_0's auc: 0.910952\n",
      "[701]\tvalid_0's binary_logloss: 0.373745\tvalid_0's auc: 0.910968\n",
      "[702]\tvalid_0's binary_logloss: 0.373752\tvalid_0's auc: 0.910961\n",
      "[703]\tvalid_0's binary_logloss: 0.37373\tvalid_0's auc: 0.910976\n",
      "[704]\tvalid_0's binary_logloss: 0.373688\tvalid_0's auc: 0.910991\n",
      "[705]\tvalid_0's binary_logloss: 0.373677\tvalid_0's auc: 0.911001\n",
      "[706]\tvalid_0's binary_logloss: 0.373638\tvalid_0's auc: 0.911028\n",
      "[707]\tvalid_0's binary_logloss: 0.373627\tvalid_0's auc: 0.911033\n",
      "[708]\tvalid_0's binary_logloss: 0.373624\tvalid_0's auc: 0.911029\n",
      "[709]\tvalid_0's binary_logloss: 0.373603\tvalid_0's auc: 0.911039\n",
      "[710]\tvalid_0's binary_logloss: 0.373594\tvalid_0's auc: 0.911043\n",
      "[711]\tvalid_0's binary_logloss: 0.373574\tvalid_0's auc: 0.911054\n",
      "[712]\tvalid_0's binary_logloss: 0.373529\tvalid_0's auc: 0.911085\n",
      "[713]\tvalid_0's binary_logloss: 0.373549\tvalid_0's auc: 0.911082\n",
      "[714]\tvalid_0's binary_logloss: 0.373544\tvalid_0's auc: 0.911083\n",
      "[715]\tvalid_0's binary_logloss: 0.373465\tvalid_0's auc: 0.911128\n",
      "[716]\tvalid_0's binary_logloss: 0.373472\tvalid_0's auc: 0.911133\n",
      "[717]\tvalid_0's binary_logloss: 0.373474\tvalid_0's auc: 0.91113\n",
      "[718]\tvalid_0's binary_logloss: 0.373494\tvalid_0's auc: 0.91113\n",
      "[719]\tvalid_0's binary_logloss: 0.373484\tvalid_0's auc: 0.911137\n",
      "[720]\tvalid_0's binary_logloss: 0.373429\tvalid_0's auc: 0.911178\n",
      "[721]\tvalid_0's binary_logloss: 0.373422\tvalid_0's auc: 0.911181\n",
      "[722]\tvalid_0's binary_logloss: 0.373465\tvalid_0's auc: 0.911157\n",
      "[723]\tvalid_0's binary_logloss: 0.373486\tvalid_0's auc: 0.911147\n",
      "[724]\tvalid_0's binary_logloss: 0.373451\tvalid_0's auc: 0.911171\n",
      "[725]\tvalid_0's binary_logloss: 0.373433\tvalid_0's auc: 0.911175\n",
      "[726]\tvalid_0's binary_logloss: 0.373402\tvalid_0's auc: 0.911192\n",
      "[727]\tvalid_0's binary_logloss: 0.373342\tvalid_0's auc: 0.911208\n",
      "[728]\tvalid_0's binary_logloss: 0.373359\tvalid_0's auc: 0.911201\n",
      "[729]\tvalid_0's binary_logloss: 0.373327\tvalid_0's auc: 0.911221\n",
      "[730]\tvalid_0's binary_logloss: 0.373313\tvalid_0's auc: 0.911226\n",
      "[731]\tvalid_0's binary_logloss: 0.373321\tvalid_0's auc: 0.911219\n",
      "[732]\tvalid_0's binary_logloss: 0.373319\tvalid_0's auc: 0.911219\n",
      "[733]\tvalid_0's binary_logloss: 0.373338\tvalid_0's auc: 0.911208\n",
      "[734]\tvalid_0's binary_logloss: 0.373276\tvalid_0's auc: 0.911249\n",
      "[735]\tvalid_0's binary_logloss: 0.373235\tvalid_0's auc: 0.911274\n",
      "[736]\tvalid_0's binary_logloss: 0.373197\tvalid_0's auc: 0.911282\n",
      "[737]\tvalid_0's binary_logloss: 0.373196\tvalid_0's auc: 0.911284\n",
      "[738]\tvalid_0's binary_logloss: 0.373173\tvalid_0's auc: 0.911299\n",
      "[739]\tvalid_0's binary_logloss: 0.373194\tvalid_0's auc: 0.911285\n",
      "[740]\tvalid_0's binary_logloss: 0.373172\tvalid_0's auc: 0.911294\n",
      "[741]\tvalid_0's binary_logloss: 0.373112\tvalid_0's auc: 0.911319\n",
      "[742]\tvalid_0's binary_logloss: 0.373106\tvalid_0's auc: 0.911321\n",
      "[743]\tvalid_0's binary_logloss: 0.373087\tvalid_0's auc: 0.911331\n",
      "[744]\tvalid_0's binary_logloss: 0.373106\tvalid_0's auc: 0.911326\n",
      "[745]\tvalid_0's binary_logloss: 0.373081\tvalid_0's auc: 0.911329\n",
      "[746]\tvalid_0's binary_logloss: 0.373087\tvalid_0's auc: 0.911326\n",
      "[747]\tvalid_0's binary_logloss: 0.373134\tvalid_0's auc: 0.911293\n",
      "[748]\tvalid_0's binary_logloss: 0.373135\tvalid_0's auc: 0.911293\n",
      "[749]\tvalid_0's binary_logloss: 0.373127\tvalid_0's auc: 0.911294\n",
      "[750]\tvalid_0's binary_logloss: 0.373133\tvalid_0's auc: 0.911298\n",
      "[751]\tvalid_0's binary_logloss: 0.37315\tvalid_0's auc: 0.911293\n",
      "[752]\tvalid_0's binary_logloss: 0.373149\tvalid_0's auc: 0.911297\n",
      "[753]\tvalid_0's binary_logloss: 0.373133\tvalid_0's auc: 0.911306\n",
      "[754]\tvalid_0's binary_logloss: 0.373118\tvalid_0's auc: 0.911308\n",
      "[755]\tvalid_0's binary_logloss: 0.373135\tvalid_0's auc: 0.911303\n",
      "[756]\tvalid_0's binary_logloss: 0.373143\tvalid_0's auc: 0.911303\n",
      "[757]\tvalid_0's binary_logloss: 0.373102\tvalid_0's auc: 0.911326\n",
      "[758]\tvalid_0's binary_logloss: 0.373102\tvalid_0's auc: 0.911326\n",
      "[759]\tvalid_0's binary_logloss: 0.373135\tvalid_0's auc: 0.911317\n",
      "[760]\tvalid_0's binary_logloss: 0.3731\tvalid_0's auc: 0.911342\n",
      "[761]\tvalid_0's binary_logloss: 0.373087\tvalid_0's auc: 0.911352\n",
      "[762]\tvalid_0's binary_logloss: 0.373066\tvalid_0's auc: 0.911363\n",
      "[763]\tvalid_0's binary_logloss: 0.373074\tvalid_0's auc: 0.911363\n",
      "[764]\tvalid_0's binary_logloss: 0.373074\tvalid_0's auc: 0.911359\n",
      "[765]\tvalid_0's binary_logloss: 0.373099\tvalid_0's auc: 0.911343\n",
      "[766]\tvalid_0's binary_logloss: 0.373112\tvalid_0's auc: 0.911336\n",
      "[767]\tvalid_0's binary_logloss: 0.373118\tvalid_0's auc: 0.911326\n",
      "[768]\tvalid_0's binary_logloss: 0.373141\tvalid_0's auc: 0.911314\n",
      "[769]\tvalid_0's binary_logloss: 0.373124\tvalid_0's auc: 0.911323\n",
      "[770]\tvalid_0's binary_logloss: 0.373141\tvalid_0's auc: 0.911313\n",
      "[771]\tvalid_0's binary_logloss: 0.373128\tvalid_0's auc: 0.911313\n",
      "[772]\tvalid_0's binary_logloss: 0.37313\tvalid_0's auc: 0.911313\n",
      "[773]\tvalid_0's binary_logloss: 0.373156\tvalid_0's auc: 0.911294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[774]\tvalid_0's binary_logloss: 0.373156\tvalid_0's auc: 0.911299\n",
      "[775]\tvalid_0's binary_logloss: 0.37316\tvalid_0's auc: 0.911301\n",
      "[776]\tvalid_0's binary_logloss: 0.373153\tvalid_0's auc: 0.911306\n",
      "[777]\tvalid_0's binary_logloss: 0.373121\tvalid_0's auc: 0.911323\n",
      "[778]\tvalid_0's binary_logloss: 0.373129\tvalid_0's auc: 0.911326\n",
      "[779]\tvalid_0's binary_logloss: 0.373119\tvalid_0's auc: 0.911334\n",
      "[780]\tvalid_0's binary_logloss: 0.373103\tvalid_0's auc: 0.911344\n",
      "[781]\tvalid_0's binary_logloss: 0.373093\tvalid_0's auc: 0.911347\n",
      "[782]\tvalid_0's binary_logloss: 0.373025\tvalid_0's auc: 0.911381\n",
      "[783]\tvalid_0's binary_logloss: 0.373053\tvalid_0's auc: 0.911361\n",
      "[784]\tvalid_0's binary_logloss: 0.37301\tvalid_0's auc: 0.911373\n",
      "[785]\tvalid_0's binary_logloss: 0.372997\tvalid_0's auc: 0.911386\n",
      "[786]\tvalid_0's binary_logloss: 0.372968\tvalid_0's auc: 0.911409\n",
      "[787]\tvalid_0's binary_logloss: 0.372983\tvalid_0's auc: 0.911401\n",
      "[788]\tvalid_0's binary_logloss: 0.372973\tvalid_0's auc: 0.911399\n",
      "[789]\tvalid_0's binary_logloss: 0.372979\tvalid_0's auc: 0.911399\n",
      "[790]\tvalid_0's binary_logloss: 0.372996\tvalid_0's auc: 0.911389\n",
      "[791]\tvalid_0's binary_logloss: 0.372987\tvalid_0's auc: 0.911396\n",
      "[792]\tvalid_0's binary_logloss: 0.373011\tvalid_0's auc: 0.911381\n",
      "[793]\tvalid_0's binary_logloss: 0.373013\tvalid_0's auc: 0.911386\n",
      "[794]\tvalid_0's binary_logloss: 0.373014\tvalid_0's auc: 0.911378\n",
      "[795]\tvalid_0's binary_logloss: 0.372987\tvalid_0's auc: 0.911384\n",
      "[796]\tvalid_0's binary_logloss: 0.372942\tvalid_0's auc: 0.911405\n",
      "[797]\tvalid_0's binary_logloss: 0.372967\tvalid_0's auc: 0.91139\n",
      "[798]\tvalid_0's binary_logloss: 0.372961\tvalid_0's auc: 0.911392\n",
      "[799]\tvalid_0's binary_logloss: 0.372978\tvalid_0's auc: 0.911384\n",
      "[800]\tvalid_0's binary_logloss: 0.372981\tvalid_0's auc: 0.911378\n",
      "[801]\tvalid_0's binary_logloss: 0.372983\tvalid_0's auc: 0.911378\n",
      "[802]\tvalid_0's binary_logloss: 0.372962\tvalid_0's auc: 0.911385\n",
      "[803]\tvalid_0's binary_logloss: 0.372966\tvalid_0's auc: 0.911387\n",
      "[804]\tvalid_0's binary_logloss: 0.372909\tvalid_0's auc: 0.911419\n",
      "[805]\tvalid_0's binary_logloss: 0.372925\tvalid_0's auc: 0.91141\n",
      "[806]\tvalid_0's binary_logloss: 0.372944\tvalid_0's auc: 0.911409\n",
      "[807]\tvalid_0's binary_logloss: 0.37292\tvalid_0's auc: 0.911427\n",
      "[808]\tvalid_0's binary_logloss: 0.372957\tvalid_0's auc: 0.911413\n",
      "[809]\tvalid_0's binary_logloss: 0.372975\tvalid_0's auc: 0.911404\n",
      "[810]\tvalid_0's binary_logloss: 0.372967\tvalid_0's auc: 0.91141\n",
      "[811]\tvalid_0's binary_logloss: 0.372938\tvalid_0's auc: 0.911428\n",
      "[812]\tvalid_0's binary_logloss: 0.372919\tvalid_0's auc: 0.911441\n",
      "[813]\tvalid_0's binary_logloss: 0.372932\tvalid_0's auc: 0.91143\n",
      "[814]\tvalid_0's binary_logloss: 0.372914\tvalid_0's auc: 0.911432\n",
      "[815]\tvalid_0's binary_logloss: 0.372884\tvalid_0's auc: 0.911448\n",
      "[816]\tvalid_0's binary_logloss: 0.372822\tvalid_0's auc: 0.911466\n",
      "[817]\tvalid_0's binary_logloss: 0.372829\tvalid_0's auc: 0.911464\n",
      "[818]\tvalid_0's binary_logloss: 0.372843\tvalid_0's auc: 0.911463\n",
      "[819]\tvalid_0's binary_logloss: 0.372833\tvalid_0's auc: 0.911461\n",
      "[820]\tvalid_0's binary_logloss: 0.372822\tvalid_0's auc: 0.91147\n",
      "[821]\tvalid_0's binary_logloss: 0.372815\tvalid_0's auc: 0.911474\n",
      "[822]\tvalid_0's binary_logloss: 0.372832\tvalid_0's auc: 0.911465\n",
      "[823]\tvalid_0's binary_logloss: 0.372813\tvalid_0's auc: 0.91147\n",
      "[824]\tvalid_0's binary_logloss: 0.372782\tvalid_0's auc: 0.911488\n",
      "[825]\tvalid_0's binary_logloss: 0.372783\tvalid_0's auc: 0.911487\n",
      "[826]\tvalid_0's binary_logloss: 0.372794\tvalid_0's auc: 0.911485\n",
      "[827]\tvalid_0's binary_logloss: 0.372784\tvalid_0's auc: 0.911491\n",
      "[828]\tvalid_0's binary_logloss: 0.372783\tvalid_0's auc: 0.91151\n",
      "[829]\tvalid_0's binary_logloss: 0.372793\tvalid_0's auc: 0.911505\n",
      "[830]\tvalid_0's binary_logloss: 0.372787\tvalid_0's auc: 0.911511\n",
      "[831]\tvalid_0's binary_logloss: 0.372797\tvalid_0's auc: 0.911509\n",
      "[832]\tvalid_0's binary_logloss: 0.372806\tvalid_0's auc: 0.911499\n",
      "[833]\tvalid_0's binary_logloss: 0.372771\tvalid_0's auc: 0.911519\n",
      "[834]\tvalid_0's binary_logloss: 0.372781\tvalid_0's auc: 0.911509\n",
      "[835]\tvalid_0's binary_logloss: 0.372794\tvalid_0's auc: 0.911511\n",
      "[836]\tvalid_0's binary_logloss: 0.372784\tvalid_0's auc: 0.911505\n",
      "[837]\tvalid_0's binary_logloss: 0.372776\tvalid_0's auc: 0.911509\n",
      "[838]\tvalid_0's binary_logloss: 0.372763\tvalid_0's auc: 0.911506\n",
      "[839]\tvalid_0's binary_logloss: 0.372773\tvalid_0's auc: 0.911496\n",
      "[840]\tvalid_0's binary_logloss: 0.372771\tvalid_0's auc: 0.911499\n",
      "[841]\tvalid_0's binary_logloss: 0.372757\tvalid_0's auc: 0.911505\n",
      "[842]\tvalid_0's binary_logloss: 0.372774\tvalid_0's auc: 0.911499\n",
      "[843]\tvalid_0's binary_logloss: 0.37273\tvalid_0's auc: 0.911524\n",
      "[844]\tvalid_0's binary_logloss: 0.372717\tvalid_0's auc: 0.911527\n",
      "[845]\tvalid_0's binary_logloss: 0.372694\tvalid_0's auc: 0.911538\n",
      "[846]\tvalid_0's binary_logloss: 0.372652\tvalid_0's auc: 0.911559\n",
      "[847]\tvalid_0's binary_logloss: 0.372626\tvalid_0's auc: 0.911568\n",
      "[848]\tvalid_0's binary_logloss: 0.372642\tvalid_0's auc: 0.911561\n",
      "[849]\tvalid_0's binary_logloss: 0.372597\tvalid_0's auc: 0.911588\n",
      "[850]\tvalid_0's binary_logloss: 0.372608\tvalid_0's auc: 0.911577\n",
      "[851]\tvalid_0's binary_logloss: 0.372604\tvalid_0's auc: 0.911584\n",
      "[852]\tvalid_0's binary_logloss: 0.372602\tvalid_0's auc: 0.911589\n",
      "[853]\tvalid_0's binary_logloss: 0.372579\tvalid_0's auc: 0.911596\n",
      "[854]\tvalid_0's binary_logloss: 0.372591\tvalid_0's auc: 0.91159\n",
      "[855]\tvalid_0's binary_logloss: 0.372595\tvalid_0's auc: 0.911594\n",
      "[856]\tvalid_0's binary_logloss: 0.372636\tvalid_0's auc: 0.911565\n",
      "[857]\tvalid_0's binary_logloss: 0.372626\tvalid_0's auc: 0.911574\n",
      "[858]\tvalid_0's binary_logloss: 0.372633\tvalid_0's auc: 0.911569\n",
      "[859]\tvalid_0's binary_logloss: 0.372621\tvalid_0's auc: 0.911582\n",
      "[860]\tvalid_0's binary_logloss: 0.372627\tvalid_0's auc: 0.91157\n",
      "[861]\tvalid_0's binary_logloss: 0.372626\tvalid_0's auc: 0.91157\n",
      "[862]\tvalid_0's binary_logloss: 0.372616\tvalid_0's auc: 0.911574\n",
      "[863]\tvalid_0's binary_logloss: 0.372637\tvalid_0's auc: 0.911569\n",
      "[864]\tvalid_0's binary_logloss: 0.372641\tvalid_0's auc: 0.911568\n",
      "[865]\tvalid_0's binary_logloss: 0.372647\tvalid_0's auc: 0.911561\n",
      "[866]\tvalid_0's binary_logloss: 0.372656\tvalid_0's auc: 0.91156\n",
      "[867]\tvalid_0's binary_logloss: 0.372664\tvalid_0's auc: 0.911559\n",
      "[868]\tvalid_0's binary_logloss: 0.372677\tvalid_0's auc: 0.911554\n",
      "[869]\tvalid_0's binary_logloss: 0.372666\tvalid_0's auc: 0.911552\n",
      "[870]\tvalid_0's binary_logloss: 0.372668\tvalid_0's auc: 0.911549\n",
      "[871]\tvalid_0's binary_logloss: 0.372686\tvalid_0's auc: 0.911548\n",
      "[872]\tvalid_0's binary_logloss: 0.372689\tvalid_0's auc: 0.911553\n",
      "[873]\tvalid_0's binary_logloss: 0.372685\tvalid_0's auc: 0.911557\n",
      "[874]\tvalid_0's binary_logloss: 0.372678\tvalid_0's auc: 0.911554\n",
      "[875]\tvalid_0's binary_logloss: 0.372709\tvalid_0's auc: 0.911546\n",
      "[876]\tvalid_0's binary_logloss: 0.372685\tvalid_0's auc: 0.911565\n",
      "[877]\tvalid_0's binary_logloss: 0.372697\tvalid_0's auc: 0.91156\n",
      "[878]\tvalid_0's binary_logloss: 0.372683\tvalid_0's auc: 0.91157\n",
      "[879]\tvalid_0's binary_logloss: 0.372687\tvalid_0's auc: 0.911567\n",
      "[880]\tvalid_0's binary_logloss: 0.372696\tvalid_0's auc: 0.911564\n",
      "[881]\tvalid_0's binary_logloss: 0.372725\tvalid_0's auc: 0.911551\n",
      "[882]\tvalid_0's binary_logloss: 0.372702\tvalid_0's auc: 0.911564\n",
      "[883]\tvalid_0's binary_logloss: 0.372705\tvalid_0's auc: 0.911567\n",
      "[884]\tvalid_0's binary_logloss: 0.372728\tvalid_0's auc: 0.911561\n",
      "[885]\tvalid_0's binary_logloss: 0.372721\tvalid_0's auc: 0.911564\n",
      "[886]\tvalid_0's binary_logloss: 0.3727\tvalid_0's auc: 0.911567\n",
      "[887]\tvalid_0's binary_logloss: 0.372725\tvalid_0's auc: 0.911555\n",
      "[888]\tvalid_0's binary_logloss: 0.372637\tvalid_0's auc: 0.911608\n",
      "[889]\tvalid_0's binary_logloss: 0.372644\tvalid_0's auc: 0.911609\n",
      "[890]\tvalid_0's binary_logloss: 0.372652\tvalid_0's auc: 0.911602\n",
      "[891]\tvalid_0's binary_logloss: 0.372691\tvalid_0's auc: 0.911594\n",
      "[892]\tvalid_0's binary_logloss: 0.372682\tvalid_0's auc: 0.911601\n",
      "[893]\tvalid_0's binary_logloss: 0.372691\tvalid_0's auc: 0.911599\n",
      "[894]\tvalid_0's binary_logloss: 0.372712\tvalid_0's auc: 0.9116\n",
      "[895]\tvalid_0's binary_logloss: 0.372733\tvalid_0's auc: 0.911592\n",
      "[896]\tvalid_0's binary_logloss: 0.372753\tvalid_0's auc: 0.911577\n",
      "[897]\tvalid_0's binary_logloss: 0.372732\tvalid_0's auc: 0.911588\n",
      "[898]\tvalid_0's binary_logloss: 0.372718\tvalid_0's auc: 0.911588\n",
      "[899]\tvalid_0's binary_logloss: 0.372703\tvalid_0's auc: 0.911596\n",
      "[900]\tvalid_0's binary_logloss: 0.372687\tvalid_0's auc: 0.911604\n",
      "[901]\tvalid_0's binary_logloss: 0.372668\tvalid_0's auc: 0.911615\n",
      "[902]\tvalid_0's binary_logloss: 0.372655\tvalid_0's auc: 0.911617\n",
      "[903]\tvalid_0's binary_logloss: 0.372669\tvalid_0's auc: 0.911614\n",
      "[904]\tvalid_0's binary_logloss: 0.372655\tvalid_0's auc: 0.911628\n",
      "[905]\tvalid_0's binary_logloss: 0.372662\tvalid_0's auc: 0.911627\n",
      "[906]\tvalid_0's binary_logloss: 0.372651\tvalid_0's auc: 0.911631\n",
      "[907]\tvalid_0's binary_logloss: 0.372611\tvalid_0's auc: 0.911647\n",
      "[908]\tvalid_0's binary_logloss: 0.372626\tvalid_0's auc: 0.91164\n",
      "[909]\tvalid_0's binary_logloss: 0.372639\tvalid_0's auc: 0.91164\n",
      "[910]\tvalid_0's binary_logloss: 0.372628\tvalid_0's auc: 0.911649\n",
      "[911]\tvalid_0's binary_logloss: 0.37265\tvalid_0's auc: 0.911636\n",
      "[912]\tvalid_0's binary_logloss: 0.372689\tvalid_0's auc: 0.91162\n",
      "[913]\tvalid_0's binary_logloss: 0.372716\tvalid_0's auc: 0.911608\n",
      "[914]\tvalid_0's binary_logloss: 0.37272\tvalid_0's auc: 0.911609\n",
      "[915]\tvalid_0's binary_logloss: 0.372718\tvalid_0's auc: 0.911613\n",
      "[916]\tvalid_0's binary_logloss: 0.37272\tvalid_0's auc: 0.911608\n",
      "[917]\tvalid_0's binary_logloss: 0.372723\tvalid_0's auc: 0.911601\n",
      "[918]\tvalid_0's binary_logloss: 0.372694\tvalid_0's auc: 0.911612\n",
      "[919]\tvalid_0's binary_logloss: 0.372687\tvalid_0's auc: 0.911622\n",
      "[920]\tvalid_0's binary_logloss: 0.372687\tvalid_0's auc: 0.911617\n",
      "[921]\tvalid_0's binary_logloss: 0.372681\tvalid_0's auc: 0.911619\n",
      "[922]\tvalid_0's binary_logloss: 0.372672\tvalid_0's auc: 0.91162\n",
      "[923]\tvalid_0's binary_logloss: 0.372681\tvalid_0's auc: 0.911613\n",
      "[924]\tvalid_0's binary_logloss: 0.372637\tvalid_0's auc: 0.911627\n",
      "[925]\tvalid_0's binary_logloss: 0.372633\tvalid_0's auc: 0.911626\n",
      "[926]\tvalid_0's binary_logloss: 0.372618\tvalid_0's auc: 0.911638\n",
      "[927]\tvalid_0's binary_logloss: 0.372639\tvalid_0's auc: 0.911633\n",
      "[928]\tvalid_0's binary_logloss: 0.372702\tvalid_0's auc: 0.911605\n",
      "[929]\tvalid_0's binary_logloss: 0.372675\tvalid_0's auc: 0.911616\n",
      "[930]\tvalid_0's binary_logloss: 0.372672\tvalid_0's auc: 0.911621\n",
      "[931]\tvalid_0's binary_logloss: 0.372665\tvalid_0's auc: 0.91163\n",
      "[932]\tvalid_0's binary_logloss: 0.372675\tvalid_0's auc: 0.91162\n",
      "[933]\tvalid_0's binary_logloss: 0.372698\tvalid_0's auc: 0.91161\n",
      "[934]\tvalid_0's binary_logloss: 0.372684\tvalid_0's auc: 0.911616\n",
      "[935]\tvalid_0's binary_logloss: 0.372676\tvalid_0's auc: 0.911626\n",
      "[936]\tvalid_0's binary_logloss: 0.372662\tvalid_0's auc: 0.911636\n",
      "[937]\tvalid_0's binary_logloss: 0.372626\tvalid_0's auc: 0.91165\n",
      "[938]\tvalid_0's binary_logloss: 0.372644\tvalid_0's auc: 0.911652\n",
      "[939]\tvalid_0's binary_logloss: 0.372596\tvalid_0's auc: 0.911677\n",
      "[940]\tvalid_0's binary_logloss: 0.372592\tvalid_0's auc: 0.911684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[941]\tvalid_0's binary_logloss: 0.372612\tvalid_0's auc: 0.911678\n",
      "[942]\tvalid_0's binary_logloss: 0.37262\tvalid_0's auc: 0.911676\n",
      "[943]\tvalid_0's binary_logloss: 0.372639\tvalid_0's auc: 0.911667\n",
      "[944]\tvalid_0's binary_logloss: 0.372648\tvalid_0's auc: 0.911654\n",
      "[945]\tvalid_0's binary_logloss: 0.372624\tvalid_0's auc: 0.911666\n",
      "[946]\tvalid_0's binary_logloss: 0.372637\tvalid_0's auc: 0.911663\n",
      "[947]\tvalid_0's binary_logloss: 0.372686\tvalid_0's auc: 0.911647\n",
      "[948]\tvalid_0's binary_logloss: 0.372702\tvalid_0's auc: 0.911642\n",
      "[949]\tvalid_0's binary_logloss: 0.3727\tvalid_0's auc: 0.911638\n",
      "[950]\tvalid_0's binary_logloss: 0.372666\tvalid_0's auc: 0.911657\n",
      "[951]\tvalid_0's binary_logloss: 0.372668\tvalid_0's auc: 0.911668\n",
      "[952]\tvalid_0's binary_logloss: 0.372656\tvalid_0's auc: 0.911673\n",
      "[953]\tvalid_0's binary_logloss: 0.372683\tvalid_0's auc: 0.911655\n",
      "[954]\tvalid_0's binary_logloss: 0.372669\tvalid_0's auc: 0.911665\n",
      "[955]\tvalid_0's binary_logloss: 0.372711\tvalid_0's auc: 0.911649\n",
      "[956]\tvalid_0's binary_logloss: 0.372719\tvalid_0's auc: 0.91164\n",
      "[957]\tvalid_0's binary_logloss: 0.372744\tvalid_0's auc: 0.911626\n",
      "[958]\tvalid_0's binary_logloss: 0.372757\tvalid_0's auc: 0.911623\n",
      "[959]\tvalid_0's binary_logloss: 0.372722\tvalid_0's auc: 0.911639\n",
      "[960]\tvalid_0's binary_logloss: 0.37272\tvalid_0's auc: 0.91164\n",
      "[961]\tvalid_0's binary_logloss: 0.372705\tvalid_0's auc: 0.911646\n",
      "[962]\tvalid_0's binary_logloss: 0.37272\tvalid_0's auc: 0.91164\n",
      "[963]\tvalid_0's binary_logloss: 0.372731\tvalid_0's auc: 0.911635\n",
      "[964]\tvalid_0's binary_logloss: 0.372671\tvalid_0's auc: 0.91165\n",
      "[965]\tvalid_0's binary_logloss: 0.372668\tvalid_0's auc: 0.911648\n",
      "[966]\tvalid_0's binary_logloss: 0.37268\tvalid_0's auc: 0.911646\n",
      "[967]\tvalid_0's binary_logloss: 0.372709\tvalid_0's auc: 0.91163\n",
      "[968]\tvalid_0's binary_logloss: 0.372687\tvalid_0's auc: 0.91164\n",
      "[969]\tvalid_0's binary_logloss: 0.372682\tvalid_0's auc: 0.911646\n",
      "[970]\tvalid_0's binary_logloss: 0.372694\tvalid_0's auc: 0.91164\n",
      "[971]\tvalid_0's binary_logloss: 0.372701\tvalid_0's auc: 0.911641\n",
      "[972]\tvalid_0's binary_logloss: 0.372644\tvalid_0's auc: 0.911679\n",
      "[973]\tvalid_0's binary_logloss: 0.372658\tvalid_0's auc: 0.911678\n",
      "[974]\tvalid_0's binary_logloss: 0.372667\tvalid_0's auc: 0.91168\n",
      "[975]\tvalid_0's binary_logloss: 0.372674\tvalid_0's auc: 0.911687\n",
      "[976]\tvalid_0's binary_logloss: 0.372675\tvalid_0's auc: 0.911684\n",
      "[977]\tvalid_0's binary_logloss: 0.372704\tvalid_0's auc: 0.911674\n",
      "[978]\tvalid_0's binary_logloss: 0.372717\tvalid_0's auc: 0.911666\n",
      "[979]\tvalid_0's binary_logloss: 0.372717\tvalid_0's auc: 0.911668\n",
      "[980]\tvalid_0's binary_logloss: 0.372704\tvalid_0's auc: 0.911669\n",
      "[981]\tvalid_0's binary_logloss: 0.372716\tvalid_0's auc: 0.91166\n",
      "[982]\tvalid_0's binary_logloss: 0.372741\tvalid_0's auc: 0.911649\n",
      "[983]\tvalid_0's binary_logloss: 0.372762\tvalid_0's auc: 0.911641\n",
      "[984]\tvalid_0's binary_logloss: 0.372775\tvalid_0's auc: 0.911636\n",
      "[985]\tvalid_0's binary_logloss: 0.37278\tvalid_0's auc: 0.911641\n",
      "[986]\tvalid_0's binary_logloss: 0.372808\tvalid_0's auc: 0.911623\n",
      "[987]\tvalid_0's binary_logloss: 0.372804\tvalid_0's auc: 0.911624\n",
      "[988]\tvalid_0's binary_logloss: 0.372823\tvalid_0's auc: 0.911603\n",
      "[989]\tvalid_0's binary_logloss: 0.372813\tvalid_0's auc: 0.911614\n",
      "[990]\tvalid_0's binary_logloss: 0.372757\tvalid_0's auc: 0.911634\n",
      "[991]\tvalid_0's binary_logloss: 0.372759\tvalid_0's auc: 0.911635\n",
      "[992]\tvalid_0's binary_logloss: 0.372765\tvalid_0's auc: 0.911632\n",
      "[993]\tvalid_0's binary_logloss: 0.372784\tvalid_0's auc: 0.911622\n",
      "[994]\tvalid_0's binary_logloss: 0.372773\tvalid_0's auc: 0.911629\n",
      "[995]\tvalid_0's binary_logloss: 0.372731\tvalid_0's auc: 0.911652\n",
      "[996]\tvalid_0's binary_logloss: 0.372738\tvalid_0's auc: 0.911656\n",
      "[997]\tvalid_0's binary_logloss: 0.372696\tvalid_0's auc: 0.911671\n",
      "[998]\tvalid_0's binary_logloss: 0.372713\tvalid_0's auc: 0.911667\n",
      "[999]\tvalid_0's binary_logloss: 0.372712\tvalid_0's auc: 0.911671\n",
      "[1000]\tvalid_0's binary_logloss: 0.372674\tvalid_0's auc: 0.911693\n",
      "[1001]\tvalid_0's binary_logloss: 0.372685\tvalid_0's auc: 0.911694\n",
      "[1002]\tvalid_0's binary_logloss: 0.372718\tvalid_0's auc: 0.911678\n",
      "[1003]\tvalid_0's binary_logloss: 0.372715\tvalid_0's auc: 0.911679\n",
      "[1004]\tvalid_0's binary_logloss: 0.372717\tvalid_0's auc: 0.911666\n",
      "[1005]\tvalid_0's binary_logloss: 0.372717\tvalid_0's auc: 0.911673\n",
      "[1006]\tvalid_0's binary_logloss: 0.372731\tvalid_0's auc: 0.911668\n",
      "[1007]\tvalid_0's binary_logloss: 0.372712\tvalid_0's auc: 0.911664\n",
      "[1008]\tvalid_0's binary_logloss: 0.372775\tvalid_0's auc: 0.911642\n",
      "[1009]\tvalid_0's binary_logloss: 0.372792\tvalid_0's auc: 0.911643\n",
      "[1010]\tvalid_0's binary_logloss: 0.372805\tvalid_0's auc: 0.91164\n",
      "[1011]\tvalid_0's binary_logloss: 0.372785\tvalid_0's auc: 0.911657\n",
      "[1012]\tvalid_0's binary_logloss: 0.372793\tvalid_0's auc: 0.911655\n",
      "[1013]\tvalid_0's binary_logloss: 0.372834\tvalid_0's auc: 0.911636\n",
      "[1014]\tvalid_0's binary_logloss: 0.372868\tvalid_0's auc: 0.911623\n",
      "[1015]\tvalid_0's binary_logloss: 0.372873\tvalid_0's auc: 0.911625\n",
      "[1016]\tvalid_0's binary_logloss: 0.372888\tvalid_0's auc: 0.911621\n",
      "[1017]\tvalid_0's binary_logloss: 0.372916\tvalid_0's auc: 0.911604\n",
      "[1018]\tvalid_0's binary_logloss: 0.372923\tvalid_0's auc: 0.911589\n",
      "[1019]\tvalid_0's binary_logloss: 0.372963\tvalid_0's auc: 0.911571\n",
      "[1020]\tvalid_0's binary_logloss: 0.372951\tvalid_0's auc: 0.911591\n",
      "[1021]\tvalid_0's binary_logloss: 0.372949\tvalid_0's auc: 0.911599\n",
      "[1022]\tvalid_0's binary_logloss: 0.372947\tvalid_0's auc: 0.911608\n",
      "[1023]\tvalid_0's binary_logloss: 0.372922\tvalid_0's auc: 0.911622\n",
      "[1024]\tvalid_0's binary_logloss: 0.372919\tvalid_0's auc: 0.911628\n",
      "[1025]\tvalid_0's binary_logloss: 0.372924\tvalid_0's auc: 0.911626\n",
      "[1026]\tvalid_0's binary_logloss: 0.372939\tvalid_0's auc: 0.911617\n",
      "[1027]\tvalid_0's binary_logloss: 0.372918\tvalid_0's auc: 0.911616\n",
      "[1028]\tvalid_0's binary_logloss: 0.372901\tvalid_0's auc: 0.911622\n",
      "[1029]\tvalid_0's binary_logloss: 0.372861\tvalid_0's auc: 0.911642\n",
      "[1030]\tvalid_0's binary_logloss: 0.372858\tvalid_0's auc: 0.911644\n",
      "[1031]\tvalid_0's binary_logloss: 0.372833\tvalid_0's auc: 0.911658\n",
      "[1032]\tvalid_0's binary_logloss: 0.372841\tvalid_0's auc: 0.911654\n",
      "[1033]\tvalid_0's binary_logloss: 0.372865\tvalid_0's auc: 0.911645\n",
      "[1034]\tvalid_0's binary_logloss: 0.372867\tvalid_0's auc: 0.911637\n",
      "[1035]\tvalid_0's binary_logloss: 0.372881\tvalid_0's auc: 0.911636\n",
      "[1036]\tvalid_0's binary_logloss: 0.372868\tvalid_0's auc: 0.911647\n",
      "[1037]\tvalid_0's binary_logloss: 0.372848\tvalid_0's auc: 0.911651\n",
      "[1038]\tvalid_0's binary_logloss: 0.372786\tvalid_0's auc: 0.911679\n",
      "[1039]\tvalid_0's binary_logloss: 0.372773\tvalid_0's auc: 0.911682\n",
      "[1040]\tvalid_0's binary_logloss: 0.372801\tvalid_0's auc: 0.911674\n",
      "[1041]\tvalid_0's binary_logloss: 0.372822\tvalid_0's auc: 0.911667\n",
      "[1042]\tvalid_0's binary_logloss: 0.372808\tvalid_0's auc: 0.91167\n",
      "[1043]\tvalid_0's binary_logloss: 0.372812\tvalid_0's auc: 0.911666\n",
      "[1044]\tvalid_0's binary_logloss: 0.372844\tvalid_0's auc: 0.911649\n",
      "[1045]\tvalid_0's binary_logloss: 0.372842\tvalid_0's auc: 0.911654\n",
      "[1046]\tvalid_0's binary_logloss: 0.372877\tvalid_0's auc: 0.911649\n",
      "[1047]\tvalid_0's binary_logloss: 0.372902\tvalid_0's auc: 0.91164\n",
      "[1048]\tvalid_0's binary_logloss: 0.372889\tvalid_0's auc: 0.911653\n",
      "[1049]\tvalid_0's binary_logloss: 0.372898\tvalid_0's auc: 0.911649\n",
      "[1050]\tvalid_0's binary_logloss: 0.372888\tvalid_0's auc: 0.91165\n",
      "[1051]\tvalid_0's binary_logloss: 0.372937\tvalid_0's auc: 0.911633\n",
      "[1052]\tvalid_0's binary_logloss: 0.372899\tvalid_0's auc: 0.91165\n",
      "[1053]\tvalid_0's binary_logloss: 0.3729\tvalid_0's auc: 0.911646\n",
      "[1054]\tvalid_0's binary_logloss: 0.372892\tvalid_0's auc: 0.911652\n",
      "[1055]\tvalid_0's binary_logloss: 0.372928\tvalid_0's auc: 0.911629\n",
      "[1056]\tvalid_0's binary_logloss: 0.372962\tvalid_0's auc: 0.911626\n",
      "[1057]\tvalid_0's binary_logloss: 0.372999\tvalid_0's auc: 0.91162\n",
      "[1058]\tvalid_0's binary_logloss: 0.373022\tvalid_0's auc: 0.911607\n",
      "[1059]\tvalid_0's binary_logloss: 0.373011\tvalid_0's auc: 0.911617\n",
      "[1060]\tvalid_0's binary_logloss: 0.373026\tvalid_0's auc: 0.911606\n",
      "[1061]\tvalid_0's binary_logloss: 0.373037\tvalid_0's auc: 0.911593\n",
      "[1062]\tvalid_0's binary_logloss: 0.373043\tvalid_0's auc: 0.911594\n",
      "[1063]\tvalid_0's binary_logloss: 0.373022\tvalid_0's auc: 0.911608\n",
      "[1064]\tvalid_0's binary_logloss: 0.373028\tvalid_0's auc: 0.911605\n",
      "[1065]\tvalid_0's binary_logloss: 0.373065\tvalid_0's auc: 0.911591\n",
      "[1066]\tvalid_0's binary_logloss: 0.373088\tvalid_0's auc: 0.911573\n",
      "[1067]\tvalid_0's binary_logloss: 0.3731\tvalid_0's auc: 0.911573\n",
      "[1068]\tvalid_0's binary_logloss: 0.373104\tvalid_0's auc: 0.911568\n",
      "[1069]\tvalid_0's binary_logloss: 0.373133\tvalid_0's auc: 0.911553\n",
      "[1070]\tvalid_0's binary_logloss: 0.373143\tvalid_0's auc: 0.911552\n",
      "[1071]\tvalid_0's binary_logloss: 0.373137\tvalid_0's auc: 0.911561\n",
      "[1072]\tvalid_0's binary_logloss: 0.373158\tvalid_0's auc: 0.911556\n",
      "[1073]\tvalid_0's binary_logloss: 0.373176\tvalid_0's auc: 0.911553\n",
      "[1074]\tvalid_0's binary_logloss: 0.373193\tvalid_0's auc: 0.911549\n",
      "[1075]\tvalid_0's binary_logloss: 0.373174\tvalid_0's auc: 0.911556\n",
      "[1076]\tvalid_0's binary_logloss: 0.37317\tvalid_0's auc: 0.911554\n",
      "[1077]\tvalid_0's binary_logloss: 0.373158\tvalid_0's auc: 0.91155\n",
      "[1078]\tvalid_0's binary_logloss: 0.373165\tvalid_0's auc: 0.911546\n",
      "[1079]\tvalid_0's binary_logloss: 0.373159\tvalid_0's auc: 0.911551\n",
      "[1080]\tvalid_0's binary_logloss: 0.373186\tvalid_0's auc: 0.911536\n",
      "[1081]\tvalid_0's binary_logloss: 0.373218\tvalid_0's auc: 0.911517\n",
      "[1082]\tvalid_0's binary_logloss: 0.373234\tvalid_0's auc: 0.911517\n",
      "[1083]\tvalid_0's binary_logloss: 0.373251\tvalid_0's auc: 0.911513\n",
      "[1084]\tvalid_0's binary_logloss: 0.373233\tvalid_0's auc: 0.911516\n",
      "[1085]\tvalid_0's binary_logloss: 0.373213\tvalid_0's auc: 0.91152\n",
      "[1086]\tvalid_0's binary_logloss: 0.373235\tvalid_0's auc: 0.911514\n",
      "[1087]\tvalid_0's binary_logloss: 0.373226\tvalid_0's auc: 0.911516\n",
      "[1088]\tvalid_0's binary_logloss: 0.373169\tvalid_0's auc: 0.911542\n",
      "[1089]\tvalid_0's binary_logloss: 0.373201\tvalid_0's auc: 0.911532\n",
      "[1090]\tvalid_0's binary_logloss: 0.373163\tvalid_0's auc: 0.911547\n",
      "[1091]\tvalid_0's binary_logloss: 0.373082\tvalid_0's auc: 0.91157\n",
      "[1092]\tvalid_0's binary_logloss: 0.373074\tvalid_0's auc: 0.911576\n",
      "[1093]\tvalid_0's binary_logloss: 0.373086\tvalid_0's auc: 0.911575\n",
      "[1094]\tvalid_0's binary_logloss: 0.373049\tvalid_0's auc: 0.911583\n",
      "[1095]\tvalid_0's binary_logloss: 0.373086\tvalid_0's auc: 0.911568\n",
      "[1096]\tvalid_0's binary_logloss: 0.37311\tvalid_0's auc: 0.911558\n",
      "[1097]\tvalid_0's binary_logloss: 0.373149\tvalid_0's auc: 0.911533\n",
      "[1098]\tvalid_0's binary_logloss: 0.373168\tvalid_0's auc: 0.91153\n",
      "[1099]\tvalid_0's binary_logloss: 0.373144\tvalid_0's auc: 0.911541\n",
      "[1100]\tvalid_0's binary_logloss: 0.373146\tvalid_0's auc: 0.91154\n",
      "[1101]\tvalid_0's binary_logloss: 0.37316\tvalid_0's auc: 0.911535\n",
      "[1102]\tvalid_0's binary_logloss: 0.373118\tvalid_0's auc: 0.911554\n",
      "[1103]\tvalid_0's binary_logloss: 0.373181\tvalid_0's auc: 0.911539\n",
      "[1104]\tvalid_0's binary_logloss: 0.373183\tvalid_0's auc: 0.911534\n",
      "[1105]\tvalid_0's binary_logloss: 0.373175\tvalid_0's auc: 0.911537\n",
      "[1106]\tvalid_0's binary_logloss: 0.373224\tvalid_0's auc: 0.911522\n",
      "[1107]\tvalid_0's binary_logloss: 0.373235\tvalid_0's auc: 0.911522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1108]\tvalid_0's binary_logloss: 0.373217\tvalid_0's auc: 0.91153\n",
      "[1109]\tvalid_0's binary_logloss: 0.373267\tvalid_0's auc: 0.91152\n",
      "[1110]\tvalid_0's binary_logloss: 0.373267\tvalid_0's auc: 0.911524\n",
      "[1111]\tvalid_0's binary_logloss: 0.373249\tvalid_0's auc: 0.911537\n",
      "[1112]\tvalid_0's binary_logloss: 0.373229\tvalid_0's auc: 0.911544\n",
      "[1113]\tvalid_0's binary_logloss: 0.373242\tvalid_0's auc: 0.911529\n",
      "[1114]\tvalid_0's binary_logloss: 0.37326\tvalid_0's auc: 0.911516\n",
      "[1115]\tvalid_0's binary_logloss: 0.373267\tvalid_0's auc: 0.911508\n",
      "[1116]\tvalid_0's binary_logloss: 0.37326\tvalid_0's auc: 0.91151\n",
      "[1117]\tvalid_0's binary_logloss: 0.373259\tvalid_0's auc: 0.911516\n",
      "[1118]\tvalid_0's binary_logloss: 0.373272\tvalid_0's auc: 0.911513\n",
      "[1119]\tvalid_0's binary_logloss: 0.373291\tvalid_0's auc: 0.911505\n",
      "[1120]\tvalid_0's binary_logloss: 0.373281\tvalid_0's auc: 0.911513\n",
      "[1121]\tvalid_0's binary_logloss: 0.373311\tvalid_0's auc: 0.9115\n",
      "[1122]\tvalid_0's binary_logloss: 0.373322\tvalid_0's auc: 0.911504\n",
      "[1123]\tvalid_0's binary_logloss: 0.373312\tvalid_0's auc: 0.911506\n",
      "[1124]\tvalid_0's binary_logloss: 0.373332\tvalid_0's auc: 0.911502\n",
      "[1125]\tvalid_0's binary_logloss: 0.373322\tvalid_0's auc: 0.911505\n",
      "[1126]\tvalid_0's binary_logloss: 0.373373\tvalid_0's auc: 0.911478\n",
      "[1127]\tvalid_0's binary_logloss: 0.373389\tvalid_0's auc: 0.91148\n",
      "[1128]\tvalid_0's binary_logloss: 0.373376\tvalid_0's auc: 0.911487\n",
      "[1129]\tvalid_0's binary_logloss: 0.373409\tvalid_0's auc: 0.91148\n",
      "[1130]\tvalid_0's binary_logloss: 0.373391\tvalid_0's auc: 0.911484\n",
      "[1131]\tvalid_0's binary_logloss: 0.373367\tvalid_0's auc: 0.911495\n",
      "[1132]\tvalid_0's binary_logloss: 0.373371\tvalid_0's auc: 0.911488\n",
      "[1133]\tvalid_0's binary_logloss: 0.373382\tvalid_0's auc: 0.911487\n",
      "[1134]\tvalid_0's binary_logloss: 0.373392\tvalid_0's auc: 0.911486\n",
      "[1135]\tvalid_0's binary_logloss: 0.373386\tvalid_0's auc: 0.911478\n",
      "[1136]\tvalid_0's binary_logloss: 0.373406\tvalid_0's auc: 0.911466\n",
      "[1137]\tvalid_0's binary_logloss: 0.373438\tvalid_0's auc: 0.91145\n",
      "[1138]\tvalid_0's binary_logloss: 0.373438\tvalid_0's auc: 0.911448\n",
      "[1139]\tvalid_0's binary_logloss: 0.37346\tvalid_0's auc: 0.911437\n",
      "[1140]\tvalid_0's binary_logloss: 0.373508\tvalid_0's auc: 0.911415\n",
      "[1141]\tvalid_0's binary_logloss: 0.373522\tvalid_0's auc: 0.911404\n",
      "[1142]\tvalid_0's binary_logloss: 0.373542\tvalid_0's auc: 0.911393\n",
      "[1143]\tvalid_0's binary_logloss: 0.373566\tvalid_0's auc: 0.911391\n",
      "[1144]\tvalid_0's binary_logloss: 0.373588\tvalid_0's auc: 0.911379\n",
      "[1145]\tvalid_0's binary_logloss: 0.373608\tvalid_0's auc: 0.91137\n",
      "[1146]\tvalid_0's binary_logloss: 0.373608\tvalid_0's auc: 0.911381\n",
      "[1147]\tvalid_0's binary_logloss: 0.373608\tvalid_0's auc: 0.911382\n",
      "[1148]\tvalid_0's binary_logloss: 0.373636\tvalid_0's auc: 0.911368\n",
      "[1149]\tvalid_0's binary_logloss: 0.373651\tvalid_0's auc: 0.911373\n",
      "[1150]\tvalid_0's binary_logloss: 0.373654\tvalid_0's auc: 0.911372\n",
      "[1151]\tvalid_0's binary_logloss: 0.373671\tvalid_0's auc: 0.911372\n",
      "[1152]\tvalid_0's binary_logloss: 0.373659\tvalid_0's auc: 0.911376\n",
      "[1153]\tvalid_0's binary_logloss: 0.373657\tvalid_0's auc: 0.911378\n",
      "[1154]\tvalid_0's binary_logloss: 0.373659\tvalid_0's auc: 0.911379\n",
      "[1155]\tvalid_0's binary_logloss: 0.373695\tvalid_0's auc: 0.911364\n",
      "[1156]\tvalid_0's binary_logloss: 0.373687\tvalid_0's auc: 0.911381\n",
      "[1157]\tvalid_0's binary_logloss: 0.373656\tvalid_0's auc: 0.911403\n",
      "[1158]\tvalid_0's binary_logloss: 0.373639\tvalid_0's auc: 0.911411\n",
      "[1159]\tvalid_0's binary_logloss: 0.373681\tvalid_0's auc: 0.911402\n",
      "[1160]\tvalid_0's binary_logloss: 0.373656\tvalid_0's auc: 0.911427\n",
      "[1161]\tvalid_0's binary_logloss: 0.373704\tvalid_0's auc: 0.911416\n",
      "[1162]\tvalid_0's binary_logloss: 0.37372\tvalid_0's auc: 0.911418\n",
      "[1163]\tvalid_0's binary_logloss: 0.373728\tvalid_0's auc: 0.911421\n",
      "[1164]\tvalid_0's binary_logloss: 0.373746\tvalid_0's auc: 0.911416\n",
      "[1165]\tvalid_0's binary_logloss: 0.373798\tvalid_0's auc: 0.911398\n",
      "[1166]\tvalid_0's binary_logloss: 0.3738\tvalid_0's auc: 0.911392\n",
      "[1167]\tvalid_0's binary_logloss: 0.373821\tvalid_0's auc: 0.911377\n",
      "[1168]\tvalid_0's binary_logloss: 0.373816\tvalid_0's auc: 0.911378\n",
      "[1169]\tvalid_0's binary_logloss: 0.373837\tvalid_0's auc: 0.911368\n",
      "[1170]\tvalid_0's binary_logloss: 0.373848\tvalid_0's auc: 0.911366\n",
      "[1171]\tvalid_0's binary_logloss: 0.373869\tvalid_0's auc: 0.91136\n",
      "[1172]\tvalid_0's binary_logloss: 0.373851\tvalid_0's auc: 0.911366\n",
      "[1173]\tvalid_0's binary_logloss: 0.373858\tvalid_0's auc: 0.911367\n",
      "[1174]\tvalid_0's binary_logloss: 0.373887\tvalid_0's auc: 0.911354\n",
      "[1175]\tvalid_0's binary_logloss: 0.373872\tvalid_0's auc: 0.911348\n",
      "[1176]\tvalid_0's binary_logloss: 0.37386\tvalid_0's auc: 0.91135\n",
      "[1177]\tvalid_0's binary_logloss: 0.373814\tvalid_0's auc: 0.911366\n",
      "[1178]\tvalid_0's binary_logloss: 0.373806\tvalid_0's auc: 0.911371\n",
      "[1179]\tvalid_0's binary_logloss: 0.373811\tvalid_0's auc: 0.91137\n",
      "[1180]\tvalid_0's binary_logloss: 0.373803\tvalid_0's auc: 0.911384\n",
      "[1181]\tvalid_0's binary_logloss: 0.373856\tvalid_0's auc: 0.911365\n",
      "[1182]\tvalid_0's binary_logloss: 0.373869\tvalid_0's auc: 0.911369\n",
      "[1183]\tvalid_0's binary_logloss: 0.373876\tvalid_0's auc: 0.911379\n",
      "[1184]\tvalid_0's binary_logloss: 0.373881\tvalid_0's auc: 0.911375\n",
      "[1185]\tvalid_0's binary_logloss: 0.37386\tvalid_0's auc: 0.911388\n",
      "[1186]\tvalid_0's binary_logloss: 0.373849\tvalid_0's auc: 0.9114\n",
      "[1187]\tvalid_0's binary_logloss: 0.37384\tvalid_0's auc: 0.911404\n",
      "[1188]\tvalid_0's binary_logloss: 0.373842\tvalid_0's auc: 0.911397\n",
      "[1189]\tvalid_0's binary_logloss: 0.373824\tvalid_0's auc: 0.911401\n",
      "[1190]\tvalid_0's binary_logloss: 0.373822\tvalid_0's auc: 0.911406\n",
      "[1191]\tvalid_0's binary_logloss: 0.373842\tvalid_0's auc: 0.911403\n",
      "[1192]\tvalid_0's binary_logloss: 0.373863\tvalid_0's auc: 0.911392\n",
      "[1193]\tvalid_0's binary_logloss: 0.373846\tvalid_0's auc: 0.911394\n",
      "[1194]\tvalid_0's binary_logloss: 0.373864\tvalid_0's auc: 0.91138\n",
      "[1195]\tvalid_0's binary_logloss: 0.373838\tvalid_0's auc: 0.911393\n",
      "[1196]\tvalid_0's binary_logloss: 0.373862\tvalid_0's auc: 0.911382\n",
      "[1197]\tvalid_0's binary_logloss: 0.373886\tvalid_0's auc: 0.911369\n",
      "[1198]\tvalid_0's binary_logloss: 0.373896\tvalid_0's auc: 0.911364\n",
      "[1199]\tvalid_0's binary_logloss: 0.37391\tvalid_0's auc: 0.911351\n",
      "[1200]\tvalid_0's binary_logloss: 0.373878\tvalid_0's auc: 0.911367\n",
      "[1201]\tvalid_0's binary_logloss: 0.373868\tvalid_0's auc: 0.911373\n",
      "[1202]\tvalid_0's binary_logloss: 0.373881\tvalid_0's auc: 0.911372\n",
      "[1203]\tvalid_0's binary_logloss: 0.373898\tvalid_0's auc: 0.911361\n",
      "[1204]\tvalid_0's binary_logloss: 0.373926\tvalid_0's auc: 0.91135\n",
      "[1205]\tvalid_0's binary_logloss: 0.373899\tvalid_0's auc: 0.911351\n",
      "[1206]\tvalid_0's binary_logloss: 0.37392\tvalid_0's auc: 0.911347\n",
      "[1207]\tvalid_0's binary_logloss: 0.373921\tvalid_0's auc: 0.911351\n",
      "[1208]\tvalid_0's binary_logloss: 0.373944\tvalid_0's auc: 0.911339\n",
      "[1209]\tvalid_0's binary_logloss: 0.373953\tvalid_0's auc: 0.911334\n",
      "[1210]\tvalid_0's binary_logloss: 0.373964\tvalid_0's auc: 0.911339\n",
      "[1211]\tvalid_0's binary_logloss: 0.37395\tvalid_0's auc: 0.911352\n",
      "[1212]\tvalid_0's binary_logloss: 0.373943\tvalid_0's auc: 0.911362\n",
      "[1213]\tvalid_0's binary_logloss: 0.373941\tvalid_0's auc: 0.911365\n",
      "[1214]\tvalid_0's binary_logloss: 0.373931\tvalid_0's auc: 0.911375\n",
      "[1215]\tvalid_0's binary_logloss: 0.373942\tvalid_0's auc: 0.911368\n",
      "[1216]\tvalid_0's binary_logloss: 0.373947\tvalid_0's auc: 0.911371\n",
      "[1217]\tvalid_0's binary_logloss: 0.373939\tvalid_0's auc: 0.91138\n",
      "[1218]\tvalid_0's binary_logloss: 0.373902\tvalid_0's auc: 0.911402\n",
      "[1219]\tvalid_0's binary_logloss: 0.373957\tvalid_0's auc: 0.911377\n",
      "[1220]\tvalid_0's binary_logloss: 0.373961\tvalid_0's auc: 0.911382\n",
      "[1221]\tvalid_0's binary_logloss: 0.373971\tvalid_0's auc: 0.911377\n",
      "[1222]\tvalid_0's binary_logloss: 0.373992\tvalid_0's auc: 0.911369\n",
      "[1223]\tvalid_0's binary_logloss: 0.374007\tvalid_0's auc: 0.911355\n",
      "[1224]\tvalid_0's binary_logloss: 0.374048\tvalid_0's auc: 0.911342\n",
      "[1225]\tvalid_0's binary_logloss: 0.374069\tvalid_0's auc: 0.911328\n",
      "[1226]\tvalid_0's binary_logloss: 0.374077\tvalid_0's auc: 0.911322\n",
      "[1227]\tvalid_0's binary_logloss: 0.374084\tvalid_0's auc: 0.911319\n",
      "[1228]\tvalid_0's binary_logloss: 0.374102\tvalid_0's auc: 0.911306\n",
      "[1229]\tvalid_0's binary_logloss: 0.374106\tvalid_0's auc: 0.911306\n",
      "[1230]\tvalid_0's binary_logloss: 0.374118\tvalid_0's auc: 0.911301\n",
      "[1231]\tvalid_0's binary_logloss: 0.374117\tvalid_0's auc: 0.911303\n",
      "[1232]\tvalid_0's binary_logloss: 0.374106\tvalid_0's auc: 0.911306\n",
      "[1233]\tvalid_0's binary_logloss: 0.374122\tvalid_0's auc: 0.911302\n",
      "[1234]\tvalid_0's binary_logloss: 0.374123\tvalid_0's auc: 0.911301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1235]\tvalid_0's binary_logloss: 0.374129\tvalid_0's auc: 0.911305\n",
      "[1236]\tvalid_0's binary_logloss: 0.374134\tvalid_0's auc: 0.911308\n",
      "[1237]\tvalid_0's binary_logloss: 0.374141\tvalid_0's auc: 0.911307\n",
      "[1238]\tvalid_0's binary_logloss: 0.374124\tvalid_0's auc: 0.91132\n",
      "[1239]\tvalid_0's binary_logloss: 0.374165\tvalid_0's auc: 0.911305\n",
      "[1240]\tvalid_0's binary_logloss: 0.374127\tvalid_0's auc: 0.911314\n",
      "[1241]\tvalid_0's binary_logloss: 0.37413\tvalid_0's auc: 0.91132\n",
      "[1242]\tvalid_0's binary_logloss: 0.374114\tvalid_0's auc: 0.911328\n",
      "[1243]\tvalid_0's binary_logloss: 0.374112\tvalid_0's auc: 0.91134\n",
      "[1244]\tvalid_0's binary_logloss: 0.374099\tvalid_0's auc: 0.911339\n",
      "[1245]\tvalid_0's binary_logloss: 0.37409\tvalid_0's auc: 0.911346\n",
      "[1246]\tvalid_0's binary_logloss: 0.374063\tvalid_0's auc: 0.911361\n",
      "[1247]\tvalid_0's binary_logloss: 0.374036\tvalid_0's auc: 0.911374\n",
      "[1248]\tvalid_0's binary_logloss: 0.374019\tvalid_0's auc: 0.911377\n",
      "[1249]\tvalid_0's binary_logloss: 0.374034\tvalid_0's auc: 0.911367\n",
      "[1250]\tvalid_0's binary_logloss: 0.374017\tvalid_0's auc: 0.91137\n",
      "[1251]\tvalid_0's binary_logloss: 0.374023\tvalid_0's auc: 0.911369\n",
      "[1252]\tvalid_0's binary_logloss: 0.374024\tvalid_0's auc: 0.911362\n",
      "[1253]\tvalid_0's binary_logloss: 0.374045\tvalid_0's auc: 0.911354\n",
      "[1254]\tvalid_0's binary_logloss: 0.374032\tvalid_0's auc: 0.911365\n",
      "[1255]\tvalid_0's binary_logloss: 0.373999\tvalid_0's auc: 0.911371\n",
      "[1256]\tvalid_0's binary_logloss: 0.373978\tvalid_0's auc: 0.91139\n",
      "[1257]\tvalid_0's binary_logloss: 0.373995\tvalid_0's auc: 0.911384\n",
      "[1258]\tvalid_0's binary_logloss: 0.374005\tvalid_0's auc: 0.91139\n",
      "[1259]\tvalid_0's binary_logloss: 0.374014\tvalid_0's auc: 0.91139\n",
      "[1260]\tvalid_0's binary_logloss: 0.374039\tvalid_0's auc: 0.91138\n",
      "[1261]\tvalid_0's binary_logloss: 0.374027\tvalid_0's auc: 0.911385\n",
      "[1262]\tvalid_0's binary_logloss: 0.374037\tvalid_0's auc: 0.911381\n",
      "[1263]\tvalid_0's binary_logloss: 0.374052\tvalid_0's auc: 0.911376\n",
      "[1264]\tvalid_0's binary_logloss: 0.37402\tvalid_0's auc: 0.911391\n",
      "[1265]\tvalid_0's binary_logloss: 0.374019\tvalid_0's auc: 0.911395\n",
      "[1266]\tvalid_0's binary_logloss: 0.374004\tvalid_0's auc: 0.911399\n",
      "[1267]\tvalid_0's binary_logloss: 0.374014\tvalid_0's auc: 0.911392\n",
      "[1268]\tvalid_0's binary_logloss: 0.374015\tvalid_0's auc: 0.911392\n",
      "[1269]\tvalid_0's binary_logloss: 0.374028\tvalid_0's auc: 0.911388\n",
      "[1270]\tvalid_0's binary_logloss: 0.374024\tvalid_0's auc: 0.911391\n",
      "[1271]\tvalid_0's binary_logloss: 0.374017\tvalid_0's auc: 0.911396\n",
      "[1272]\tvalid_0's binary_logloss: 0.37401\tvalid_0's auc: 0.911406\n",
      "[1273]\tvalid_0's binary_logloss: 0.37402\tvalid_0's auc: 0.911401\n",
      "[1274]\tvalid_0's binary_logloss: 0.37404\tvalid_0's auc: 0.911387\n",
      "[1275]\tvalid_0's binary_logloss: 0.374066\tvalid_0's auc: 0.911387\n",
      "[1276]\tvalid_0's binary_logloss: 0.374093\tvalid_0's auc: 0.911372\n",
      "[1277]\tvalid_0's binary_logloss: 0.374103\tvalid_0's auc: 0.911368\n",
      "[1278]\tvalid_0's binary_logloss: 0.374069\tvalid_0's auc: 0.911388\n",
      "[1279]\tvalid_0's binary_logloss: 0.37406\tvalid_0's auc: 0.911393\n",
      "[1280]\tvalid_0's binary_logloss: 0.37407\tvalid_0's auc: 0.911386\n",
      "[1281]\tvalid_0's binary_logloss: 0.374087\tvalid_0's auc: 0.911386\n",
      "[1282]\tvalid_0's binary_logloss: 0.374115\tvalid_0's auc: 0.911382\n",
      "[1283]\tvalid_0's binary_logloss: 0.374082\tvalid_0's auc: 0.911396\n",
      "[1284]\tvalid_0's binary_logloss: 0.374079\tvalid_0's auc: 0.911398\n",
      "[1285]\tvalid_0's binary_logloss: 0.374104\tvalid_0's auc: 0.911394\n",
      "[1286]\tvalid_0's binary_logloss: 0.374074\tvalid_0's auc: 0.911402\n",
      "[1287]\tvalid_0's binary_logloss: 0.374065\tvalid_0's auc: 0.911404\n",
      "[1288]\tvalid_0's binary_logloss: 0.37405\tvalid_0's auc: 0.911405\n",
      "[1289]\tvalid_0's binary_logloss: 0.374048\tvalid_0's auc: 0.911404\n",
      "[1290]\tvalid_0's binary_logloss: 0.374063\tvalid_0's auc: 0.911391\n",
      "[1291]\tvalid_0's binary_logloss: 0.374023\tvalid_0's auc: 0.911412\n",
      "[1292]\tvalid_0's binary_logloss: 0.374008\tvalid_0's auc: 0.911418\n",
      "[1293]\tvalid_0's binary_logloss: 0.374018\tvalid_0's auc: 0.91141\n",
      "[1294]\tvalid_0's binary_logloss: 0.373996\tvalid_0's auc: 0.911412\n",
      "[1295]\tvalid_0's binary_logloss: 0.373982\tvalid_0's auc: 0.91142\n",
      "[1296]\tvalid_0's binary_logloss: 0.373989\tvalid_0's auc: 0.911419\n",
      "[1297]\tvalid_0's binary_logloss: 0.374027\tvalid_0's auc: 0.91141\n",
      "[1298]\tvalid_0's binary_logloss: 0.374037\tvalid_0's auc: 0.911401\n",
      "[1299]\tvalid_0's binary_logloss: 0.374065\tvalid_0's auc: 0.91139\n",
      "[1300]\tvalid_0's binary_logloss: 0.374083\tvalid_0's auc: 0.911391\n",
      "[1301]\tvalid_0's binary_logloss: 0.37409\tvalid_0's auc: 0.911385\n",
      "[1302]\tvalid_0's binary_logloss: 0.374088\tvalid_0's auc: 0.911392\n",
      "[1303]\tvalid_0's binary_logloss: 0.374117\tvalid_0's auc: 0.911381\n",
      "[1304]\tvalid_0's binary_logloss: 0.374129\tvalid_0's auc: 0.911371\n",
      "[1305]\tvalid_0's binary_logloss: 0.374136\tvalid_0's auc: 0.911366\n",
      "[1306]\tvalid_0's binary_logloss: 0.37413\tvalid_0's auc: 0.911368\n",
      "[1307]\tvalid_0's binary_logloss: 0.374108\tvalid_0's auc: 0.911382\n",
      "[1308]\tvalid_0's binary_logloss: 0.37414\tvalid_0's auc: 0.911372\n",
      "[1309]\tvalid_0's binary_logloss: 0.374138\tvalid_0's auc: 0.911377\n",
      "[1310]\tvalid_0's binary_logloss: 0.374157\tvalid_0's auc: 0.911367\n",
      "[1311]\tvalid_0's binary_logloss: 0.374174\tvalid_0's auc: 0.911364\n",
      "[1312]\tvalid_0's binary_logloss: 0.374183\tvalid_0's auc: 0.911361\n",
      "[1313]\tvalid_0's binary_logloss: 0.374202\tvalid_0's auc: 0.911352\n",
      "[1314]\tvalid_0's binary_logloss: 0.374214\tvalid_0's auc: 0.911343\n",
      "[1315]\tvalid_0's binary_logloss: 0.374213\tvalid_0's auc: 0.911349\n",
      "[1316]\tvalid_0's binary_logloss: 0.374238\tvalid_0's auc: 0.911343\n",
      "[1317]\tvalid_0's binary_logloss: 0.37423\tvalid_0's auc: 0.911348\n",
      "[1318]\tvalid_0's binary_logloss: 0.374249\tvalid_0's auc: 0.911343\n",
      "[1319]\tvalid_0's binary_logloss: 0.374258\tvalid_0's auc: 0.911332\n",
      "[1320]\tvalid_0's binary_logloss: 0.374249\tvalid_0's auc: 0.911338\n",
      "[1321]\tvalid_0's binary_logloss: 0.374261\tvalid_0's auc: 0.911342\n",
      "[1322]\tvalid_0's binary_logloss: 0.374295\tvalid_0's auc: 0.911331\n",
      "[1323]\tvalid_0's binary_logloss: 0.374294\tvalid_0's auc: 0.911336\n",
      "[1324]\tvalid_0's binary_logloss: 0.374294\tvalid_0's auc: 0.911341\n",
      "[1325]\tvalid_0's binary_logloss: 0.374309\tvalid_0's auc: 0.911335\n",
      "[1326]\tvalid_0's binary_logloss: 0.374297\tvalid_0's auc: 0.91134\n",
      "[1327]\tvalid_0's binary_logloss: 0.374335\tvalid_0's auc: 0.911336\n",
      "[1328]\tvalid_0's binary_logloss: 0.374323\tvalid_0's auc: 0.911339\n",
      "[1329]\tvalid_0's binary_logloss: 0.374341\tvalid_0's auc: 0.91132\n",
      "[1330]\tvalid_0's binary_logloss: 0.374338\tvalid_0's auc: 0.911326\n",
      "[1331]\tvalid_0's binary_logloss: 0.374316\tvalid_0's auc: 0.911335\n",
      "[1332]\tvalid_0's binary_logloss: 0.374299\tvalid_0's auc: 0.911348\n",
      "[1333]\tvalid_0's binary_logloss: 0.374308\tvalid_0's auc: 0.911348\n",
      "[1334]\tvalid_0's binary_logloss: 0.374311\tvalid_0's auc: 0.911353\n",
      "[1335]\tvalid_0's binary_logloss: 0.374309\tvalid_0's auc: 0.911352\n",
      "[1336]\tvalid_0's binary_logloss: 0.374289\tvalid_0's auc: 0.911356\n",
      "[1337]\tvalid_0's binary_logloss: 0.374281\tvalid_0's auc: 0.911361\n",
      "[1338]\tvalid_0's binary_logloss: 0.374248\tvalid_0's auc: 0.911376\n",
      "[1339]\tvalid_0's binary_logloss: 0.374282\tvalid_0's auc: 0.911352\n",
      "[1340]\tvalid_0's binary_logloss: 0.374255\tvalid_0's auc: 0.911359\n",
      "[1341]\tvalid_0's binary_logloss: 0.374239\tvalid_0's auc: 0.911381\n",
      "[1342]\tvalid_0's binary_logloss: 0.374229\tvalid_0's auc: 0.911383\n",
      "[1343]\tvalid_0's binary_logloss: 0.374226\tvalid_0's auc: 0.911392\n",
      "[1344]\tvalid_0's binary_logloss: 0.37425\tvalid_0's auc: 0.911382\n",
      "[1345]\tvalid_0's binary_logloss: 0.374218\tvalid_0's auc: 0.91139\n",
      "[1346]\tvalid_0's binary_logloss: 0.374238\tvalid_0's auc: 0.911386\n",
      "[1347]\tvalid_0's binary_logloss: 0.374221\tvalid_0's auc: 0.9114\n",
      "[1348]\tvalid_0's binary_logloss: 0.374229\tvalid_0's auc: 0.911396\n",
      "[1349]\tvalid_0's binary_logloss: 0.374214\tvalid_0's auc: 0.911399\n",
      "[1350]\tvalid_0's binary_logloss: 0.374197\tvalid_0's auc: 0.911397\n",
      "[1351]\tvalid_0's binary_logloss: 0.374214\tvalid_0's auc: 0.911396\n",
      "[1352]\tvalid_0's binary_logloss: 0.374216\tvalid_0's auc: 0.911398\n",
      "[1353]\tvalid_0's binary_logloss: 0.374234\tvalid_0's auc: 0.911391\n",
      "Early stopping, best iteration is:\n",
      "[853]\tvalid_0's binary_logloss: 0.372579\tvalid_0's auc: 0.911596\n"
     ]
    }
   ],
   "source": [
    "gbm = lgb.train(params,\n",
    "               lgb_train,\n",
    "               num_boost_round=10000,\n",
    "               valid_sets=lgb_eval,\n",
    "               early_stopping_rounds=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbm_cv = lgb.cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9,   6,  19,  28,  38,   0,   0,   2,   9,   8,   3,   8,   9,\n",
       "        21,   2,   0,   5,  19,  14,  31,  16,  22,   9,  29,  50,  40,\n",
       "        32,   6,  20,  35,  39,  15,   0,   0,   4,   9,   0,  21,  28,\n",
       "        24,  30,   0,  32,  36,   6,  32,  43,  17,  26,  12,  16,  19,\n",
       "         5,  18,   3,   0,   0,   0,   0,   0,   0,   0,   1,  13,   3,\n",
       "         0,   0,   0,   6,   1,  15,   3,  65,  29,  56,  30,  35,  18,\n",
       "         9,   6,  17,   0,  41,   0,   9,   0,   0, 120,  42,  27,  51,\n",
       "        10,   6,  41,  21,  29,  31,   2,  41,  27,  39,  80,  23,  47,\n",
       "        23,  43,  31,  38,  45,  25,   9,  17,  66,  33,   0,   0,   0,\n",
       "         0,   0,  38,  34,  40,  38,   0,  37,  41,   4,  17,  32,  49,\n",
       "        34,  26,   5,  17,  12,  15,   2,   0,   0,   0,   0,   0,   0,\n",
       "         0,   3,  17,   1,   1,   0,   4,   8,   6,  19,  14,  37,  41,\n",
       "        62,  15,  49,  45,  23,  15,  15,   0,  60,   0,   0,   0,   0,\n",
       "        47, 135,  48,  47,  10])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "before_sumbit_df = pd.read_csv(path%'total_app_var_before.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_name = list(before_sumbit_df.iloc[:, :-1 ].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importance= gbm.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name:</th>\n",
       "      <th>f_imp:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a.login_cnt_07</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a.login_cnt_15</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a.login_cnt_30</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a.login_cnt_60</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a.login_cnt_90</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a.login_agent_cnt_07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a.login_agent_cnt_15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a.login_agent_cnt_30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a.login_agent_cnt_60</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a.login_agent_cnt_90</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a.register_cnt_7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a.register_tot_times_7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>a.register_avg_times_7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>a.register_min_times_7</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>a.register_max_times_7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>a.register_succ_cust_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>a.gesturesetup_cnt_7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>a.gesturesetup_tot_times_7</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>a.gesturesetup_avg_times_7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a.gesturesetup_min_times_7</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>a.gesturesetup_max_times_7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>a.login_tot_cnt_7</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>a.login_by_cnt_7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a.login_session_tot_times_7</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>a.login_session_avg_times_7</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>a.login_session_min_times_7</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>a.login_session_max_times_7</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>a.login_cnt_07_7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>a.login_cnt_15_7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>a.login_cnt_30_7</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>a.midlevy_tot_click_30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>a.midlevy_tot_times_30</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>a.midlevy_avg_times_30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>a.midlevy_min_times_30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>a.midlevy_max_times_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>a.levycredit_tot_click_30</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>a.levycredit_tot_times_30</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>a.levycredit_avg_times_30</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>a.levycredit_min_times_30</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>a.levycredit_max_times_30</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>a.creditloan_tot_click_30</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>a.creditloan_tot_times_30</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>a.creditloan_avg_times_30</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>a.creditloan_min_times_30</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>a.creditloan_max_times_30</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>a.foru_tot_click_30</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>a.foru_tot_times_30</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>a.foru_avg_times_30</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>a.foru_min_times_30</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>a.app_vision_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>a.app_vision_cnt_30</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>a.os_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>a.os_change_cnt_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>a.install_time_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>a.recent_time_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>a.online_30</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>a.order_gap_30</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>a.after_order_30</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>a.time_gap_mean_30</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>a.foru_max_times_30</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       col_name:  f_imp:\n",
       "0                 a.login_cnt_07       9\n",
       "1                 a.login_cnt_15       6\n",
       "2                 a.login_cnt_30      19\n",
       "3                 a.login_cnt_60      28\n",
       "4                 a.login_cnt_90      38\n",
       "5           a.login_agent_cnt_07       0\n",
       "6           a.login_agent_cnt_15       0\n",
       "7           a.login_agent_cnt_30       2\n",
       "8           a.login_agent_cnt_60       9\n",
       "9           a.login_agent_cnt_90       8\n",
       "10              a.register_cnt_7       3\n",
       "11        a.register_tot_times_7       8\n",
       "12        a.register_avg_times_7       9\n",
       "13        a.register_min_times_7      21\n",
       "14        a.register_max_times_7       2\n",
       "15        a.register_succ_cust_7       0\n",
       "16          a.gesturesetup_cnt_7       5\n",
       "17    a.gesturesetup_tot_times_7      19\n",
       "18    a.gesturesetup_avg_times_7      14\n",
       "19    a.gesturesetup_min_times_7      31\n",
       "20    a.gesturesetup_max_times_7      16\n",
       "21             a.login_tot_cnt_7      22\n",
       "22              a.login_by_cnt_7       9\n",
       "23   a.login_session_tot_times_7      29\n",
       "24   a.login_session_avg_times_7      50\n",
       "25   a.login_session_min_times_7      40\n",
       "26   a.login_session_max_times_7      32\n",
       "27              a.login_cnt_07_7       6\n",
       "28              a.login_cnt_15_7      20\n",
       "29              a.login_cnt_30_7      35\n",
       "..                           ...     ...\n",
       "144       a.midlevy_tot_click_30       3\n",
       "145       a.midlevy_tot_times_30      17\n",
       "146       a.midlevy_avg_times_30       1\n",
       "147       a.midlevy_min_times_30       1\n",
       "148       a.midlevy_max_times_30       0\n",
       "149    a.levycredit_tot_click_30       4\n",
       "150    a.levycredit_tot_times_30       8\n",
       "151    a.levycredit_avg_times_30       6\n",
       "152    a.levycredit_min_times_30      19\n",
       "153    a.levycredit_max_times_30      14\n",
       "154    a.creditloan_tot_click_30      37\n",
       "155    a.creditloan_tot_times_30      41\n",
       "156    a.creditloan_avg_times_30      62\n",
       "157    a.creditloan_min_times_30      15\n",
       "158    a.creditloan_max_times_30      49\n",
       "159          a.foru_tot_click_30      45\n",
       "160          a.foru_tot_times_30      23\n",
       "161          a.foru_avg_times_30      15\n",
       "162          a.foru_min_times_30      15\n",
       "163              a.app_vision_30       0\n",
       "164          a.app_vision_cnt_30      60\n",
       "165                      a.os_30       0\n",
       "166           a.os_change_cnt_30       0\n",
       "167            a.install_time_30       0\n",
       "168             a.recent_time_30       0\n",
       "169                  a.online_30      47\n",
       "170               a.order_gap_30     135\n",
       "171             a.after_order_30      48\n",
       "172           a.time_gap_mean_30      47\n",
       "173          a.foru_max_times_30      10\n",
       "\n",
       "[174 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'col_name:' : col_name, 'f_imp:': feature_importance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = pd.DataFrame({'col_name:' : col_name, 'f_imp:': feature_importance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name:</th>\n",
       "      <th>f_imp:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>a.order_gap_30</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>a.online_7</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>a.gesturesetup_min_times_30</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>a.login_cnt_60_30</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>a.creditloan_tot_click_7</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>a.creditloan_avg_times_30</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>a.app_vision_cnt_30</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>a.creditloan_avg_times_7</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>a.time_gap_mean_7</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>a.login_session_avg_times_7</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>a.mypage_tot_click_30</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>a.creditloan_max_times_30</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>a.after_order_30</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>a.login_tot_cnt_30</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>a.time_gap_mean_30</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>a.online_30</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>a.foru_tot_click_30</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>a.login_session_max_times_30</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>a.home_max_times_7</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>a.login_session_tot_times_30</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>a.order_gap_7</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>a.home_tot_times_30</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>a.app_vision_cnt_7</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>a.register_tot_times_30</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>a.creditloan_tot_times_30</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>a.gesturesetup_cnt_30</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>a.login_session_min_times_7</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>a.login_timeslot_12_14_30</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>a.login_cnt_60_7</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>a.gesturesetup_avg_times_30</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>a.install_time_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>a.recent_time_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a.login_agent_cnt_15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a.login_agent_cnt_07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>a.midlevy_max_times_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>a.mycard_max_times_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>a.midlevy_max_times_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>a.midlevy_min_times_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>a.login_agent_cnt_07_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>a.login_agent_cnt_15_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>a.login_agent_cnt_30_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>a.login_agent_cnt_60_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>a.login_agent_cnt_90_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>a.mycard_max_times_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>a.mycard_min_times_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>a.login_timeslot_21_23_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>a.mycard_avg_times_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>a.mycard_tot_times_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>a.mycard_tot_click_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>a.realname_max_times_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>a.realname_min_times_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>a.levycredit_tot_click_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>a.login_timeslot_21_23_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>a.realname_min_times_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>a.realname_max_times_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>a.mycard_tot_click_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>a.mycard_tot_times_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>a.mycard_avg_times_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>a.mycard_min_times_30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>a.login_agent_cnt_90_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        col_name:  f_imp:\n",
       "170                a.order_gap_30     135\n",
       "87                     a.online_7     120\n",
       "101   a.gesturesetup_min_times_30      80\n",
       "112             a.login_cnt_60_30      66\n",
       "72       a.creditloan_tot_click_7      65\n",
       "156     a.creditloan_avg_times_30      62\n",
       "164           a.app_vision_cnt_30      60\n",
       "74       a.creditloan_avg_times_7      56\n",
       "90              a.time_gap_mean_7      51\n",
       "24    a.login_session_avg_times_7      50\n",
       "129         a.mypage_tot_click_30      49\n",
       "158     a.creditloan_max_times_30      49\n",
       "171              a.after_order_30      48\n",
       "103            a.login_tot_cnt_30      47\n",
       "172            a.time_gap_mean_30      47\n",
       "169                   a.online_30      47\n",
       "159           a.foru_tot_click_30      45\n",
       "108  a.login_session_max_times_30      45\n",
       "46             a.home_max_times_7      43\n",
       "105  a.login_session_tot_times_30      43\n",
       "88                  a.order_gap_7      42\n",
       "125           a.home_tot_times_30      41\n",
       "82             a.app_vision_cnt_7      41\n",
       "93        a.register_tot_times_30      41\n",
       "155     a.creditloan_tot_times_30      41\n",
       "98          a.gesturesetup_cnt_30      41\n",
       "25    a.login_session_min_times_7      40\n",
       "121     a.login_timeslot_12_14_30      40\n",
       "30               a.login_cnt_60_7      39\n",
       "100   a.gesturesetup_avg_times_30      39\n",
       "..                            ...     ...\n",
       "167             a.install_time_30       0\n",
       "168              a.recent_time_30       0\n",
       "6            a.login_agent_cnt_15       0\n",
       "5            a.login_agent_cnt_07       0\n",
       "66          a.midlevy_max_times_7       0\n",
       "143         a.mycard_max_times_30       0\n",
       "148        a.midlevy_max_times_30       0\n",
       "65          a.midlevy_min_times_7       0\n",
       "114       a.login_agent_cnt_07_30       0\n",
       "115       a.login_agent_cnt_15_30       0\n",
       "116       a.login_agent_cnt_30_30       0\n",
       "117       a.login_agent_cnt_60_30       0\n",
       "118       a.login_agent_cnt_90_30       0\n",
       "61           a.mycard_max_times_7       0\n",
       "60           a.mycard_min_times_7       0\n",
       "123     a.login_timeslot_21_23_30       0\n",
       "59           a.mycard_avg_times_7       0\n",
       "58           a.mycard_tot_times_7       0\n",
       "57           a.mycard_tot_click_7       0\n",
       "56         a.realname_max_times_7       0\n",
       "55         a.realname_min_times_7       0\n",
       "67       a.levycredit_tot_click_7       0\n",
       "41       a.login_timeslot_21_23_7       0\n",
       "137       a.realname_min_times_30       0\n",
       "138       a.realname_max_times_30       0\n",
       "139         a.mycard_tot_click_30       0\n",
       "140         a.mycard_tot_times_30       0\n",
       "141         a.mycard_avg_times_30       0\n",
       "142         a.mycard_min_times_30       0\n",
       "36         a.login_agent_cnt_90_7       0\n",
       "\n",
       "[174 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.sort_values(by='f_imp:', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name:</th>\n",
       "      <th>f_imp:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>a.order_gap_30</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>a.online_7</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>a.gesturesetup_min_times_30</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>a.login_cnt_60_30</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>a.creditloan_tot_click_7</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>a.creditloan_avg_times_30</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>a.app_vision_cnt_30</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>a.creditloan_avg_times_7</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>a.time_gap_mean_7</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>a.login_session_avg_times_7</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>a.mypage_tot_click_30</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>a.creditloan_max_times_30</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>a.after_order_30</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>a.login_tot_cnt_30</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>a.time_gap_mean_30</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>a.online_30</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>a.foru_tot_click_30</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>a.login_session_max_times_30</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>a.home_max_times_7</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>a.login_session_tot_times_30</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>a.order_gap_7</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>a.home_tot_times_30</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>a.app_vision_cnt_7</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>a.register_tot_times_30</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>a.creditloan_tot_times_30</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>a.gesturesetup_cnt_30</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>a.login_session_min_times_7</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>a.login_timeslot_12_14_30</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>a.login_cnt_60_7</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>a.gesturesetup_avg_times_30</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>a.login_timeslot_15_20_30</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>a.login_session_min_times_30</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a.login_cnt_90</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>a.login_timeslot_0_6_30</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>a.home_tot_click_30</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>a.creditloan_tot_click_30</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>a.home_tot_times_7</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>a.creditloan_max_times_7</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>a.login_cnt_30_7</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>a.mypage_tot_times_30</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>a.login_timeslot_7_11_30</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>a.login_cnt_90_30</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>a.home_tot_click_7</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>a.home_min_times_7</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>a.login_session_max_times_7</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>a.home_max_times_30</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>a.login_session_avg_times_30</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a.gesturesetup_min_times_7</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>a.register_max_times_30</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>a.login_timeslot_15_20_7</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        col_name:  f_imp:\n",
       "170                a.order_gap_30     135\n",
       "87                     a.online_7     120\n",
       "101   a.gesturesetup_min_times_30      80\n",
       "112             a.login_cnt_60_30      66\n",
       "72       a.creditloan_tot_click_7      65\n",
       "156     a.creditloan_avg_times_30      62\n",
       "164           a.app_vision_cnt_30      60\n",
       "74       a.creditloan_avg_times_7      56\n",
       "90              a.time_gap_mean_7      51\n",
       "24    a.login_session_avg_times_7      50\n",
       "129         a.mypage_tot_click_30      49\n",
       "158     a.creditloan_max_times_30      49\n",
       "171              a.after_order_30      48\n",
       "103            a.login_tot_cnt_30      47\n",
       "172            a.time_gap_mean_30      47\n",
       "169                   a.online_30      47\n",
       "159           a.foru_tot_click_30      45\n",
       "108  a.login_session_max_times_30      45\n",
       "46             a.home_max_times_7      43\n",
       "105  a.login_session_tot_times_30      43\n",
       "88                  a.order_gap_7      42\n",
       "125           a.home_tot_times_30      41\n",
       "82             a.app_vision_cnt_7      41\n",
       "93        a.register_tot_times_30      41\n",
       "155     a.creditloan_tot_times_30      41\n",
       "98          a.gesturesetup_cnt_30      41\n",
       "25    a.login_session_min_times_7      40\n",
       "121     a.login_timeslot_12_14_30      40\n",
       "30               a.login_cnt_60_7      39\n",
       "100   a.gesturesetup_avg_times_30      39\n",
       "122     a.login_timeslot_15_20_30      38\n",
       "107  a.login_session_min_times_30      38\n",
       "4                  a.login_cnt_90      38\n",
       "119       a.login_timeslot_0_6_30      38\n",
       "124           a.home_tot_click_30      37\n",
       "154     a.creditloan_tot_click_30      37\n",
       "43             a.home_tot_times_7      36\n",
       "76       a.creditloan_max_times_7      35\n",
       "29               a.login_cnt_30_7      35\n",
       "130         a.mypage_tot_times_30      34\n",
       "120      a.login_timeslot_7_11_30      34\n",
       "113             a.login_cnt_90_30      33\n",
       "42             a.home_tot_click_7      32\n",
       "45             a.home_min_times_7      32\n",
       "26    a.login_session_max_times_7      32\n",
       "128           a.home_max_times_30      32\n",
       "106  a.login_session_avg_times_30      31\n",
       "19     a.gesturesetup_min_times_7      31\n",
       "96        a.register_max_times_30      31\n",
       "40       a.login_timeslot_15_20_7      30"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.sort_values(by='f_imp:', axis=0, ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc, precision_recall_curve,accuracy_score, precision_score, recall_score,f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 扩展同时支持多个分数输出\n",
    "from multiscorer import MultiScorer  #https://github.com/StKyr/multiscorer/\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../DataSource/埋点/%s'\n",
    "X = np.load(path%'x_before.npy')\n",
    "Y = np.load(path%'y_before.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Accuracy: 0.71719 (+/- 0.039)\n",
      "Precision: 0.63250 (+/- 0.038)\n",
      "Recall: 0.91621 (+/- 0.024)\n",
      "F1: 0.74809 (+/- 0.024)\n",
      "time 1.1275959014892578 \n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Accuracy: 0.73834 (+/- 0.051)\n",
      "Precision: 0.68608 (+/- 0.111)\n",
      "Recall: 0.82435 (+/- 0.185)\n",
      "F1: 0.74205 (+/- 0.022)\n",
      "time 37.91132307052612 \n",
      "\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.76843 (+/- 0.095)\n",
      "Precision: 0.72244 (+/- 0.137)\n",
      "Recall: 0.83046 (+/- 0.152)\n",
      "F1: 0.76743 (+/- 0.074)\n",
      "time 2941.5278124809265 \n",
      "\n",
      "\n",
      "RF\n",
      "Accuracy: 0.78179 (+/- 0.095)\n",
      "Precision: 0.74535 (+/- 0.143)\n",
      "Recall: 0.82076 (+/- 0.136)\n",
      "F1: 0.77625 (+/- 0.071)\n",
      "time 17.219913482666016 \n",
      "\n",
      "\n",
      "GDBT\n",
      "Accuracy: 0.79368 (+/- 0.097)\n",
      "Precision: 0.75825 (+/- 0.138)\n",
      "Recall: 0.82735 (+/- 0.125)\n",
      "F1: 0.78723 (+/- 0.078)\n",
      "time 313.9631726741791 \n",
      "\n",
      "\n",
      "ADBT\n",
      "Accuracy: 0.80040 (+/- 0.094)\n",
      "Precision: 0.76535 (+/- 0.130)\n",
      "Recall: 0.83084 (+/- 0.115)\n",
      "F1: 0.79333 (+/- 0.076)\n",
      "time 122.36558771133423 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 问题，准确率与单独运行cross_val_score时貌似有出入\n",
    "\n",
    "models = [GaussianNB(), DecisionTreeClassifier(), SVC(), RandomForestClassifier(),GradientBoostingClassifier(), AdaBoostClassifier()]\n",
    "names = [\"Naive Bayes\", \"Decision Tree\", \"SVM\", \"RF\", \"GDBT\", \"ADBT\"]\n",
    "\n",
    "scorer = MultiScorer({\n",
    "    'Accuracy' : (accuracy_score, {}),\n",
    "    'Precision' : (precision_score, {}),\n",
    "    'Recall' : (recall_score, {}),\n",
    "    'F1' : (f1_score, {})  # 因为目标是四个类，不能用默认值\n",
    "})\n",
    "\n",
    "for model, name in zip(models, names):\n",
    "    print (name)\n",
    "    start = time.time()\n",
    "\n",
    "    cross_val_score(model, X, Y, scoring=scorer, cv=10)\n",
    "    results = scorer.get_results()\n",
    "\n",
    "    for metric_name in results.keys():\n",
    "        average_score = np.average(results[metric_name])\n",
    "        scores = np.array(results[metric_name])\n",
    "        print(\"%s: %0.5f (+/- %0.3f)\" % (metric_name, scores.mean(), scores.std() * 2))\n",
    "#         print('%s : %f' % (metric_name, average_score))\n",
    "\n",
    "    print ('time', time.time() - start, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "s = 'a//b/c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('/+', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hyperopt import hp, tpe, f\n",
    "from hyperopt.fmin import fmin\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 多个树相关模型\n",
    "\n",
    "def gini(truth, predictions):\n",
    "    g = np.asarray(np.c_[truth, predictions, np.arange(len(truth))], dtype=np.float)\n",
    "    g = g[np.lexsort((g[:2], -1*g[:1]))]\n",
    "    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n",
    "    gs -= (len(truth) + 1) / 2.\n",
    "    return gs / len(truth)\n",
    "\n",
    "def gini_xgb(predictions, truth):\n",
    "    truth = truth.get_label()\n",
    "    return 'gini', -1.0 * gini(truth, predictions) / gini(truth, truth)\n",
    "\n",
    "def gini_lgb(truth, predictions):\n",
    "    score = gini(truth, predictions) / gini(truth, truth)\n",
    "    return 'gini', score, True\n",
    "\n",
    "def gini_sklearn(truth, predictions):\n",
    "    return gini(truth, predictions) / gini(truth, truth)\n",
    "\n",
    "gini_scorer = make_scorer(gini_sklearn, greater_is_better=True, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_score_(truth, predictions):\n",
    "    return -1* f1_score(y_true=truth, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_scorer = make_scorer(f1_score_, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_auc_score(clf,X,Y):\n",
    "    cv = StratifiedKFold(n_splits=6)\n",
    "    cv_probs = cross_val_predict(clf, X, Y, method='predict_proba', cv=cv)[:,1]\n",
    "    auc = []\n",
    "    for train_idx, test_idx in cv.split(X, Y):\n",
    "        auc.append(roc_auc_score(Y[test_idx], cv_probs[test_idx]))\n",
    "    return np.mean(auc) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC -0.909 params {'n_estimators': 700, 'max_depth': 14}\n",
      "AUC -0.909 params {'n_estimators': 300, 'max_depth': 12}\n",
      "AUC -0.909 params {'n_estimators': 425, 'max_depth': 13}\n",
      "AUC -0.909 params {'n_estimators': 200, 'max_depth': 12}\n",
      "AUC -0.909 params {'n_estimators': 175, 'max_depth': 14}\n",
      "AUC -0.904 params {'n_estimators': 125, 'max_depth': 8}\n",
      "AUC -0.909 params {'n_estimators': 525, 'max_depth': 14}\n",
      "AUC -0.887 params {'n_estimators': 525, 'max_depth': 3}\n",
      "AUC -0.907 params {'n_estimators': 650, 'max_depth': 10}\n",
      "AUC -0.900 params {'n_estimators': 500, 'max_depth': 6}\n",
      "AUC -0.907 params {'n_estimators': 100, 'max_depth': 10}\n",
      "AUC -0.904 params {'n_estimators': 75, 'max_depth': 8}\n",
      "AUC -0.897 params {'n_estimators': 675, 'max_depth': 5}\n",
      "AUC -0.892 params {'n_estimators': 700, 'max_depth': 4}\n",
      "AUC -0.904 params {'n_estimators': 150, 'max_depth': 8}\n",
      "AUC -0.897 params {'n_estimators': 775, 'max_depth': 5}\n",
      "AUC -0.909 params {'n_estimators': 775, 'max_depth': 12}\n",
      "AUC -0.878 params {'n_estimators': 175, 'max_depth': 2}\n",
      "AUC -0.909 params {'n_estimators': 200, 'max_depth': 13}\n",
      "AUC -0.879 params {'n_estimators': 475, 'max_depth': 2}\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    params = {'n_estimators': int(params['n_estimators']), \n",
    "             'max_depth': int(params['max_depth'])}\n",
    "    clf = RandomForestClassifier(n_jobs=4, class_weight='balanced', **params)\n",
    "#     score = cross_val_score(clf, X, Y, scoring=f1_scorer, cv=StratifiedKFold(n_splits=6)).mean()\n",
    "    score = get_auc_score(clf, X, Y)\n",
    "        \n",
    "    print ('AUC {:.3f} params {}'.format(score, params))\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 25, 800, 25),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 15, 1)\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "           space=space,\n",
    "           algo=tpe.suggest,\n",
    "           max_evals=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 14.0, 'n_estimators': 700.0}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC -0.828 params {'n_estimators': 150, 'max_depth': 85}\n",
      "AUC -0.828 params {'n_estimators': 450, 'max_depth': 57}\n",
      "AUC -0.829 params {'n_estimators': 275, 'max_depth': 61}\n",
      "AUC -0.829 params {'n_estimators': 1625, 'max_depth': 81}\n",
      "AUC -0.830 params {'n_estimators': 1175, 'max_depth': 17}\n",
      "AUC -0.829 params {'n_estimators': 1425, 'max_depth': 79}\n",
      "AUC -0.829 params {'n_estimators': 575, 'max_depth': 82}\n",
      "AUC -0.829 params {'n_estimators': 1200, 'max_depth': 65}\n",
      "AUC -0.818 params {'n_estimators': 2800, 'max_depth': 6}\n",
      "AUC -0.829 params {'n_estimators': 2000, 'max_depth': 73}\n",
      "AUC -0.829 params {'n_estimators': 550, 'max_depth': 48}\n",
      "AUC -0.829 params {'n_estimators': 2275, 'max_depth': 30}\n",
      "AUC -0.829 params {'n_estimators': 2900, 'max_depth': 14}\n",
      "AUC -0.823 params {'n_estimators': 1425, 'max_depth': 8}\n",
      "AUC -0.829 params {'n_estimators': 850, 'max_depth': 22}\n",
      "AUC -0.827 params {'n_estimators': 75, 'max_depth': 39}\n",
      "AUC -0.829 params {'n_estimators': 3075, 'max_depth': 26}\n",
      "AUC -0.829 params {'n_estimators': 3025, 'max_depth': 59}\n",
      "AUC -0.829 params {'n_estimators': 900, 'max_depth': 73}\n",
      "AUC -0.829 params {'n_estimators': 1525, 'max_depth': 80}\n",
      "AUC -0.829 params {'n_estimators': 2400, 'max_depth': 34}\n",
      "AUC -0.828 params {'n_estimators': 2425, 'max_depth': 39}\n",
      "AUC -0.829 params {'n_estimators': 1975, 'max_depth': 18}\n",
      "AUC -0.829 params {'n_estimators': 2550, 'max_depth': 34}\n",
      "AUC -0.829 params {'n_estimators': 1875, 'max_depth': 47}\n",
      "AUC -0.829 params {'n_estimators': 1150, 'max_depth': 15}\n",
      "AUC -0.829 params {'n_estimators': 2150, 'max_depth': 41}\n",
      "AUC -0.818 params {'n_estimators': 1725, 'max_depth': 6}\n",
      "AUC -0.829 params {'n_estimators': 2625, 'max_depth': 29}\n",
      "AUC -0.829 params {'n_estimators': 1150, 'max_depth': 97}\n",
      "AUC -0.810 params {'n_estimators': 750, 'max_depth': 2}\n",
      "AUC -0.828 params {'n_estimators': 275, 'max_depth': 22}\n",
      "AUC -0.829 params {'n_estimators': 2275, 'max_depth': 55}\n",
      "AUC -0.829 params {'n_estimators': 1800, 'max_depth': 35}\n",
      "AUC -0.828 params {'n_estimators': 3200, 'max_depth': 11}\n",
      "AUC -0.829 params {'n_estimators': 1300, 'max_depth': 53}\n",
      "AUC -0.829 params {'n_estimators': 1625, 'max_depth': 94}\n",
      "AUC -0.829 params {'n_estimators': 1025, 'max_depth': 44}\n",
      "AUC -0.829 params {'n_estimators': 2600, 'max_depth': 23}\n",
      "AUC -0.829 params {'n_estimators': 2100, 'max_depth': 68}\n",
      "AUC -0.811 params {'n_estimators': 1400, 'max_depth': 3}\n",
      "AUC -0.829 params {'n_estimators': 600, 'max_depth': 33}\n",
      "AUC -0.829 params {'n_estimators': 2375, 'max_depth': 63}\n",
      "AUC -0.829 params {'n_estimators': 400, 'max_depth': 17}\n",
      "AUC -0.829 params {'n_estimators': 2825, 'max_depth': 28}\n",
      "AUC -0.828 params {'n_estimators': 2825, 'max_depth': 11}\n",
      "AUC -0.829 params {'n_estimators': 725, 'max_depth': 50}\n",
      "AUC -0.829 params {'n_estimators': 1000, 'max_depth': 27}\n",
      "AUC -0.829 params {'n_estimators': 1650, 'max_depth': 86}\n",
      "AUC -0.825 params {'n_estimators': 25, 'max_depth': 20}\n",
      "AUC -0.828 params {'n_estimators': 1250, 'max_depth': 43}\n",
      "AUC -0.828 params {'n_estimators': 1475, 'max_depth': 12}\n",
      "AUC -0.829 params {'n_estimators': 3025, 'max_depth': 24}\n",
      "AUC -0.829 params {'n_estimators': 2725, 'max_depth': 29}\n",
      "AUC -0.828 params {'n_estimators': 250, 'max_depth': 37}\n",
      "AUC -0.818 params {'n_estimators': 1925, 'max_depth': 6}\n",
      "AUC -0.829 params {'n_estimators': 3175, 'max_depth': 59}\n",
      "AUC -0.829 params {'n_estimators': 425, 'max_depth': 47}\n",
      "AUC -0.829 params {'n_estimators': 1350, 'max_depth': 32}\n",
      "AUC -0.830 params {'n_estimators': 1050, 'max_depth': 18}\n",
      "AUC -0.809 params {'n_estimators': 925, 'max_depth': 2}\n",
      "AUC -0.829 params {'n_estimators': 1075, 'max_depth': 16}\n",
      "AUC -0.829 params {'n_estimators': 750, 'max_depth': 74}\n",
      "AUC -0.823 params {'n_estimators': 550, 'max_depth': 8}\n",
      "AUC -0.829 params {'n_estimators': 200, 'max_depth': 20}\n",
      "AUC -0.829 params {'n_estimators': 1575, 'max_depth': 26}\n",
      "AUC -0.829 params {'n_estimators': 1550, 'max_depth': 14}\n",
      "AUC -0.829 params {'n_estimators': 1725, 'max_depth': 38}\n",
      "AUC -0.829 params {'n_estimators': 850, 'max_depth': 25}\n",
      "AUC -0.824 params {'n_estimators': 1225, 'max_depth': 8}\n",
      "AUC -0.829 params {'n_estimators': 2075, 'max_depth': 20}\n",
      "AUC -0.813 params {'n_estimators': 1525, 'max_depth': 4}\n",
      "AUC -0.828 params {'n_estimators': 1825, 'max_depth': 42}\n",
      "AUC -0.829 params {'n_estimators': 1375, 'max_depth': 13}\n",
      "AUC -0.829 params {'n_estimators': 1100, 'max_depth': 31}\n",
      "AUC -0.829 params {'n_estimators': 625, 'max_depth': 36}\n",
      "AUC -0.827 params {'n_estimators': 900, 'max_depth': 10}\n",
      "AUC -0.829 params {'n_estimators': 2225, 'max_depth': 19}\n",
      "AUC -0.829 params {'n_estimators': 1700, 'max_depth': 52}\n",
      "AUC -0.828 params {'n_estimators': 1300, 'max_depth': 46}\n",
      "AUC -0.829 params {'n_estimators': 675, 'max_depth': 26}\n",
      "AUC -0.829 params {'n_estimators': 350, 'max_depth': 22}\n",
      "AUC -0.829 params {'n_estimators': 500, 'max_depth': 40}\n",
      "AUC -0.829 params {'n_estimators': 1000, 'max_depth': 16}\n",
      "AUC -0.829 params {'n_estimators': 2025, 'max_depth': 56}\n",
      "AUC -0.815 params {'n_estimators': 100, 'max_depth': 5}\n",
      "AUC -0.808 params {'n_estimators': 1800, 'max_depth': 1}\n",
      "AUC -0.825 params {'n_estimators': 1150, 'max_depth': 9}\n",
      "AUC -0.829 params {'n_estimators': 1575, 'max_depth': 34}\n",
      "AUC -0.829 params {'n_estimators': 825, 'max_depth': 30}\n",
      "AUC -0.829 params {'n_estimators': 1425, 'max_depth': 87}\n",
      "AUC -0.829 params {'n_estimators': 2500, 'max_depth': 45}\n",
      "AUC -0.829 params {'n_estimators': 1900, 'max_depth': 18}\n",
      "AUC -0.829 params {'n_estimators': 1175, 'max_depth': 14}\n",
      "AUC -0.829 params {'n_estimators': 1475, 'max_depth': 49}\n",
      "AUC -0.829 params {'n_estimators': 2175, 'max_depth': 66}\n",
      "AUC -0.829 params {'n_estimators': 2325, 'max_depth': 23}\n",
      "AUC -0.829 params {'n_estimators': 1300, 'max_depth': 27}\n",
      "AUC -0.818 params {'n_estimators': 950, 'max_depth': 6}\n",
      "AUC -0.829 params {'n_estimators': 1675, 'max_depth': 21}\n",
      "AUC -0.829 params {'n_estimators': 1050, 'max_depth': 25}\n",
      "AUC -0.829 params {'n_estimators': 775, 'max_depth': 32}\n",
      "AUC -0.829 params {'n_estimators': 1475, 'max_depth': 38}\n",
      "AUC -0.829 params {'n_estimators': 1600, 'max_depth': 12}\n",
      "AUC -0.829 params {'n_estimators': 1975, 'max_depth': 70}\n",
      "AUC -0.829 params {'n_estimators': 350, 'max_depth': 59}\n",
      "AUC -0.829 params {'n_estimators': 1225, 'max_depth': 41}\n",
      "AUC -0.829 params {'n_estimators': 500, 'max_depth': 53}\n",
      "AUC -0.828 params {'n_estimators': 650, 'max_depth': 30}\n",
      "AUC -0.829 params {'n_estimators': 825, 'max_depth': 18}\n",
      "AUC -0.829 params {'n_estimators': 1850, 'max_depth': 35}\n",
      "AUC -0.811 params {'n_estimators': 1750, 'max_depth': 3}\n",
      "AUC -0.829 params {'n_estimators': 1325, 'max_depth': 28}\n",
      "AUC -0.829 params {'n_estimators': 975, 'max_depth': 15}\n",
      "AUC -0.829 params {'n_estimators': 1100, 'max_depth': 76}\n",
      "AUC -0.827 params {'n_estimators': 2050, 'max_depth': 10}\n",
      "AUC -0.830 params {'n_estimators': 700, 'max_depth': 63}\n",
      "AUC -0.828 params {'n_estimators': 150, 'max_depth': 89}\n",
      "AUC -0.829 params {'n_estimators': 725, 'max_depth': 82}\n",
      "AUC -0.828 params {'n_estimators': 250, 'max_depth': 61}\n",
      "AUC -0.829 params {'n_estimators': 575, 'max_depth': 93}\n",
      "AUC -0.829 params {'n_estimators': 350, 'max_depth': 78}\n",
      "AUC -0.828 params {'n_estimators': 475, 'max_depth': 69}\n",
      "AUC -0.826 params {'n_estimators': 50, 'max_depth': 82}\n",
      "AUC -0.829 params {'n_estimators': 875, 'max_depth': 67}\n",
      "AUC -0.829 params {'n_estimators': 675, 'max_depth': 63}\n",
      "AUC -0.828 params {'n_estimators': 950, 'max_depth': 74}\n",
      "AUC -0.828 params {'n_estimators': 425, 'max_depth': 99}\n",
      "AUC -0.829 params {'n_estimators': 1250, 'max_depth': 72}\n",
      "AUC -0.829 params {'n_estimators': 1100, 'max_depth': 57}\n",
      "AUC -0.829 params {'n_estimators': 175, 'max_depth': 63}\n",
      "AUC -0.828 params {'n_estimators': 550, 'max_depth': 53}\n",
      "AUC -0.829 params {'n_estimators': 1375, 'max_depth': 51}\n",
      "AUC -0.830 params {'n_estimators': 800, 'max_depth': 71}\n",
      "AUC -0.829 params {'n_estimators': 1500, 'max_depth': 91}\n",
      "AUC -0.829 params {'n_estimators': 800, 'max_depth': 79}\n",
      "AUC -0.829 params {'n_estimators': 1025, 'max_depth': 48}\n",
      "AUC -0.828 params {'n_estimators': 875, 'max_depth': 84}\n",
      "AUC -0.829 params {'n_estimators': 1175, 'max_depth': 76}\n",
      "AUC -0.829 params {'n_estimators': 1625, 'max_depth': 45}\n",
      "AUC -0.829 params {'n_estimators': 1450, 'max_depth': 70}\n",
      "AUC -0.829 params {'n_estimators': 325, 'max_depth': 58}\n",
      "AUC -0.829 params {'n_estimators': 600, 'max_depth': 43}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC -0.829 params {'n_estimators': 1275, 'max_depth': 95}\n",
      "AUC -0.829 params {'n_estimators': 1125, 'max_depth': 100}\n",
      "AUC -0.829 params {'n_estimators': 700, 'max_depth': 62}\n",
      "AUC -0.829 params {'n_estimators': 750, 'max_depth': 85}\n",
      "AUC -0.829 params {'n_estimators': 1050, 'max_depth': 64}\n",
      "AUC -0.829 params {'n_estimators': 925, 'max_depth': 55}\n",
      "AUC -0.828 params {'n_estimators': 300, 'max_depth': 72}\n",
      "AUC -0.829 params {'n_estimators': 525, 'max_depth': 65}\n",
      "AUC -0.826 params {'n_estimators': 100, 'max_depth': 68}\n",
      "AUC -0.829 params {'n_estimators': 625, 'max_depth': 77}\n",
      "AUC -0.828 params {'n_estimators': 225, 'max_depth': 66}\n",
      "AUC -0.829 params {'n_estimators': 400, 'max_depth': 80}\n",
      "AUC -0.829 params {'n_estimators': 1200, 'max_depth': 61}\n",
      "AUC -0.829 params {'n_estimators': 775, 'max_depth': 60}\n",
      "AUC -0.829 params {'n_estimators': 1375, 'max_depth': 50}\n",
      "AUC -0.829 params {'n_estimators': 975, 'max_depth': 54}\n",
      "AUC -0.829 params {'n_estimators': 850, 'max_depth': 87}\n",
      "AUC -0.830 params {'n_estimators': 475, 'max_depth': 75}\n",
      "AUC -0.829 params {'n_estimators': 1000, 'max_depth': 91}\n",
      "AUC -0.828 params {'n_estimators': 425, 'max_depth': 75}\n",
      "AUC -0.828 params {'n_estimators': 500, 'max_depth': 98}\n",
      "AUC -0.825 params {'n_estimators': 50, 'max_depth': 84}\n",
      "AUC -0.828 params {'n_estimators': 900, 'max_depth': 96}\n",
      "AUC -0.829 params {'n_estimators': 1325, 'max_depth': 89}\n",
      "AUC -0.828 params {'n_estimators': 150, 'max_depth': 71}\n",
      "AUC -0.829 params {'n_estimators': 625, 'max_depth': 23}\n",
      "AUC -0.829 params {'n_estimators': 1550, 'max_depth': 81}\n",
      "AUC -0.824 params {'n_estimators': 1750, 'max_depth': 8}\n",
      "AUC -0.828 params {'n_estimators': 1150, 'max_depth': 36}\n",
      "AUC -0.829 params {'n_estimators': 1425, 'max_depth': 39}\n",
      "AUC -0.829 params {'n_estimators': 1950, 'max_depth': 83}\n",
      "AUC -0.829 params {'n_estimators': 1250, 'max_depth': 79}\n",
      "AUC -0.829 params {'n_estimators': 375, 'max_depth': 17}\n",
      "AUC -0.829 params {'n_estimators': 450, 'max_depth': 33}\n",
      "AUC -0.829 params {'n_estimators': 1075, 'max_depth': 73}\n",
      "AUC -0.829 params {'n_estimators': 250, 'max_depth': 13}\n",
      "AUC -0.809 params {'n_estimators': 800, 'max_depth': 2}\n",
      "AUC -0.829 params {'n_estimators': 675, 'max_depth': 56}\n",
      "AUC -0.829 params {'n_estimators': 575, 'max_depth': 92}\n",
      "AUC -0.818 params {'n_estimators': 2950, 'max_depth': 6}\n",
      "AUC -0.829 params {'n_estimators': 1650, 'max_depth': 47}\n",
      "AUC -0.829 params {'n_estimators': 900, 'max_depth': 88}\n",
      "AUC -0.829 params {'n_estimators': 1200, 'max_depth': 68}\n",
      "AUC -0.829 params {'n_estimators': 1025, 'max_depth': 20}\n",
      "AUC -0.828 params {'n_estimators': 725, 'max_depth': 11}\n",
      "AUC -0.829 params {'n_estimators': 825, 'max_depth': 74}\n",
      "AUC -0.829 params {'n_estimators': 275, 'max_depth': 30}\n",
      "AUC -0.829 params {'n_estimators': 950, 'max_depth': 42}\n",
      "AUC -0.828 params {'n_estimators': 525, 'max_depth': 77}\n",
      "AUC -0.828 params {'n_estimators': 1375, 'max_depth': 86}\n",
      "AUC -0.829 params {'n_estimators': 1125, 'max_depth': 59}\n",
      "AUC -0.829 params {'n_estimators': 1300, 'max_depth': 26}\n",
      "AUC -0.829 params {'n_estimators': 1550, 'max_depth': 15}\n",
      "AUC -0.813 params {'n_estimators': 100, 'max_depth': 4}\n",
      "AUC -0.828 params {'n_estimators': 175, 'max_depth': 94}\n",
      "AUC -0.829 params {'n_estimators': 2150, 'max_depth': 66}\n",
      "AUC -0.830 params {'n_estimators': 1850, 'max_depth': 28}\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    params = {'n_estimators': int(params['n_estimators']), \n",
    "             'max_depth': int(params['max_depth'])}\n",
    "    clf = RandomForestClassifier(n_jobs=4, class_weight='balanced', **params)\n",
    "    score = cross_val_score(clf, X, Y, scoring=f1_scorer, cv=StratifiedKFold(n_splits=6)).mean()\n",
    "#     score = get_auc_score(clf, X, Y)\n",
    "        \n",
    "    print ('AUC {:.3f} params {}'.format(score, params))\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 25, 3200, 25),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 100, 1)\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "           space=space,\n",
    "           algo=tpe.suggest,\n",
    "           max_evals=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC -0.91626 params {'gamma': '0.207', 'max_depth': 8, 'colsample_bytree': '0.532'}\n",
      "AUC -0.89894 params {'gamma': '0.002', 'max_depth': 1, 'colsample_bytree': '0.560'}\n",
      "AUC -0.91219 params {'gamma': '0.320', 'max_depth': 3, 'colsample_bytree': '0.341'}\n",
      "AUC -0.91608 params {'gamma': '0.395', 'max_depth': 8, 'colsample_bytree': '0.584'}\n",
      "AUC -0.91490 params {'gamma': '0.440', 'max_depth': 14, 'colsample_bytree': '0.467'}\n",
      "AUC -0.91408 params {'gamma': '0.038', 'max_depth': 14, 'colsample_bytree': '0.752'}\n",
      "AUC -0.91443 params {'gamma': '0.377', 'max_depth': 15, 'colsample_bytree': '0.640'}\n",
      "AUC -0.89894 params {'gamma': '0.402', 'max_depth': 1, 'colsample_bytree': '0.474'}\n",
      "AUC -0.91617 params {'gamma': '0.252', 'max_depth': 6, 'colsample_bytree': '0.868'}\n",
      "AUC -0.91556 params {'gamma': '0.308', 'max_depth': 9, 'colsample_bytree': '0.567'}\n",
      "AUC -0.91406 params {'gamma': '0.454', 'max_depth': 13, 'colsample_bytree': '0.836'}\n",
      "AUC -0.91432 params {'gamma': '0.007', 'max_depth': 4, 'colsample_bytree': '0.974'}\n",
      "AUC -0.91621 params {'gamma': '0.064', 'max_depth': 6, 'colsample_bytree': '0.675'}\n",
      "AUC -0.91419 params {'gamma': '0.468', 'max_depth': 4, 'colsample_bytree': '0.957'}\n",
      "AUC -0.91228 params {'gamma': '0.134', 'max_depth': 3, 'colsample_bytree': '0.860'}\n",
      "AUC -0.91443 params {'gamma': '0.021', 'max_depth': 4, 'colsample_bytree': '0.858'}\n",
      "AUC -0.90787 params {'gamma': '0.422', 'max_depth': 2, 'colsample_bytree': '0.452'}\n",
      "AUC -0.90783 params {'gamma': '0.101', 'max_depth': 2, 'colsample_bytree': '0.618'}\n",
      "AUC -0.91450 params {'gamma': '0.259', 'max_depth': 14, 'colsample_bytree': '0.693'}\n",
      "AUC -0.91540 params {'gamma': '0.484', 'max_depth': 9, 'colsample_bytree': '0.585'}\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    params = {'gamma': \"{:.3f}\".format(params['gamma']), \n",
    "             'max_depth': int(params['max_depth']),\n",
    "             'colsample_bytree': \"{:.3f}\".format(params['colsample_bytree'])}\n",
    "    clf = xgb.XGBClassifier(n_jobs=4, \n",
    "                            n_estimators=250,\n",
    "                            learning_rate=0.05,\n",
    "                            **params)\n",
    "#     score = cross_val_score(clf, X, Y, scoring=f1_scorer, cv=StratifiedKFold(n_splits=6)).mean()\n",
    "    score = get_auc_score(clf, X, Y)\n",
    "        \n",
    "    print ('AUC {:.5f} params {}'.format(score, params))\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    'gamma': hp.uniform('gamma', 0.0, 0.5),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 15, 1)\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "           space=space,\n",
    "           algo=tpe.suggest,\n",
    "           max_evals=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.1, 0.2, 0.3, 0.4]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i/10 for i in range(0,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC -0.91516 params {'num_leaves': 42, 'colsample_bytree': '0.749'}\n",
      "AUC -0.91137 params {'num_leaves': 14, 'colsample_bytree': '0.510'}\n",
      "AUC -0.91148 params {'num_leaves': 48, 'colsample_bytree': '0.419'}\n",
      "AUC -0.91411 params {'num_leaves': 28, 'colsample_bytree': '0.748'}\n",
      "AUC -0.91638 params {'num_leaves': 102, 'colsample_bytree': '0.809'}\n",
      "AUC -0.91461 params {'num_leaves': 34, 'colsample_bytree': '0.831'}\n",
      "AUC -0.91619 params {'num_leaves': 80, 'colsample_bytree': '0.611'}\n",
      "AUC -0.91322 params {'num_leaves': 22, 'colsample_bytree': '0.685'}\n",
      "AUC -0.91032 params {'num_leaves': 16, 'colsample_bytree': '0.313'}\n",
      "AUC -0.91595 params {'num_leaves': 102, 'colsample_bytree': '0.324'}\n",
      "AUC -0.90980 params {'num_leaves': 18, 'colsample_bytree': '0.449'}\n",
      "AUC -0.91630 params {'num_leaves': 92, 'colsample_bytree': '0.722'}\n",
      "AUC -0.91615 params {'num_leaves': 116, 'colsample_bytree': '0.980'}\n",
      "AUC -0.91645 params {'num_leaves': 96, 'colsample_bytree': '0.538'}\n",
      "AUC -0.91553 params {'num_leaves': 50, 'colsample_bytree': '0.642'}\n",
      "AUC -0.91282 params {'num_leaves': 116, 'colsample_bytree': '0.417'}\n",
      "AUC -0.91620 params {'num_leaves': 108, 'colsample_bytree': '0.950'}\n",
      "AUC -0.91640 params {'num_leaves': 120, 'colsample_bytree': '0.766'}\n",
      "AUC -0.91238 params {'num_leaves': 38, 'colsample_bytree': '0.302'}\n",
      "AUC -0.91289 params {'num_leaves': 110, 'colsample_bytree': '0.405'}\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    params = { \n",
    "             'num_leaves': int(params['num_leaves']),\n",
    "             'colsample_bytree': \"{:.3f}\".format(params['colsample_bytree'])}\n",
    "    clf = lgb.LGBMClassifier(\n",
    "                            n_estimators=500,\n",
    "                            learning_rate=0.01,\n",
    "                            **params)\n",
    "#     score = cross_val_score(clf, X, Y, scoring=f1_scorer, cv=StratifiedKFold(n_splits=6)).mean()\n",
    "    score = get_auc_score(clf, X, Y)\n",
    "        \n",
    "    print ('AUC {:.5f} params {}'.format(score, params))\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    \n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n",
    "    'num_leaves': hp.quniform('num_leaves', 8, 128, 2)\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "           space=space,\n",
    "           algo=tpe.suggest,\n",
    "           max_evals=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sklearn_tunning' from '/tmp/working/GitHub/PyData-Cookbook/learning/sklearn_tunning.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  sklearn_tunning as skt\n",
    "import importlib\n",
    "importlib.reload(skt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sktune = skt.SK_Tunning(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploring parameters for RF\n",
      "AUC -0.90447 params {'n_estimators': 275, 'max_depth': 8}\n",
      "AUC -0.89655 params {'n_estimators': 75, 'max_depth': 5}\n",
      "AUC -0.90000 params {'n_estimators': 675, 'max_depth': 6}\n",
      "AUC -0.88621 params {'n_estimators': 50, 'max_depth': 3}\n",
      "AUC -0.90793 params {'n_estimators': 100, 'max_depth': 15}\n",
      "AUC -0.90450 params {'n_estimators': 200, 'max_depth': 8}\n",
      "AUC -0.89247 params {'n_estimators': 600, 'max_depth': 4}\n",
      "AUC -0.90865 params {'n_estimators': 200, 'max_depth': 15}\n",
      "AUC -0.90469 params {'n_estimators': 550, 'max_depth': 8}\n",
      "AUC -0.87816 params {'n_estimators': 350, 'max_depth': 2}\n",
      "exploring parameters for XGB\n",
      "AUC -0.91641 params {'gamma': '0.300', 'max_depth': 8, 'colsample_bytree': '0.675'}\n",
      "AUC -0.91213 params {'gamma': '0.026', 'max_depth': 3, 'colsample_bytree': '0.885'}\n",
      "AUC -0.91584 params {'gamma': '0.465', 'max_depth': 8, 'colsample_bytree': '0.716'}\n",
      "AUC -0.91447 params {'gamma': '0.277', 'max_depth': 14, 'colsample_bytree': '0.400'}\n",
      "AUC -0.91411 params {'gamma': '0.231', 'max_depth': 12, 'colsample_bytree': '0.933'}\n",
      "AUC -0.91399 params {'gamma': '0.193', 'max_depth': 12, 'colsample_bytree': '0.968'}\n",
      "AUC -0.91457 params {'gamma': '0.186', 'max_depth': 4, 'colsample_bytree': '0.632'}\n",
      "AUC -0.91463 params {'gamma': '0.064', 'max_depth': 15, 'colsample_bytree': '0.497'}\n",
      "AUC -0.91466 params {'gamma': '0.427', 'max_depth': 11, 'colsample_bytree': '0.595'}\n",
      "AUC -0.91464 params {'gamma': '0.418', 'max_depth': 4, 'colsample_bytree': '0.628'}\n",
      "exploring parameters for LGBM\n",
      "AUC -0.91612 params {'num_leaves': 68, 'colsample_bytree': '0.715'}\n",
      "AUC -0.91264 params {'num_leaves': 102, 'colsample_bytree': '0.387'}\n",
      "AUC -0.91127 params {'num_leaves': 14, 'colsample_bytree': '0.801'}\n",
      "AUC -0.91628 params {'num_leaves': 82, 'colsample_bytree': '0.695'}\n",
      "AUC -0.91508 params {'num_leaves': 44, 'colsample_bytree': '0.910'}\n",
      "AUC -0.91597 params {'num_leaves': 72, 'colsample_bytree': '0.985'}\n",
      "AUC -0.91195 params {'num_leaves': 16, 'colsample_bytree': '0.697'}\n",
      "AUC -0.90950 params {'num_leaves': 10, 'colsample_bytree': '0.888'}\n",
      "AUC -0.91633 params {'num_leaves': 126, 'colsample_bytree': '0.910'}\n",
      "AUC -0.91633 params {'num_leaves': 94, 'colsample_bytree': '0.722'}\n",
      "Naive Bayes\n",
      "Accuracy: 0.71719 (+/- 0.039)\n",
      "Precision: 0.63250 (+/- 0.038)\n",
      "Recall: 0.91621 (+/- 0.024)\n",
      "F1: 0.74809 (+/- 0.024)\n",
      "time 2.025099039077759 \n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Accuracy: 0.73814 (+/- 0.051)\n",
      "Precision: 0.68515 (+/- 0.109)\n",
      "Recall: 0.82558 (+/- 0.182)\n",
      "F1: 0.74221 (+/- 0.022)\n",
      "time 66.50459837913513 \n",
      "\n",
      "\n",
      "RF\n",
      "Accuracy: 0.77190 (+/- 0.104)\n",
      "Precision: 0.72581 (+/- 0.146)\n",
      "Recall: 0.83477 (+/- 0.152)\n",
      "F1: 0.77129 (+/- 0.084)\n",
      "time 298.36580657958984 \n",
      "\n",
      "\n",
      "GDBT\n",
      "Accuracy: 0.78925 (+/- 0.109)\n",
      "Precision: 0.74682 (+/- 0.146)\n",
      "Recall: 0.83952 (+/- 0.133)\n",
      "F1: 0.78627 (+/- 0.090)\n",
      "time 618.059864282608 \n",
      "\n",
      "\n",
      "ADBT\n",
      "Accuracy: 0.79819 (+/- 0.104)\n",
      "Precision: 0.75763 (+/- 0.137)\n",
      "Recall: 0.84127 (+/- 0.119)\n",
      "F1: 0.79378 (+/- 0.086)\n",
      "time 211.6959900856018 \n",
      "\n",
      "\n",
      "XGB\n",
      "Accuracy: 0.80639 (+/- 0.102)\n",
      "Precision: 0.76729 (+/- 0.133)\n",
      "Recall: 0.84460 (+/- 0.110)\n",
      "F1: 0.80112 (+/- 0.085)\n",
      "time 687.1675353050232 \n",
      "\n",
      "\n",
      "LGBM\n",
      "Accuracy: 0.81213 (+/- 0.098)\n",
      "Precision: 0.77408 (+/- 0.127)\n",
      "Recall: 0.84687 (+/- 0.103)\n",
      "F1: 0.80624 (+/- 0.083)\n",
      "time 426.8637866973877 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = sktune.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GaussianNB(priors=None),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       " RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "             criterion='gini', max_depth=15, max_features='auto',\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=200, n_jobs=4, oob_score=False, random_state=None,\n",
       "             verbose=0, warm_start=False),\n",
       " GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "               n_iter_no_change=None, presort='auto', random_state=None,\n",
       "               subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "               verbose=0, warm_start=False),\n",
       " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "        colsample_bytree='0.675', gamma='0.300', learning_rate=0.05,\n",
       "        max_delta_step=0, max_depth=8, min_child_weight=1, missing=None,\n",
       "        n_estimators=250, n_jobs=4, nthread=None,\n",
       "        objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "        reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "        subsample=1),\n",
       " LGBMClassifier(boosting_type='gbdt', colsample_bytree='0.722',\n",
       "         learning_rate=0.01, max_bin=255, max_depth=-1,\n",
       "         min_child_samples=10, min_child_weight=5, min_split_gain=0,\n",
       "         n_estimators=500, nthread=-1, num_leaves=94, objective='binary',\n",
       "         reg_alpha=0, reg_lambda=0, seed=0, silent=True, subsample=1,\n",
       "         subsample_for_bin=50000, subsample_freq=1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5383012455359746, 'num_leaves': 96.0}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stacking as st\n",
    "importlib.reload(st)\n",
    "s = st.Stacking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ..., \n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.isinf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:604: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个模型， 第1次迭代########################################################\n",
      "第1个模型， 第2次迭代########################################################\n",
      "第1个模型， 第3次迭代########################################################\n",
      "                   0\n",
      "0       1.000000e+00\n",
      "1       1.000000e+00\n",
      "2       2.175395e-33\n",
      "3       1.488659e-14\n",
      "4       2.002772e-33\n",
      "5      5.744382e-125\n",
      "6       1.000000e+00\n",
      "7       1.000000e+00\n",
      "8       1.000000e+00\n",
      "9       1.000000e+00\n",
      "10      3.502566e-01\n",
      "11      1.000000e+00\n",
      "12      1.000000e+00\n",
      "13      1.000000e+00\n",
      "14      1.000000e+00\n",
      "15      9.999998e-01\n",
      "16      9.991845e-01\n",
      "17      1.000000e+00\n",
      "18      4.636770e-07\n",
      "19      3.523048e-35\n",
      "20      1.000000e+00\n",
      "21      1.000000e+00\n",
      "22      1.000000e+00\n",
      "23      3.869862e-02\n",
      "24      9.992815e-01\n",
      "25      1.000000e+00\n",
      "26      8.367409e-11\n",
      "27      1.000000e+00\n",
      "28      1.000000e+00\n",
      "29      7.408817e-12\n",
      "...              ...\n",
      "12264   5.546157e-31\n",
      "12265   1.000000e+00\n",
      "12266   7.858847e-04\n",
      "12267   1.000000e+00\n",
      "12268   1.868350e-19\n",
      "12269   1.000000e+00\n",
      "12270   1.000000e+00\n",
      "12271   3.327013e-61\n",
      "12272   1.000000e+00\n",
      "12273   1.000000e+00\n",
      "12274   1.000000e+00\n",
      "12275   0.000000e+00\n",
      "12276   1.361227e-25\n",
      "12277   1.164519e-02\n",
      "12278   7.382676e-24\n",
      "12279   1.000000e+00\n",
      "12280   1.000000e+00\n",
      "12281   1.000000e+00\n",
      "12282   1.000000e+00\n",
      "12283   1.000000e+00\n",
      "12284   1.000000e+00\n",
      "12285  2.899414e-171\n",
      "12286   1.000000e+00\n",
      "12287   1.000000e+00\n",
      "12288   1.000000e+00\n",
      "12289   1.000000e+00\n",
      "12290   1.000000e+00\n",
      "12291   1.000000e+00\n",
      "12292   9.681171e-01\n",
      "12293   7.616697e-23\n",
      "\n",
      "[36881 rows x 1 columns]\n",
      "第2个模型， 第1次迭代########################################################\n",
      "第2个模型， 第2次迭代########################################################\n",
      "第2个模型， 第3次迭代########################################################\n",
      "         0\n",
      "0      0.0\n",
      "1      1.0\n",
      "2      0.0\n",
      "3      0.0\n",
      "4      0.0\n",
      "5      0.0\n",
      "6      1.0\n",
      "7      1.0\n",
      "8      0.0\n",
      "9      1.0\n",
      "10     0.0\n",
      "11     1.0\n",
      "12     0.0\n",
      "13     1.0\n",
      "14     0.0\n",
      "15     0.0\n",
      "16     1.0\n",
      "17     1.0\n",
      "18     0.0\n",
      "19     0.0\n",
      "20     1.0\n",
      "21     1.0\n",
      "22     1.0\n",
      "23     0.0\n",
      "24     0.0\n",
      "25     1.0\n",
      "26     0.0\n",
      "27     0.0\n",
      "28     1.0\n",
      "29     0.0\n",
      "...    ...\n",
      "12264  0.0\n",
      "12265  0.0\n",
      "12266  0.0\n",
      "12267  1.0\n",
      "12268  0.0\n",
      "12269  0.0\n",
      "12270  0.0\n",
      "12271  0.0\n",
      "12272  1.0\n",
      "12273  1.0\n",
      "12274  1.0\n",
      "12275  0.0\n",
      "12276  0.0\n",
      "12277  0.0\n",
      "12278  0.0\n",
      "12279  1.0\n",
      "12280  1.0\n",
      "12281  1.0\n",
      "12282  0.0\n",
      "12283  1.0\n",
      "12284  1.0\n",
      "12285  0.0\n",
      "12286  0.0\n",
      "12287  0.0\n",
      "12288  1.0\n",
      "12289  0.0\n",
      "12290  1.0\n",
      "12291  0.0\n",
      "12292  0.0\n",
      "12293  0.0\n",
      "\n",
      "[36881 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/working/GitHub/PyData-Cookbook/learning/stacking.py:107: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  cur_model.fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个模型， 第1次迭代########################################################\n",
      "第3个模型， 第2次迭代########################################################\n",
      "第3个模型， 第3次迭代########################################################\n",
      "              0\n",
      "0      0.711991\n",
      "1      0.862226\n",
      "2      0.029723\n",
      "3      0.145054\n",
      "4      0.014620\n",
      "5      0.008569\n",
      "6      0.866872\n",
      "7      0.457507\n",
      "8      0.288260\n",
      "9      0.891326\n",
      "10     0.206892\n",
      "11     0.897345\n",
      "12     0.661975\n",
      "13     0.587659\n",
      "14     0.036924\n",
      "15     0.127339\n",
      "16     0.299567\n",
      "17     0.868186\n",
      "18     0.022696\n",
      "19     0.016875\n",
      "20     0.873120\n",
      "21     0.567363\n",
      "22     0.891690\n",
      "23     0.120286\n",
      "24     0.307103\n",
      "25     0.782598\n",
      "26     0.181366\n",
      "27     0.857128\n",
      "28     0.959574\n",
      "29     0.073167\n",
      "...         ...\n",
      "12264  0.072270\n",
      "12265  0.764365\n",
      "12266  0.214501\n",
      "12267  0.909441\n",
      "12268  0.015034\n",
      "12269  0.213159\n",
      "12270  0.791722\n",
      "12271  0.132238\n",
      "12272  0.809137\n",
      "12273  0.205276\n",
      "12274  0.893624\n",
      "12275  0.098692\n",
      "12276  0.007143\n",
      "12277  0.137339\n",
      "12278  0.450808\n",
      "12279  0.872038\n",
      "12280  0.801540\n",
      "12281  0.843656\n",
      "12282  0.853077\n",
      "12283  0.825116\n",
      "12284  0.843135\n",
      "12285  0.041973\n",
      "12286  0.851171\n",
      "12287  0.159577\n",
      "12288  0.966773\n",
      "12289  0.095631\n",
      "12290  0.921613\n",
      "12291  0.908765\n",
      "12292  0.016539\n",
      "12293  0.019009\n",
      "\n",
      "[36881 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:604: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第4个模型， 第1次迭代########################################################\n",
      "第4个模型， 第2次迭代########################################################\n",
      "第4个模型， 第3次迭代########################################################\n",
      "              0\n",
      "0      0.627477\n",
      "1      0.845253\n",
      "2      0.036316\n",
      "3      0.136783\n",
      "4      0.019190\n",
      "5      0.024473\n",
      "6      0.861233\n",
      "7      0.607257\n",
      "8      0.401561\n",
      "9      0.866360\n",
      "10     0.292161\n",
      "11     0.905748\n",
      "12     0.777756\n",
      "13     0.583432\n",
      "14     0.066265\n",
      "15     0.075181\n",
      "16     0.263048\n",
      "17     0.805016\n",
      "18     0.040320\n",
      "19     0.023091\n",
      "20     0.866295\n",
      "21     0.536562\n",
      "22     0.959367\n",
      "23     0.059902\n",
      "24     0.245995\n",
      "25     0.850971\n",
      "26     0.057118\n",
      "27     0.741239\n",
      "28     0.942345\n",
      "29     0.090255\n",
      "...         ...\n",
      "12264  0.041116\n",
      "12265  0.760344\n",
      "12266  0.357605\n",
      "12267  0.826163\n",
      "12268  0.027862\n",
      "12269  0.120813\n",
      "12270  0.767542\n",
      "12271  0.201430\n",
      "12272  0.813070\n",
      "12273  0.233104\n",
      "12274  0.841494\n",
      "12275  0.024650\n",
      "12276  0.030931\n",
      "12277  0.218705\n",
      "12278  0.439877\n",
      "12279  0.914657\n",
      "12280  0.883603\n",
      "12281  0.807945\n",
      "12282  0.782348\n",
      "12283  0.793793\n",
      "12284  0.790957\n",
      "12285  0.050914\n",
      "12286  0.815239\n",
      "12287  0.105561\n",
      "12288  0.974495\n",
      "12289  0.087761\n",
      "12290  0.931575\n",
      "12291  0.851689\n",
      "12292  0.051658\n",
      "12293  0.032840\n",
      "\n",
      "[36881 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:604: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第5个模型， 第1次迭代########################################################\n",
      "第5个模型， 第2次迭代########################################################\n",
      "第5个模型， 第3次迭代########################################################\n",
      "              0\n",
      "0      0.500112\n",
      "1      0.509923\n",
      "2      0.482738\n",
      "3      0.486397\n",
      "4      0.472267\n",
      "5      0.475990\n",
      "6      0.507496\n",
      "7      0.500192\n",
      "8      0.498078\n",
      "9      0.509382\n",
      "10     0.498771\n",
      "11     0.513900\n",
      "12     0.505779\n",
      "13     0.499347\n",
      "14     0.485626\n",
      "15     0.479208\n",
      "16     0.496178\n",
      "17     0.508299\n",
      "18     0.484969\n",
      "19     0.470046\n",
      "20     0.512476\n",
      "21     0.496459\n",
      "22     0.517103\n",
      "23     0.483557\n",
      "24     0.492251\n",
      "25     0.508654\n",
      "26     0.481999\n",
      "27     0.505541\n",
      "28     0.518876\n",
      "29     0.484438\n",
      "...         ...\n",
      "12264  0.480710\n",
      "12265  0.504234\n",
      "12266  0.495873\n",
      "12267  0.508145\n",
      "12268  0.476466\n",
      "12269  0.493516\n",
      "12270  0.507037\n",
      "12271  0.491073\n",
      "12272  0.508499\n",
      "12273  0.498954\n",
      "12274  0.509529\n",
      "12275  0.483088\n",
      "12276  0.482726\n",
      "12277  0.496151\n",
      "12278  0.503802\n",
      "12279  0.513075\n",
      "12280  0.509349\n",
      "12281  0.507176\n",
      "12282  0.505869\n",
      "12283  0.507037\n",
      "12284  0.506872\n",
      "12285  0.488714\n",
      "12286  0.507037\n",
      "12287  0.488833\n",
      "12288  0.520529\n",
      "12289  0.490248\n",
      "12290  0.512563\n",
      "12291  0.509618\n",
      "12292  0.486898\n",
      "12293  0.479786\n",
      "\n",
      "[36881 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第6个模型， 第1次迭代########################################################\n",
      "第6个模型， 第2次迭代########################################################\n",
      "第6个模型， 第3次迭代########################################################\n",
      "              0\n",
      "0      0.538569\n",
      "1      0.847160\n",
      "2      0.014573\n",
      "3      0.094938\n",
      "4      0.006204\n",
      "5      0.006405\n",
      "6      0.902047\n",
      "7      0.478989\n",
      "8      0.110134\n",
      "9      0.871128\n",
      "10     0.262961\n",
      "11     0.956820\n",
      "12     0.873927\n",
      "13     0.398033\n",
      "14     0.044429\n",
      "15     0.034643\n",
      "16     0.277051\n",
      "17     0.915228\n",
      "18     0.016331\n",
      "19     0.002521\n",
      "20     0.903600\n",
      "21     0.680436\n",
      "22     0.926500\n",
      "23     0.019123\n",
      "24     0.128423\n",
      "25     0.904732\n",
      "26     0.039596\n",
      "27     0.817428\n",
      "28     0.954993\n",
      "29     0.037844\n",
      "...         ...\n",
      "12264  0.029453\n",
      "12265  0.676844\n",
      "12266  0.614203\n",
      "12267  0.906673\n",
      "12268  0.004446\n",
      "12269  0.097610\n",
      "12270  0.737781\n",
      "12271  0.063323\n",
      "12272  0.771095\n",
      "12273  0.259779\n",
      "12274  0.965695\n",
      "12275  0.023144\n",
      "12276  0.011662\n",
      "12277  0.038425\n",
      "12278  0.353107\n",
      "12279  0.915685\n",
      "12280  0.908240\n",
      "12281  0.721551\n",
      "12282  0.772923\n",
      "12283  0.809554\n",
      "12284  0.879833\n",
      "12285  0.020196\n",
      "12286  0.884958\n",
      "12287  0.082294\n",
      "12288  0.982636\n",
      "12289  0.049273\n",
      "12290  0.970178\n",
      "12291  0.929003\n",
      "12292  0.032744\n",
      "12293  0.009630\n",
      "\n",
      "[36881 rows x 1 columns]\n",
      "第7个模型， 第1次迭代########################################################\n",
      "第7个模型， 第2次迭代########################################################\n",
      "第7个模型， 第3次迭代########################################################\n",
      "              0\n",
      "0      0.677206\n",
      "1      0.827391\n",
      "2      0.024515\n",
      "3      0.136235\n",
      "4      0.014965\n",
      "5      0.019009\n",
      "6      0.867475\n",
      "7      0.649766\n",
      "8      0.320249\n",
      "9      0.845300\n",
      "10     0.314328\n",
      "11     0.932609\n",
      "12     0.822349\n",
      "13     0.538918\n",
      "14     0.070893\n",
      "15     0.076817\n",
      "16     0.266840\n",
      "17     0.857949\n",
      "18     0.038976\n",
      "19     0.013297\n",
      "20     0.904340\n",
      "21     0.635910\n",
      "22     0.924780\n",
      "23     0.050270\n",
      "24     0.194926\n",
      "25     0.856269\n",
      "26     0.069676\n",
      "27     0.801162\n",
      "28     0.944321\n",
      "29     0.070965\n",
      "...         ...\n",
      "12264  0.039824\n",
      "12265  0.705377\n",
      "12266  0.405948\n",
      "12267  0.918593\n",
      "12268  0.017820\n",
      "12269  0.111801\n",
      "12270  0.746502\n",
      "12271  0.110608\n",
      "12272  0.782696\n",
      "12273  0.170657\n",
      "12274  0.919397\n",
      "12275  0.029596\n",
      "12276  0.018016\n",
      "12277  0.174647\n",
      "12278  0.446184\n",
      "12279  0.874516\n",
      "12280  0.878493\n",
      "12281  0.832474\n",
      "12282  0.799380\n",
      "12283  0.777222\n",
      "12284  0.800128\n",
      "12285  0.029415\n",
      "12286  0.843068\n",
      "12287  0.123971\n",
      "12288  0.965605\n",
      "12289  0.052983\n",
      "12290  0.953069\n",
      "12291  0.910885\n",
      "12292  0.039560\n",
      "12293  0.023507\n",
      "\n",
      "[36881 rows x 1 columns]\n",
      "Train Features Shape (36881, 7)\n",
      "Test Features Shape (9221, 7)\n"
     ]
    }
   ],
   "source": [
    "pred_LGB = s.fit_multi_lgb(\n",
    "        X=x_train,\n",
    "        y=y_train,\n",
    "        X_target=x_test,\n",
    "        models=models\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9221,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_LGB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9221, 1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc, precision_recall_curve,accuracy_score, precision_score, recall_score,f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84188265914759786"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true=y_test, y_pred=pred_LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80268663290024223"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_true=y_test, y_pred=pred_LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86641312098882817"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_true=y_test, y_pred=pred_LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83333333333333337"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    f1_score(y_true=y_test, y_pred=pred_LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "roc_auc_score() got an unexpected keyword argument 'y_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-ca6aa0aaa437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_LGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: roc_auc_score() got an unexpected keyword argument 'y_pred'"
     ]
    }
   ],
   "source": [
    "roc_auc_score(y_true=y_test, y_pred=pred_LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(data, num_parts):\n",
    "    result = []\n",
    "    length = len(data)\n",
    "    for i in range(num_parts):\n",
    "        start = length * i//num_parts\n",
    "        end   = length * (i+1)//num_parts\n",
    "        result.append(data[start:end])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = split_data(X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6    \\\n",
      "30734 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30735 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30736 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30737 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30738 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30739 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30740 -0.334609 -0.416400 -0.493638 -0.488363 -0.544817 -0.538918 -0.676487   \n",
      "30741 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30742 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30743 -0.334609 -0.416400 -0.493638 -0.417401 -0.449044 -0.538918 -0.676487   \n",
      "30744 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30745 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30746 -0.334609 -0.416400 -0.493638 -0.559324 -0.353271 -0.538918 -0.676487   \n",
      "30747 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30748 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30749 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30750 -0.334609  0.003075 -0.241095  0.717981  0.700230 -0.538918  1.295652   \n",
      "30751 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30752 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30753 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30754 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30755 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30756 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30757 -0.334609 -0.416400  0.642807  0.079329 -0.161726 -0.538918 -0.676487   \n",
      "30758 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30759 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30760 -0.334609  0.003075  0.137721 -0.204517  0.077707 -0.538918  1.295652   \n",
      "30761 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30762 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "30763 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "46072 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46073 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46074 -0.334609 -0.416400  0.263992 -0.133556 -0.305385 -0.538918 -0.676487   \n",
      "46075 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46076 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46077 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46078 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46079 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46080 -0.334609  2.729667  1.400438  0.505097  0.125593 -0.538918  1.295652   \n",
      "46081 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46082 -0.334609  0.632289  0.137721 -0.062594 -0.257498 -0.538918  1.295652   \n",
      "46083 -0.334609 -0.416400  1.779253  0.717981  0.317139 -0.538918 -0.676487   \n",
      "46084 -0.334609 -0.416400 -0.493638 -0.133556 -0.305385 -0.538918 -0.676487   \n",
      "46085 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46086 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46087 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46088 -0.334609 -0.206662 -0.114823  0.079329  0.029820 -0.538918  1.295652   \n",
      "46089 -0.334609 -0.416400 -0.493638 -0.559324 -0.544817 -0.538918 -0.676487   \n",
      "46090 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46091 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46092 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46093 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46094 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46095 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46096 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46097 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46098 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46099 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46100 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
      "46101 -0.334609 -0.416400 -0.493638 -0.275478 -0.353271 -0.538918 -0.676487   \n",
      "\n",
      "            7         8         9      ...          164  165       166  167  \\\n",
      "30734 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "30735 -0.810187 -0.888607 -0.919954    ...    -0.653359  0.0 -0.652301  0.0   \n",
      "30736 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "30737 -0.810187 -0.888607 -0.919954    ...    -0.984604  0.0 -0.652301  0.0   \n",
      "30738 -0.810187 -0.888607 -0.919954    ...    -0.653359  0.0 -0.652301  0.0   \n",
      "30739 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "30740 -0.810187  0.581045  0.331092    ...    -0.984604  0.0  1.533034  0.0   \n",
      "30741 -0.810187 -0.888607 -0.919954    ...    -0.818982  0.0 -0.652301  0.0   \n",
      "30742 -0.810187 -0.888607 -0.919954    ...    -0.818982  0.0 -0.652301  0.0   \n",
      "30743 -0.810187  0.581045  0.331092    ...     1.334114  0.0 -0.652301  0.0   \n",
      "30744 -0.810187 -0.888607 -0.919954    ...    -0.818982  0.0 -0.652301  0.0   \n",
      "30745 -0.810187 -0.888607 -0.919954    ...    -0.984604  0.0 -0.652301  0.0   \n",
      "30746 -0.810187 -0.888607  0.331092    ...     0.340378  0.0 -0.652301  0.0   \n",
      "30747 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "30748 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "30749 -0.810187 -0.888607 -0.919954    ...     0.174755  0.0 -0.652301  0.0   \n",
      "30750  0.942980  0.581045  0.331092    ...    -0.487736  0.0 -0.652301  0.0   \n",
      "30751 -0.810187 -0.888607 -0.919954    ...     0.009132  0.0 -0.652301  0.0   \n",
      "30752 -0.810187 -0.888607 -0.919954    ...    -0.487736  0.0 -0.652301  0.0   \n",
      "30753 -0.810187 -0.888607 -0.919954    ...    -0.487736  0.0 -0.652301  0.0   \n",
      "30754 -0.810187 -0.888607 -0.919954    ...     0.174755  0.0 -0.652301  0.0   \n",
      "30755 -0.810187 -0.888607 -0.919954    ...    -0.818982  0.0 -0.652301  0.0   \n",
      "30756 -0.810187 -0.888607 -0.919954    ...     1.334114  0.0 -0.652301  0.0   \n",
      "30757  0.942980  0.581045  0.331092    ...    -0.322113  0.0 -0.652301  0.0   \n",
      "30758 -0.810187 -0.888607 -0.919954    ...    -0.818982  0.0 -0.652301  0.0   \n",
      "30759 -0.810187 -0.888607 -0.919954    ...    -0.818982  0.0 -0.652301  0.0   \n",
      "30760  0.942980  0.581045  0.331092    ...     0.009132  0.0 -0.652301  0.0   \n",
      "30761 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "30762 -0.810187 -0.888607 -0.919954    ...    -0.653359  0.0 -0.652301  0.0   \n",
      "30763 -0.810187 -0.888607 -0.919954    ...    -0.322113  0.0 -0.652301  0.0   \n",
      "...         ...       ...       ...    ...          ...  ...       ...  ...   \n",
      "46072 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "46073 -0.810187 -0.888607 -0.919954    ...    -0.487736  0.0 -0.652301  0.0   \n",
      "46074  0.942980  0.581045  0.331092    ...     0.671623  0.0 -0.652301  0.0   \n",
      "46075 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "46076 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "46077 -0.810187 -0.888607 -0.919954    ...     0.174755  0.0 -0.652301  0.0   \n",
      "46078 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "46079 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "46080  0.942980  0.581045  0.331092    ...    -0.487736  0.0 -0.652301  0.0   \n",
      "46081 -0.810187 -0.888607 -0.919954    ...     0.506000  0.0 -0.652301  0.0   \n",
      "46082  0.942980  2.050697  1.582138    ...     1.168491  0.0  1.533034  0.0   \n",
      "46083  0.942980  0.581045  0.331092    ...     1.002869  0.0 -0.652301  0.0   \n",
      "46084 -0.810187  0.581045  0.331092    ...     1.334114  0.0 -0.652301  0.0   \n",
      "46085 -0.810187 -0.888607 -0.919954    ...    -0.984604  0.0 -0.652301  0.0   \n",
      "46086 -0.810187 -0.888607 -0.919954    ...    -0.487736  0.0 -0.652301  0.0   \n",
      "46087 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "46088  0.942980  0.581045  0.331092    ...     1.499737  0.0  1.533034  0.0   \n",
      "46089 -0.810187 -0.888607  0.331092    ...    -0.156491  0.0 -0.652301  0.0   \n",
      "46090 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "46091 -0.810187 -0.888607 -0.919954    ...    -0.984604  0.0 -0.652301  0.0   \n",
      "46092 -0.810187 -0.888607 -0.919954    ...    -0.984604  0.0 -0.652301  0.0   \n",
      "46093 -0.810187 -0.888607 -0.919954    ...    -0.653359  0.0 -0.652301  0.0   \n",
      "46094 -0.810187 -0.888607 -0.919954    ...    -0.487736  0.0 -0.652301  0.0   \n",
      "46095 -0.810187 -0.888607 -0.919954    ...    -0.984604  0.0 -0.652301  0.0   \n",
      "46096 -0.810187 -0.888607 -0.919954    ...    -0.156491  0.0 -0.652301  0.0   \n",
      "46097 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "46098 -0.810187 -0.888607 -0.919954    ...    -0.818982  0.0 -0.652301  0.0   \n",
      "46099 -0.810187 -0.888607 -0.919954    ...     0.671623  0.0 -0.652301  0.0   \n",
      "46100 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
      "46101 -0.810187  0.581045  1.582138    ...     0.174755  0.0  1.533034  0.0   \n",
      "\n",
      "       168       169       170       171       172       173  \n",
      "30734  0.0 -1.554953 -0.969474 -1.357383 -1.427762 -0.096484  \n",
      "30735  0.0 -1.081882 -0.533171  0.293302 -0.352890 -0.096484  \n",
      "30736  0.0 -1.560867 -0.963414 -1.362007 -1.435008 -0.096484  \n",
      "30737  0.0 -1.495819 -0.902816 -0.705432 -1.065445  0.120387  \n",
      "30738  0.0 -1.365725 -0.854338 -1.223294 -1.275589 -0.056575  \n",
      "30739  0.0 -1.525386 -0.933115 -1.334264 -1.413269 -0.096484  \n",
      "30740  0.0  0.473342 -0.721024  0.228569  0.946618 -0.055456  \n",
      "30741  0.0 -0.868999 -0.951294 -0.834897 -0.608927 -0.096484  \n",
      "30742  0.0 -1.401205 -0.951294 -1.251037 -1.261096 -0.096484  \n",
      "30743  0.0  0.839972  0.036446  0.515243  0.905556 -0.096484  \n",
      "30744  0.0 -0.975440 -0.363498 -0.904254 -1.079938 -0.096484  \n",
      "30745  0.0 -1.342071 -0.969474 -1.190928 -1.166894 -0.096484  \n",
      "30746  0.0  0.449688 -0.224124  0.210074  0.655040 -0.026698  \n",
      "30747  0.0 -1.554953 -0.963414 -1.362007 -1.435008 -0.096484  \n",
      "30748  0.0 -1.554953 -0.969474 -1.357383 -1.427762 -0.096484  \n",
      "30749  0.0 -0.295400 -0.957354 -0.381768  0.101213 -0.096484  \n",
      "30750  0.0  0.780838 -0.224124  0.469005  0.778745 -0.096484  \n",
      "30751  0.0 -0.005644 -0.012032 -0.159827 -0.367383 -0.040858  \n",
      "30752  0.0 -0.845346 -0.945235 -0.821026 -0.587188 -0.083313  \n",
      "30753  0.0 -0.697511 -0.963414 -0.691561 -0.384291 -0.089044  \n",
      "30754  0.0 -0.466888 -0.042331 -0.506610 -0.590811  0.089036  \n",
      "30755  0.0 -0.691597 -0.642247 -0.682313 -0.565449 -0.096484  \n",
      "30756  0.0  0.609350  0.412151  0.769550  0.438497 -0.055350  \n",
      "30757  0.0  0.219066 -0.218064  0.029747  0.226816 -0.096455  \n",
      "30758  0.0 -0.892653 -0.284721 -0.839521 -1.060614 -0.096484  \n",
      "30759  0.0 -1.206063 -0.963414 -1.089205 -1.007475 -0.096484  \n",
      "30760  0.0  0.047577 -0.472574 -0.108966  0.235270 -0.047065  \n",
      "30761  0.0 -1.543126 -0.963414 -1.348136 -1.416892 -0.096484  \n",
      "30762  0.0 -0.721164 -0.139287 -0.705432 -1.165686  0.054166  \n",
      "30763  0.0 -0.703424 -0.369558 -0.788660 -0.819070 -0.096484  \n",
      "...    ...       ...       ...       ...       ...       ...  \n",
      "46072  0.0 -1.549040 -0.951294 -1.362007 -1.435008 -0.096484  \n",
      "46073  0.0 -0.549676 -0.054451 -0.571343 -0.778491 -0.096445  \n",
      "46074  0.0  0.390554 -0.678605  0.149965  0.771498 -0.086095  \n",
      "46075  0.0 -1.554953 -0.969474 -1.357383 -1.427762 -0.091218  \n",
      "46076  0.0 -1.211976 -0.969474 -1.089205 -1.007475 -0.096484  \n",
      "46077  0.0 -0.112085  0.333374  0.353411 -0.302407 -0.074682  \n",
      "46078  0.0 -1.560867 -0.969474 -1.362007 -1.435008 -0.096484  \n",
      "46079  0.0 -0.768472 -0.963414 -0.747046 -0.471247 -0.096484  \n",
      "46080  0.0 -0.047037 -0.702844 -0.178322  0.189981 -0.096434  \n",
      "46081  0.0 -0.384101  0.054625 -0.446501 -0.790085 -0.039957  \n",
      "46082  0.0  1.372178  0.878752  0.903639  0.902657 -0.032789  \n",
      "46083  0.0  0.295940 -0.478633 -0.025738  0.485269  0.089464  \n",
      "46084  0.0  0.999634  1.442309  0.677075  0.072746  0.123466  \n",
      "46085  0.0 -0.360447 -0.799801 -0.552848 -0.166901 -0.096484  \n",
      "46086  0.0 -0.756645 -0.145347 -0.733175 -0.753853 -0.096484  \n",
      "46087  0.0 -1.466252 -0.969474 -1.288027 -1.319067 -0.096484  \n",
      "46088  0.0  1.206603  0.654541  0.801916  0.751571 -0.003865  \n",
      "46089  0.0  0.686224  1.333234  0.395025 -0.106515 -0.096484  \n",
      "46090  0.0 -1.537213 -0.957354 -1.343512 -1.413269 -0.096484  \n",
      "46091  0.0 -0.833519 -0.224124  0.066737 -0.391537 -0.096362  \n",
      "46092  0.0 -1.525386 -0.969474 -1.334264 -1.391530 -0.096484  \n",
      "46093  0.0 -0.934047 -0.963414 -0.876511 -0.674144 -0.096484  \n",
      "46094  0.0 -0.863086 -0.884637 -0.821026 -0.634289 -0.096484  \n",
      "46095  0.0 -1.235630 -0.969474 -0.987482 -0.848056 -0.096484  \n",
      "46096  0.0  0.130365  0.672720 -0.039609 -0.775592 -0.096420  \n",
      "46097  0.0 -1.383465 -0.969474 -1.223294 -1.217618 -0.096484  \n",
      "46098  0.0 -1.365725 -0.963414 -1.214046 -1.203126 -0.096484  \n",
      "46099  0.0  0.313680  0.060685  0.103727  0.135029 -0.084654  \n",
      "46100  0.0 -1.472166 -0.957354 -1.301898 -1.340806 -0.096484  \n",
      "46101  0.0 -0.283573 -0.969474 -0.363273  0.130199 -0.096484  \n",
      "\n",
      "[15368 rows x 174 columns]\n"
     ]
    }
   ],
   "source": [
    "for feature in xx[1:]:\n",
    "    print( feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.329699</td>\n",
       "      <td>0.632289</td>\n",
       "      <td>0.137721</td>\n",
       "      <td>-0.204517</td>\n",
       "      <td>-0.353271</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.484532</td>\n",
       "      <td>2.151301</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>0.721982</td>\n",
       "      <td>4.175879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.822232</td>\n",
       "      <td>1.321114</td>\n",
       "      <td>0.501372</td>\n",
       "      <td>0.569809</td>\n",
       "      <td>-0.048546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.331114</td>\n",
       "      <td>0.632289</td>\n",
       "      <td>0.137721</td>\n",
       "      <td>-0.062594</td>\n",
       "      <td>-0.209612</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>2.162228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.809769</td>\n",
       "      <td>1.496847</td>\n",
       "      <td>1.666561</td>\n",
       "      <td>1.657966</td>\n",
       "      <td>-0.096418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.150227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.560867</td>\n",
       "      <td>-0.878577</td>\n",
       "      <td>-1.362007</td>\n",
       "      <td>-1.435008</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.984604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.513559</td>\n",
       "      <td>-0.920996</td>\n",
       "      <td>-1.325017</td>\n",
       "      <td>-1.406023</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.329699</td>\n",
       "      <td>0.842027</td>\n",
       "      <td>0.263992</td>\n",
       "      <td>0.292213</td>\n",
       "      <td>0.221366</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>3.267791</td>\n",
       "      <td>2.696148</td>\n",
       "      <td>4.990002</td>\n",
       "      <td>4.084229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.322113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.472802</td>\n",
       "      <td>-0.296841</td>\n",
       "      <td>-0.377144</td>\n",
       "      <td>-0.398783</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.206662</td>\n",
       "      <td>-0.367366</td>\n",
       "      <td>-0.417401</td>\n",
       "      <td>-0.401158</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165845</td>\n",
       "      <td>-0.424096</td>\n",
       "      <td>-0.011867</td>\n",
       "      <td>0.354835</td>\n",
       "      <td>-0.096482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.331114</td>\n",
       "      <td>0.842027</td>\n",
       "      <td>0.390264</td>\n",
       "      <td>0.647020</td>\n",
       "      <td>0.508685</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.283573</td>\n",
       "      <td>-0.890697</td>\n",
       "      <td>-0.423382</td>\n",
       "      <td>0.035996</td>\n",
       "      <td>-0.091282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.204517</td>\n",
       "      <td>-0.161726</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>2.493473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.165209</td>\n",
       "      <td>0.133402</td>\n",
       "      <td>0.755679</td>\n",
       "      <td>1.385262</td>\n",
       "      <td>-0.096458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.001747</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>0.011449</td>\n",
       "      <td>0.079329</td>\n",
       "      <td>0.077707</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>2.824719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.839336</td>\n",
       "      <td>0.678780</td>\n",
       "      <td>1.389135</td>\n",
       "      <td>2.120523</td>\n",
       "      <td>-0.090321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>0.212813</td>\n",
       "      <td>0.011449</td>\n",
       "      <td>-0.133556</td>\n",
       "      <td>-0.209612</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.168491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.603436</td>\n",
       "      <td>-0.896757</td>\n",
       "      <td>0.274807</td>\n",
       "      <td>1.130192</td>\n",
       "      <td>-0.086575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>0.632289</td>\n",
       "      <td>0.642807</td>\n",
       "      <td>0.647020</td>\n",
       "      <td>0.269252</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>2.696148</td>\n",
       "      <td>2.050697</td>\n",
       "      <td>1.582138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.644830</td>\n",
       "      <td>1.109023</td>\n",
       "      <td>0.270183</td>\n",
       "      <td>0.009426</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.662561</td>\n",
       "      <td>3.149143</td>\n",
       "      <td>2.663155</td>\n",
       "      <td>3.201630</td>\n",
       "      <td>2.950893</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.721704</td>\n",
       "      <td>0.951469</td>\n",
       "      <td>0.427391</td>\n",
       "      <td>0.291429</td>\n",
       "      <td>-0.069297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.206662</td>\n",
       "      <td>-0.241095</td>\n",
       "      <td>-0.417401</td>\n",
       "      <td>-0.449044</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.830982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.460879</td>\n",
       "      <td>1.727118</td>\n",
       "      <td>0.949877</td>\n",
       "      <td>0.504109</td>\n",
       "      <td>0.022619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>1.665360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.809769</td>\n",
       "      <td>2.302795</td>\n",
       "      <td>1.375264</td>\n",
       "      <td>1.263524</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>2.493473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.809769</td>\n",
       "      <td>1.157501</td>\n",
       "      <td>1.389135</td>\n",
       "      <td>1.702651</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.331114</td>\n",
       "      <td>1.261502</td>\n",
       "      <td>0.642807</td>\n",
       "      <td>1.569518</td>\n",
       "      <td>1.514300</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>1.582138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.271746</td>\n",
       "      <td>0.351553</td>\n",
       "      <td>-0.377144</td>\n",
       "      <td>-0.663274</td>\n",
       "      <td>-0.096432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>1.143750</td>\n",
       "      <td>0.748117</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.330880</td>\n",
       "      <td>-0.199885</td>\n",
       "      <td>-0.423382</td>\n",
       "      <td>-0.406030</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>1.680978</td>\n",
       "      <td>0.769079</td>\n",
       "      <td>0.505097</td>\n",
       "      <td>0.221366</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.254006</td>\n",
       "      <td>-0.927055</td>\n",
       "      <td>-0.367897</td>\n",
       "      <td>0.119329</td>\n",
       "      <td>-0.096430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>0.842027</td>\n",
       "      <td>0.263992</td>\n",
       "      <td>-0.133556</td>\n",
       "      <td>-0.305385</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.499737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.194776</td>\n",
       "      <td>1.854373</td>\n",
       "      <td>0.681698</td>\n",
       "      <td>0.069122</td>\n",
       "      <td>0.069632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.996838</td>\n",
       "      <td>1.051764</td>\n",
       "      <td>0.516536</td>\n",
       "      <td>0.859904</td>\n",
       "      <td>0.843890</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.525926</td>\n",
       "      <td>1.054485</td>\n",
       "      <td>1.611076</td>\n",
       "      <td>1.152966</td>\n",
       "      <td>-0.096459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.488363</td>\n",
       "      <td>-0.257498</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.715155</td>\n",
       "      <td>1.090843</td>\n",
       "      <td>1.416878</td>\n",
       "      <td>1.728014</td>\n",
       "      <td>-0.096450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.206662</td>\n",
       "      <td>-0.367366</td>\n",
       "      <td>0.434136</td>\n",
       "      <td>0.412912</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>2.050697</td>\n",
       "      <td>1.582138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.987807</td>\n",
       "      <td>1.405951</td>\n",
       "      <td>0.630837</td>\n",
       "      <td>0.077663</td>\n",
       "      <td>1.963398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.206662</td>\n",
       "      <td>0.011449</td>\n",
       "      <td>-0.275478</td>\n",
       "      <td>-0.401158</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.786752</td>\n",
       "      <td>-0.757382</td>\n",
       "      <td>0.473629</td>\n",
       "      <td>1.314973</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.331114</td>\n",
       "      <td>1.890716</td>\n",
       "      <td>1.779253</td>\n",
       "      <td>1.569518</td>\n",
       "      <td>1.562186</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384641</td>\n",
       "      <td>-0.969474</td>\n",
       "      <td>0.159213</td>\n",
       "      <td>0.949033</td>\n",
       "      <td>-0.073377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.663976</td>\n",
       "      <td>0.212813</td>\n",
       "      <td>0.263992</td>\n",
       "      <td>0.717981</td>\n",
       "      <td>0.460798</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.487736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.319053</td>\n",
       "      <td>-0.830099</td>\n",
       "      <td>-0.497362</td>\n",
       "      <td>-0.079945</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.150227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.454425</td>\n",
       "      <td>-0.969474</td>\n",
       "      <td>-1.278779</td>\n",
       "      <td>-1.304574</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>0.221251</td>\n",
       "      <td>0.125593</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>2.162228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.815682</td>\n",
       "      <td>1.933149</td>\n",
       "      <td>1.874631</td>\n",
       "      <td>1.184818</td>\n",
       "      <td>0.113359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>0.842027</td>\n",
       "      <td>0.516536</td>\n",
       "      <td>0.292213</td>\n",
       "      <td>0.125593</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>3.267791</td>\n",
       "      <td>2.696148</td>\n",
       "      <td>2.050697</td>\n",
       "      <td>1.582138</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.322113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.413668</td>\n",
       "      <td>-0.951294</td>\n",
       "      <td>-0.478867</td>\n",
       "      <td>-0.050960</td>\n",
       "      <td>-0.091181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>2.284339</td>\n",
       "      <td>1.214711</td>\n",
       "      <td>0.987549</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>2.050697</td>\n",
       "      <td>2.833183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.052951</td>\n",
       "      <td>0.206119</td>\n",
       "      <td>-0.233808</td>\n",
       "      <td>-0.403614</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46072</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.150227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.549040</td>\n",
       "      <td>-0.951294</td>\n",
       "      <td>-1.362007</td>\n",
       "      <td>-1.435008</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46073</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.487736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.549676</td>\n",
       "      <td>-0.054451</td>\n",
       "      <td>-0.571343</td>\n",
       "      <td>-0.778491</td>\n",
       "      <td>-0.096445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46074</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>0.263992</td>\n",
       "      <td>-0.133556</td>\n",
       "      <td>-0.305385</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.671623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.390554</td>\n",
       "      <td>-0.678605</td>\n",
       "      <td>0.149965</td>\n",
       "      <td>0.771498</td>\n",
       "      <td>-0.086095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46075</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.150227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.554953</td>\n",
       "      <td>-0.969474</td>\n",
       "      <td>-1.357383</td>\n",
       "      <td>-1.427762</td>\n",
       "      <td>-0.091218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46076</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.150227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.211976</td>\n",
       "      <td>-0.969474</td>\n",
       "      <td>-1.089205</td>\n",
       "      <td>-1.007475</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46077</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.112085</td>\n",
       "      <td>0.333374</td>\n",
       "      <td>0.353411</td>\n",
       "      <td>-0.302407</td>\n",
       "      <td>-0.074682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46078</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.150227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.560867</td>\n",
       "      <td>-0.969474</td>\n",
       "      <td>-1.362007</td>\n",
       "      <td>-1.435008</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46079</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.150227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.768472</td>\n",
       "      <td>-0.963414</td>\n",
       "      <td>-0.747046</td>\n",
       "      <td>-0.471247</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46080</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>2.729667</td>\n",
       "      <td>1.400438</td>\n",
       "      <td>0.505097</td>\n",
       "      <td>0.125593</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.487736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.047037</td>\n",
       "      <td>-0.702844</td>\n",
       "      <td>-0.178322</td>\n",
       "      <td>0.189981</td>\n",
       "      <td>-0.096434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46081</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.384101</td>\n",
       "      <td>0.054625</td>\n",
       "      <td>-0.446501</td>\n",
       "      <td>-0.790085</td>\n",
       "      <td>-0.039957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46082</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>0.632289</td>\n",
       "      <td>0.137721</td>\n",
       "      <td>-0.062594</td>\n",
       "      <td>-0.257498</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>2.050697</td>\n",
       "      <td>1.582138</td>\n",
       "      <td>...</td>\n",
       "      <td>1.168491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.372178</td>\n",
       "      <td>0.878752</td>\n",
       "      <td>0.903639</td>\n",
       "      <td>0.902657</td>\n",
       "      <td>-0.032789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46083</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>1.779253</td>\n",
       "      <td>0.717981</td>\n",
       "      <td>0.317139</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.295940</td>\n",
       "      <td>-0.478633</td>\n",
       "      <td>-0.025738</td>\n",
       "      <td>0.485269</td>\n",
       "      <td>0.089464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46084</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.133556</td>\n",
       "      <td>-0.305385</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.334114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>1.442309</td>\n",
       "      <td>0.677075</td>\n",
       "      <td>0.072746</td>\n",
       "      <td>0.123466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46085</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.984604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.360447</td>\n",
       "      <td>-0.799801</td>\n",
       "      <td>-0.552848</td>\n",
       "      <td>-0.166901</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46086</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.487736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.756645</td>\n",
       "      <td>-0.145347</td>\n",
       "      <td>-0.733175</td>\n",
       "      <td>-0.753853</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46087</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.150227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.466252</td>\n",
       "      <td>-0.969474</td>\n",
       "      <td>-1.288027</td>\n",
       "      <td>-1.319067</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46088</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.206662</td>\n",
       "      <td>-0.114823</td>\n",
       "      <td>0.079329</td>\n",
       "      <td>0.029820</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>1.295652</td>\n",
       "      <td>0.942980</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.499737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.206603</td>\n",
       "      <td>0.654541</td>\n",
       "      <td>0.801916</td>\n",
       "      <td>0.751571</td>\n",
       "      <td>-0.003865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46089</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.544817</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686224</td>\n",
       "      <td>1.333234</td>\n",
       "      <td>0.395025</td>\n",
       "      <td>-0.106515</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46090</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.150227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.537213</td>\n",
       "      <td>-0.957354</td>\n",
       "      <td>-1.343512</td>\n",
       "      <td>-1.413269</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46091</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.984604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.833519</td>\n",
       "      <td>-0.224124</td>\n",
       "      <td>0.066737</td>\n",
       "      <td>-0.391537</td>\n",
       "      <td>-0.096362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46092</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.984604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.525386</td>\n",
       "      <td>-0.969474</td>\n",
       "      <td>-1.334264</td>\n",
       "      <td>-1.391530</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46093</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.934047</td>\n",
       "      <td>-0.963414</td>\n",
       "      <td>-0.876511</td>\n",
       "      <td>-0.674144</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46094</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.487736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.863086</td>\n",
       "      <td>-0.884637</td>\n",
       "      <td>-0.821026</td>\n",
       "      <td>-0.634289</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46095</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.984604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.235630</td>\n",
       "      <td>-0.969474</td>\n",
       "      <td>-0.987482</td>\n",
       "      <td>-0.848056</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46096</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130365</td>\n",
       "      <td>0.672720</td>\n",
       "      <td>-0.039609</td>\n",
       "      <td>-0.775592</td>\n",
       "      <td>-0.096420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46097</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.150227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.383465</td>\n",
       "      <td>-0.969474</td>\n",
       "      <td>-1.223294</td>\n",
       "      <td>-1.217618</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46098</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.818982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.365725</td>\n",
       "      <td>-0.963414</td>\n",
       "      <td>-1.214046</td>\n",
       "      <td>-1.203126</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46099</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.671623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313680</td>\n",
       "      <td>0.060685</td>\n",
       "      <td>0.103727</td>\n",
       "      <td>0.135029</td>\n",
       "      <td>-0.084654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46100</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.559324</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>-0.888607</td>\n",
       "      <td>-0.919954</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.150227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.652301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.472166</td>\n",
       "      <td>-0.957354</td>\n",
       "      <td>-1.301898</td>\n",
       "      <td>-1.340806</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46101</th>\n",
       "      <td>-0.334609</td>\n",
       "      <td>-0.416400</td>\n",
       "      <td>-0.493638</td>\n",
       "      <td>-0.275478</td>\n",
       "      <td>-0.353271</td>\n",
       "      <td>-0.538918</td>\n",
       "      <td>-0.676487</td>\n",
       "      <td>-0.810187</td>\n",
       "      <td>0.581045</td>\n",
       "      <td>1.582138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.533034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.283573</td>\n",
       "      <td>-0.969474</td>\n",
       "      <td>-0.363273</td>\n",
       "      <td>0.130199</td>\n",
       "      <td>-0.096484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30735 rows × 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      1.329699  0.632289  0.137721 -0.204517 -0.353271  1.718917  1.295652   \n",
       "1     -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "2      0.331114  0.632289  0.137721 -0.062594 -0.209612  1.718917  1.295652   \n",
       "3     -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "4     -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "5      1.329699  0.842027  0.263992  0.292213  0.221366  1.718917  3.267791   \n",
       "6     -0.334609 -0.206662 -0.367366 -0.417401 -0.401158 -0.538918  1.295652   \n",
       "7      0.331114  0.842027  0.390264  0.647020  0.508685  1.718917  1.295652   \n",
       "8     -0.334609 -0.416400 -0.493638 -0.204517 -0.161726 -0.538918 -0.676487   \n",
       "9     -0.001747  0.003075  0.011449  0.079329  0.077707  1.718917  1.295652   \n",
       "10    -0.334609  0.212813  0.011449 -0.133556 -0.209612 -0.538918  1.295652   \n",
       "11    -0.334609  0.632289  0.642807  0.647020  0.269252 -0.538918  1.295652   \n",
       "12     1.662561  3.149143  2.663155  3.201630  2.950893  1.718917  1.295652   \n",
       "13    -0.334609 -0.206662 -0.241095 -0.417401 -0.449044 -0.538918  1.295652   \n",
       "14    -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "15    -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "16     0.331114  1.261502  0.642807  1.569518  1.514300  1.718917  1.295652   \n",
       "17    -0.334609 -0.416400 -0.493638  1.143750  0.748117 -0.538918 -0.676487   \n",
       "18    -0.334609  1.680978  0.769079  0.505097  0.221366 -0.538918  1.295652   \n",
       "19    -0.334609  0.842027  0.263992 -0.133556 -0.305385 -0.538918  1.295652   \n",
       "20     0.996838  1.051764  0.516536  0.859904  0.843890  1.718917  1.295652   \n",
       "21    -0.334609 -0.416400 -0.493638 -0.488363 -0.257498 -0.538918 -0.676487   \n",
       "22    -0.334609 -0.206662 -0.367366  0.434136  0.412912 -0.538918  1.295652   \n",
       "23    -0.334609 -0.206662  0.011449 -0.275478 -0.401158 -0.538918  1.295652   \n",
       "24     0.331114  1.890716  1.779253  1.569518  1.562186  1.718917  1.295652   \n",
       "25     0.663976  0.212813  0.263992  0.717981  0.460798  1.718917  1.295652   \n",
       "26    -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "27    -0.334609 -0.416400 -0.493638  0.221251  0.125593 -0.538918 -0.676487   \n",
       "28    -0.334609  0.842027  0.516536  0.292213  0.125593 -0.538918  3.267791   \n",
       "29    -0.334609 -0.416400  2.284339  1.214711  0.987549 -0.538918 -0.676487   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "46072 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46073 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46074 -0.334609 -0.416400  0.263992 -0.133556 -0.305385 -0.538918 -0.676487   \n",
       "46075 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46076 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46077 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46078 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46079 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46080 -0.334609  2.729667  1.400438  0.505097  0.125593 -0.538918  1.295652   \n",
       "46081 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46082 -0.334609  0.632289  0.137721 -0.062594 -0.257498 -0.538918  1.295652   \n",
       "46083 -0.334609 -0.416400  1.779253  0.717981  0.317139 -0.538918 -0.676487   \n",
       "46084 -0.334609 -0.416400 -0.493638 -0.133556 -0.305385 -0.538918 -0.676487   \n",
       "46085 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46086 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46087 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46088 -0.334609 -0.206662 -0.114823  0.079329  0.029820 -0.538918  1.295652   \n",
       "46089 -0.334609 -0.416400 -0.493638 -0.559324 -0.544817 -0.538918 -0.676487   \n",
       "46090 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46091 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46092 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46093 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46094 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46095 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46096 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46097 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46098 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46099 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46100 -0.334609 -0.416400 -0.493638 -0.559324 -0.592703 -0.538918 -0.676487   \n",
       "46101 -0.334609 -0.416400 -0.493638 -0.275478 -0.353271 -0.538918 -0.676487   \n",
       "\n",
       "            7         8         9      ...          164  165       166  167  \\\n",
       "0      0.942980  0.581045  0.331092    ...    -0.156491  0.0  1.533034  0.0   \n",
       "1     -0.810187 -0.888607 -0.919954    ...     0.009132  0.0 -0.652301  0.0   \n",
       "2      0.942980  0.581045  0.331092    ...     2.162228  0.0  1.533034  0.0   \n",
       "3     -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
       "4     -0.810187 -0.888607 -0.919954    ...    -0.984604  0.0 -0.652301  0.0   \n",
       "5      2.696148  4.990002  4.084229    ...    -0.322113  0.0  1.533034  0.0   \n",
       "6      0.942980  0.581045  0.331092    ...     0.506000  0.0  1.533034  0.0   \n",
       "7      0.942980  0.581045  0.331092    ...     0.340378  0.0  1.533034  0.0   \n",
       "8     -0.810187  0.581045  0.331092    ...     2.493473  0.0 -0.652301  0.0   \n",
       "9      0.942980  0.581045  0.331092    ...     2.824719  0.0  1.533034  0.0   \n",
       "10     0.942980  0.581045  0.331092    ...     1.168491  0.0  1.533034  0.0   \n",
       "11     2.696148  2.050697  1.582138    ...     0.174755  0.0  1.533034  0.0   \n",
       "12     0.942980  0.581045  0.331092    ...     1.002869  0.0 -0.652301  0.0   \n",
       "13     0.942980  0.581045  0.331092    ...     1.830982  0.0 -0.652301  0.0   \n",
       "14    -0.810187 -0.888607 -0.919954    ...     1.665360  0.0  1.533034  0.0   \n",
       "15    -0.810187 -0.888607 -0.919954    ...     2.493473  0.0 -0.652301  0.0   \n",
       "16     0.942980  0.581045  1.582138    ...     0.506000  0.0  1.533034  0.0   \n",
       "17    -0.810187  0.581045  0.331092    ...     0.009132  0.0 -0.652301  0.0   \n",
       "18     0.942980  0.581045  0.331092    ...     0.340378  0.0 -0.652301  0.0   \n",
       "19     0.942980  0.581045  0.331092    ...     1.499737  0.0 -0.652301  0.0   \n",
       "20     0.942980  0.581045  0.331092    ...     1.002869  0.0  1.533034  0.0   \n",
       "21    -0.810187  0.581045  0.331092    ...     1.002869  0.0 -0.652301  0.0   \n",
       "22     0.942980  2.050697  1.582138    ...     0.009132  0.0  1.533034  0.0   \n",
       "23     0.942980  0.581045  0.331092    ...    -0.156491  0.0  1.533034  0.0   \n",
       "24     0.942980  0.581045  0.331092    ...     0.506000  0.0  1.533034  0.0   \n",
       "25     0.942980  0.581045  0.331092    ...    -0.487736  0.0  1.533034  0.0   \n",
       "26    -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
       "27    -0.810187  0.581045  0.331092    ...     2.162228  0.0 -0.652301  0.0   \n",
       "28     2.696148  2.050697  1.582138    ...    -0.322113  0.0  1.533034  0.0   \n",
       "29     0.942980  2.050697  2.833183    ...     0.009132  0.0  1.533034  0.0   \n",
       "...         ...       ...       ...    ...          ...  ...       ...  ...   \n",
       "46072 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
       "46073 -0.810187 -0.888607 -0.919954    ...    -0.487736  0.0 -0.652301  0.0   \n",
       "46074  0.942980  0.581045  0.331092    ...     0.671623  0.0 -0.652301  0.0   \n",
       "46075 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
       "46076 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
       "46077 -0.810187 -0.888607 -0.919954    ...     0.174755  0.0 -0.652301  0.0   \n",
       "46078 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
       "46079 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
       "46080  0.942980  0.581045  0.331092    ...    -0.487736  0.0 -0.652301  0.0   \n",
       "46081 -0.810187 -0.888607 -0.919954    ...     0.506000  0.0 -0.652301  0.0   \n",
       "46082  0.942980  2.050697  1.582138    ...     1.168491  0.0  1.533034  0.0   \n",
       "46083  0.942980  0.581045  0.331092    ...     1.002869  0.0 -0.652301  0.0   \n",
       "46084 -0.810187  0.581045  0.331092    ...     1.334114  0.0 -0.652301  0.0   \n",
       "46085 -0.810187 -0.888607 -0.919954    ...    -0.984604  0.0 -0.652301  0.0   \n",
       "46086 -0.810187 -0.888607 -0.919954    ...    -0.487736  0.0 -0.652301  0.0   \n",
       "46087 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
       "46088  0.942980  0.581045  0.331092    ...     1.499737  0.0  1.533034  0.0   \n",
       "46089 -0.810187 -0.888607  0.331092    ...    -0.156491  0.0 -0.652301  0.0   \n",
       "46090 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
       "46091 -0.810187 -0.888607 -0.919954    ...    -0.984604  0.0 -0.652301  0.0   \n",
       "46092 -0.810187 -0.888607 -0.919954    ...    -0.984604  0.0 -0.652301  0.0   \n",
       "46093 -0.810187 -0.888607 -0.919954    ...    -0.653359  0.0 -0.652301  0.0   \n",
       "46094 -0.810187 -0.888607 -0.919954    ...    -0.487736  0.0 -0.652301  0.0   \n",
       "46095 -0.810187 -0.888607 -0.919954    ...    -0.984604  0.0 -0.652301  0.0   \n",
       "46096 -0.810187 -0.888607 -0.919954    ...    -0.156491  0.0 -0.652301  0.0   \n",
       "46097 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
       "46098 -0.810187 -0.888607 -0.919954    ...    -0.818982  0.0 -0.652301  0.0   \n",
       "46099 -0.810187 -0.888607 -0.919954    ...     0.671623  0.0 -0.652301  0.0   \n",
       "46100 -0.810187 -0.888607 -0.919954    ...    -1.150227  0.0 -0.652301  0.0   \n",
       "46101 -0.810187  0.581045  1.582138    ...     0.174755  0.0  1.533034  0.0   \n",
       "\n",
       "       168       169       170       171       172       173  \n",
       "0      0.0  1.484532  2.151301  1.583333  0.721982  4.175879  \n",
       "1      0.0  0.822232  1.321114  0.501372  0.569809 -0.048546  \n",
       "2      0.0  1.809769  1.496847  1.666561  1.657966 -0.096418  \n",
       "3      0.0 -1.560867 -0.878577 -1.362007 -1.435008 -0.096484  \n",
       "4      0.0 -1.513559 -0.920996 -1.325017 -1.406023 -0.096484  \n",
       "5      0.0 -0.472802 -0.296841 -0.377144 -0.398783 -0.096484  \n",
       "6      0.0  0.165845 -0.424096 -0.011867  0.354835 -0.096482  \n",
       "7      0.0 -0.283573 -0.890697 -0.423382  0.035996 -0.091282  \n",
       "8      0.0  1.165209  0.133402  0.755679  1.385262 -0.096458  \n",
       "9      0.0  1.839336  0.678780  1.389135  2.120523 -0.090321  \n",
       "10     0.0  0.603436 -0.896757  0.274807  1.130192 -0.086575  \n",
       "11     0.0  0.644830  1.109023  0.270183  0.009426 -0.096484  \n",
       "12     0.0  0.721704  0.951469  0.427391  0.291429 -0.069297  \n",
       "13     0.0  1.460879  1.727118  0.949877  0.504109  0.022619  \n",
       "14     0.0  1.809769  2.302795  1.375264  1.263524 -0.096484  \n",
       "15     0.0  1.809769  1.157501  1.389135  1.702651 -0.096484  \n",
       "16     0.0 -0.271746  0.351553 -0.377144 -0.663274 -0.096432  \n",
       "17     0.0 -0.330880 -0.199885 -0.423382 -0.406030 -0.096484  \n",
       "18     0.0 -0.254006 -0.927055 -0.367897  0.119329 -0.096430  \n",
       "19     0.0  1.194776  1.854373  0.681698  0.069122  0.069632  \n",
       "20     0.0  1.525926  1.054485  1.611076  1.152966 -0.096459  \n",
       "21     0.0  1.715155  1.090843  1.416878  1.728014 -0.096450  \n",
       "22     0.0  0.987807  1.405951  0.630837  0.077663  1.963398  \n",
       "23     0.0  0.786752 -0.757382  0.473629  1.314973 -0.096484  \n",
       "24     0.0  0.384641 -0.969474  0.159213  0.949033 -0.073377  \n",
       "25     0.0 -0.319053 -0.830099 -0.497362 -0.079945 -0.096484  \n",
       "26     0.0 -1.454425 -0.969474 -1.278779 -1.304574 -0.096484  \n",
       "27     0.0  1.815682  1.933149  1.874631  1.184818  0.113359  \n",
       "28     0.0 -0.413668 -0.951294 -0.478867 -0.050960 -0.091181  \n",
       "29     0.0 -0.052951  0.206119 -0.233808 -0.403614 -0.096484  \n",
       "...    ...       ...       ...       ...       ...       ...  \n",
       "46072  0.0 -1.549040 -0.951294 -1.362007 -1.435008 -0.096484  \n",
       "46073  0.0 -0.549676 -0.054451 -0.571343 -0.778491 -0.096445  \n",
       "46074  0.0  0.390554 -0.678605  0.149965  0.771498 -0.086095  \n",
       "46075  0.0 -1.554953 -0.969474 -1.357383 -1.427762 -0.091218  \n",
       "46076  0.0 -1.211976 -0.969474 -1.089205 -1.007475 -0.096484  \n",
       "46077  0.0 -0.112085  0.333374  0.353411 -0.302407 -0.074682  \n",
       "46078  0.0 -1.560867 -0.969474 -1.362007 -1.435008 -0.096484  \n",
       "46079  0.0 -0.768472 -0.963414 -0.747046 -0.471247 -0.096484  \n",
       "46080  0.0 -0.047037 -0.702844 -0.178322  0.189981 -0.096434  \n",
       "46081  0.0 -0.384101  0.054625 -0.446501 -0.790085 -0.039957  \n",
       "46082  0.0  1.372178  0.878752  0.903639  0.902657 -0.032789  \n",
       "46083  0.0  0.295940 -0.478633 -0.025738  0.485269  0.089464  \n",
       "46084  0.0  0.999634  1.442309  0.677075  0.072746  0.123466  \n",
       "46085  0.0 -0.360447 -0.799801 -0.552848 -0.166901 -0.096484  \n",
       "46086  0.0 -0.756645 -0.145347 -0.733175 -0.753853 -0.096484  \n",
       "46087  0.0 -1.466252 -0.969474 -1.288027 -1.319067 -0.096484  \n",
       "46088  0.0  1.206603  0.654541  0.801916  0.751571 -0.003865  \n",
       "46089  0.0  0.686224  1.333234  0.395025 -0.106515 -0.096484  \n",
       "46090  0.0 -1.537213 -0.957354 -1.343512 -1.413269 -0.096484  \n",
       "46091  0.0 -0.833519 -0.224124  0.066737 -0.391537 -0.096362  \n",
       "46092  0.0 -1.525386 -0.969474 -1.334264 -1.391530 -0.096484  \n",
       "46093  0.0 -0.934047 -0.963414 -0.876511 -0.674144 -0.096484  \n",
       "46094  0.0 -0.863086 -0.884637 -0.821026 -0.634289 -0.096484  \n",
       "46095  0.0 -1.235630 -0.969474 -0.987482 -0.848056 -0.096484  \n",
       "46096  0.0  0.130365  0.672720 -0.039609 -0.775592 -0.096420  \n",
       "46097  0.0 -1.383465 -0.969474 -1.223294 -1.217618 -0.096484  \n",
       "46098  0.0 -1.365725 -0.963414 -1.214046 -1.203126 -0.096484  \n",
       "46099  0.0  0.313680  0.060685  0.103727  0.135029 -0.084654  \n",
       "46100  0.0 -1.472166 -0.957354 -1.301898 -1.340806 -0.096484  \n",
       "46101  0.0 -0.283573 -0.969474 -0.363273  0.130199 -0.096484  \n",
       "\n",
       "[30735 rows x 174 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([xx[0], xx[1]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LGBMClassifier in module lightgbm.sklearn:\n",
      "\n",
      "class LGBMClassifier(LGBMModel, sklearn.base.ClassifierMixin)\n",
      " |  Base class for all estimators in scikit-learn\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  All estimators should specify all the parameters that can be set\n",
      " |  at the class level in their ``__init__`` as explicit keyword\n",
      " |  arguments (no ``*args`` or ``**kwargs``).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LGBMClassifier\n",
      " |      LGBMModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, init_score=None, eval_set=None, eval_names=None, eval_sample_weight=None, eval_init_score=None, eval_metric='logloss', early_stopping_rounds=None, verbose=True, feature_name='auto', categorical_feature='auto', callbacks=None)\n",
      " |      Fit the gradient boosting model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix\n",
      " |      y : array_like\n",
      " |          Labels\n",
      " |      sample_weight : array_like\n",
      " |          weight of training data\n",
      " |      init_score : array_like\n",
      " |          init score of training data\n",
      " |      group : array_like\n",
      " |          group data of training data\n",
      " |      eval_set : list, optional\n",
      " |          A list of (X, y) tuple pairs to use as a validation set for early-stopping\n",
      " |      eval_names: list of string\n",
      " |          Names of eval_set\n",
      " |      eval_sample_weight : List of array\n",
      " |          weight of eval data\n",
      " |      eval_init_score : List of array\n",
      " |          init score of eval data\n",
      " |      eval_group : List of array\n",
      " |          group data of eval data\n",
      " |      eval_metric : str, list of str, callable, optional\n",
      " |          If a str, should be a built-in evaluation metric to use.\n",
      " |          If callable, a custom evaluation metric, see note for more details.\n",
      " |      early_stopping_rounds : int\n",
      " |      verbose : bool\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      " |      feature_name : list of str, or 'auto'\n",
      " |          Feature names\n",
      " |          If 'auto' and data is pandas DataFrame, use data columns name\n",
      " |      categorical_feature : list of str or int, or 'auto'\n",
      " |          Categorical features,\n",
      " |          type int represents index,\n",
      " |          type str represents feature names (need to specify feature_name as well)\n",
      " |          If 'auto' and data is pandas DataFrame, use pandas categorical columns\n",
      " |      callbacks : list of callback functions\n",
      " |          List of callback functions that are applied at each iteration.\n",
      " |          See Callbacks in Python-API.md for more information.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      Custom eval function expects a callable with following functions:\n",
      " |          ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)``\n",
      " |              or ``func(y_true, y_pred, weight, group)``.\n",
      " |          return (eval_name, eval_result, is_bigger_better)\n",
      " |              or list of (eval_name, eval_result, is_bigger_better)\n",
      " |      \n",
      " |          y_true: array_like of shape [n_samples]\n",
      " |              The target values\n",
      " |          y_pred: array_like of shape [n_samples] or shape[n_samples * n_class] (for multi-class)\n",
      " |              The predicted values\n",
      " |          weight: array_like of shape [n_samples]\n",
      " |              The weight of samples\n",
      " |          group: array_like\n",
      " |              group/query data, used for ranking task\n",
      " |          eval_name: str\n",
      " |              name of evaluation\n",
      " |          eval_result: float\n",
      " |              eval result\n",
      " |          is_bigger_better: bool\n",
      " |              is eval result bigger better, e.g. AUC is bigger_better.\n",
      " |      for multi-class task, the y_pred is group by class_id first, then group by row_id\n",
      " |        if you want to get i-th row y_pred in j-th class, the access way is y_pred[j*num_data+i]\n",
      " |  \n",
      " |  predict(self, X, raw_score=False, num_iteration=0)\n",
      " |      Return the predicted value for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      num_iteration : int\n",
      " |          Limit number of iterations in the prediction; defaults to 0 (use all trees).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      predicted_result : array_like, shape=[n_samples] or [n_samples, n_classes]\n",
      " |  \n",
      " |  predict_proba(self, X, raw_score=False, num_iteration=0)\n",
      " |      Return the predicted probability for each class for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      num_iteration : int\n",
      " |          Limit number of iterations in the prediction; defaults to 0 (use all trees).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      predicted_probability : array_like, shape=[n_samples, n_classes]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  classes_\n",
      " |      Get class label array.\n",
      " |  \n",
      " |  n_classes_\n",
      " |      Get number of classes\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LGBMModel:\n",
      " |  \n",
      " |  __init__(self, boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=10, max_bin=255, subsample_for_bin=50000, objective=None, min_split_gain=0, min_child_weight=5, min_child_samples=10, subsample=1, subsample_freq=1, colsample_bytree=1, reg_alpha=0, reg_lambda=0, seed=0, nthread=-1, silent=True, **kwargs)\n",
      " |      Implementation of the Scikit-Learn API for LightGBM.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      boosting_type : string\n",
      " |          gbdt, traditional Gradient Boosting Decision Tree\n",
      " |          dart, Dropouts meet Multiple Additive Regression Trees\n",
      " |      num_leaves : int\n",
      " |          Maximum tree leaves for base learners.\n",
      " |      max_depth : int\n",
      " |          Maximum tree depth for base learners, -1 means no limit.\n",
      " |      learning_rate : float\n",
      " |          Boosting learning rate\n",
      " |      n_estimators : int\n",
      " |          Number of boosted trees to fit.\n",
      " |      max_bin : int\n",
      " |          Number of bucketed bin for feature values\n",
      " |      subsample_for_bin : int\n",
      " |          Number of samples for constructing bins.\n",
      " |      objective : string or callable\n",
      " |          Specify the learning task and the corresponding learning objective or\n",
      " |          a custom objective function to be used (see note below).\n",
      " |          default: binary for LGBMClassifier, lambdarank for LGBMRanker\n",
      " |      min_split_gain : float\n",
      " |          Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      " |      min_child_weight : int\n",
      " |          Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
      " |      min_child_samples : int\n",
      " |          Minimum number of data need in a child(leaf)\n",
      " |      subsample : float\n",
      " |          Subsample ratio of the training instance.\n",
      " |      subsample_freq : int\n",
      " |          frequence of subsample, <=0 means no enable\n",
      " |      colsample_bytree : float\n",
      " |          Subsample ratio of columns when constructing each tree.\n",
      " |      reg_alpha : float\n",
      " |          L1 regularization term on weights\n",
      " |      reg_lambda : float\n",
      " |          L2 regularization term on weights\n",
      " |      seed : int\n",
      " |          Random number seed.\n",
      " |      nthread : int\n",
      " |          Number of parallel threads\n",
      " |      silent : boolean\n",
      " |          Whether to print messages while running boosting.\n",
      " |      **kwargs : other parameters\n",
      " |          Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      " |          Note: **kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      A custom objective function can be provided for the ``objective``\n",
      " |      parameter. In this case, it should have the signature\n",
      " |      ``objective(y_true, y_pred) -> grad, hess``\n",
      " |          or ``objective(y_true, y_pred, group) -> grad, hess``:\n",
      " |      \n",
      " |          y_true: array_like of shape [n_samples]\n",
      " |              The target values\n",
      " |          y_pred: array_like of shape [n_samples] or shape[n_samples * n_class]\n",
      " |              The predicted values\n",
      " |          group: array_like\n",
      " |              group/query data, used for ranking task\n",
      " |          grad: array_like of shape [n_samples] or shape[n_samples * n_class]\n",
      " |              The value of the gradient for each sample point.\n",
      " |          hess: array_like of shape [n_samples] or shape[n_samples * n_class]\n",
      " |              The value of the second derivative for each sample point\n",
      " |      \n",
      " |      for multi-class task, the y_pred is group by class_id first, then group by row_id\n",
      " |          if you want to get i-th row y_pred in j-th class, the access way is y_pred[j*num_data+i]\n",
      " |          and you should group grad and hess in this way as well\n",
      " |  \n",
      " |  apply(self, X, num_iteration=0)\n",
      " |      Return the predicted leaf every tree for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      num_iteration : int\n",
      " |          Limit number of iterations in the prediction; defaults to 0 (use all trees).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |  \n",
      " |  booster(*args, **kwargs)\n",
      " |      DEPRECATED: Use attribute booster_ instead.\n",
      " |  \n",
      " |  feature_importance(*args, **kwargs)\n",
      " |      DEPRECATED: Use attribute feature_importances_ instead.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from LGBMModel:\n",
      " |  \n",
      " |  booster_\n",
      " |      Get the underlying lightgbm Booster of this model.\n",
      " |  \n",
      " |  evals_result_\n",
      " |      Get the evaluation results.\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Get feature importances.\n",
      " |      \n",
      " |      Note: feature importance in sklearn interface used to normalize to 1,\n",
      " |          it's deprecated after 2.0.4 and same as Booster.feature_importance() now\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help (LGBMClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
